# On Understanding Types, Data Abstraction, and Polymorphism #


## From Untyped To Typed Universes ##


### Organizing Untyped Universes ###

"Untyped" actually means there is only one type.

As soon as we start working in an untyped universe, we begin to organize it in different ways for different purposes. Types arise informally in any domain to categorize objects according to their usage and behavior. The classification of objects in terms of the purposes for which they are used eventually results in a more or less well-defined type system. Types arise naturally, even starting from untyped universes.

Untyped universes of computational objects decompose naturally into subsets with uniform behavior. Sets of objects with uniform behavior may be named and are referred to as types.

After a valiant organization effort, then, we may start thinking of untyped universes as if they were typed. But this is just an illusion, because it is very easy to violate the type distinctions that we have just created. In computer memory, what is the bitwise Boolean OF of a character and a machine operation? In LISP, what is the effect of treating an arbitrary S-expression as a program? In the lambda calculus, what is the effect of a conditional over a non-Boolean value? In set theory, what is the set union of the function successor and the function predecessor? Such questions are the unfortunate consequence of organizing untyped universes without going all the way to typed systems; it is then meaningful to ask about the (arbitrary) representations of higher level concepts and their interactions.

### Static and Strong Typing ###

Types impose constraints on object interaction that prevent objects from inconsistent interaction with other objects.

A type may be viewed as a set of clothes (or a suit of armor) that protects an underlying untyped representation from arbitrary or unintended use. It provides a protective covering that hides the underlying representation and constrains the way objects may interact with other objects. In an untyped system untyped objects are naked in that the underlying representation is exposed for all to see. Violating the type system involves removing the protective set of clothing and operating directly on the naked representation.

To prevent type violations, we generally impose a static type structure on programs. Types are associated with constants, operators, variables, and function symbols. A type inference system can be used to infer the types of expressions when little or no type information  is given explicitly.

Programming languages in which the type of every expression can be determined by static program analysis are said to be statically typed. Static typing is a useful property, but the requirement that all variables and expressions are bound to a type at compile time is sometimes too restrictive. It may be replaced by the weaker requirement that all expressions are guaranteed to be type consistent although the type itself may be statically unknown; this can be generally done by introducing some run-time type checking. Languages in which all expressions are type consistent are called strongly typed languages. If a language is strongly typed, its compiler can guarantee that the programs it accepts will execute without type errors. In general, we should strive for strong typing and adopt static typing whenever possible. Note that every statically typed language is strongly typed, but the converse is not necessarily true.

Traditional statically typed systems exclude programming techniques that, although sound, are incompatible with early binding of program objects to a specific type. For example, they exclude generic procedures, such as sorting, that capture the structure of an algorithm uniformly applicable to a range of types.

### Kinds of Polymorphism ###

Conventional typed languages, such as Pascal, are based on the idea that functions and procedures, and hence their operands, have a unique type. Such languages are said to be monomorphic, in the sense that every value and variable can be interpreted to be of one and only one type. Monomorphic programming languages may be contrasted with polymorphic languages in which some values and variables may have more than one type. Polymorphic functions are functions whose operands (actual parameters) can have more than one type. Polymorphic types may be defined as types whose operations are applicable to operands of more than one type.

Strachey [1967] distinguished, informally, between two major kinds of polymorphism. Parametric polymorphism is obtained when a function works uniformly on a range of types; these types normally exhibit some common structure. Ad-hoc polymorphism is obtained when a function works, or appears to work, on several different types (which may not exhibit a common structure) and may behave in unrelated ways for each type. Our classification of polymorphism in Figure 1 refines that of Strachey by introducing a new form of polymorphism called inclusion polymorphism to model subtypes and inheritance. Parametric and inclusion polymorphism are classified as the two major subcategories of “universal polymorphism,” which is contrasted with nonuniversal or ad-hoc polymorphism. Thus Figure 1 reflects Strachey’s view of polymorphism but adds inclusion polymorphism to model object-oriented programming.

Parametric polymorphism is so called because the uniformity of type structure is normally achieved by type parameters, but uniformity can be achieved in different ways, and this more general concept is called universal polymorphism. Universally polymorphic functions will normally work on an infinite number of types (all the types having a given common structure), whereas an ad-hoc polymorphic function will only work on a finite set of different and potentially unrelated types. In the case of universal polymorphism, one can assert with confidence that some values (i.e., polymorphic functions) have many types, whereas in ad-hoc polymorphism this is more difficult to maintain, as one may take the position that an ad-hoc polymorphic function is really a small set of monomorphic functions. In terms of implementation, a universally polymorphic function will execute the same code for arguments of any admissible type, whereas an ad-hoc polymorphic function may execute different code for each type of argument.

  Figure 1. Varieties of polymorphism

  polymorphism --- universal --- parametric
                |             |
                |             -- inclusion
                |
                -- ad hoc    --- overloading
                              |
                              -- coercion

There are two major kinds of universal polymorphism, that is, two major ways in which a value can have many types. In parametric polymorphism, a polymorphic function has an implicit or explicit type parameter which determines the type of the argument for each application of that function. In inclusion polymorphism an object can be viewed as belonging to many different classes that need not be disjoint; that is, there may be inclusion of classes. These two views of universal polymorphism are not unrelated, but are sufficiently distinct in theory and in practice to deserve different names. The functions that exhibit parametric polymorphism are also called generic functions.

There are also two major kinds of ad-hoc polymorphism. In overloading, the same variable name is used to denote different functions and the context is used to decide which function is denoted by a particular instance of the name. We may imagine that a preprocessing of the program will eliminate overloading by giving different names to the different functions; in this sense overloading is just a convenient syntactic abbreviation. A coercion is instead a semantic operation that is needed to convert an argument to the type expected by a function, in a situation that would otherwise result in a type error. Coercions can be provided statically, by automatically inserting them between arguments and functions at compile time, or they may have to be determined dynamically by run-time tests on the arguments.

Our definition of polymorphism is applicable only to languages with a very clear notion of both type and value. In particular, there must be a clear distinction between the inherent type of an object and the apparent type of its syntactic representations in languages that permit overloading and coercion.

  4 ways of extending monomorphic type system:

  * Overloading is a purely syntactic way of using the same name for different semantic objects; the compiler can resolve the ambiguity at compile time and then proceed as usual.

  * Coercion allows the user to omit semantically necessary type conversions. The required type conversions must be determined by the system, inserted in the program, and used by the compiler to generate required type conversion code. Coercions are essentially a form of abbreviation that may reduce program size and improve program readability, but may also cause subtle and sometimes dangerous system errors. The need for run-time coercions is usually detected at compile time, but languages like (impure) LISP have plenty of coercions that are only detected and performed at run time.

  * Subtyping is an instance of inclusion polymorphism. The idea of a type being a subtype of another type is useful not only for subranges of ordered types such as integers, but also for more complex structures such as a type representing Toyotas, which is a subtype of a more general type such as Vehicles. Every object of a subtype can be used in a supertype context, in the sense that every Toyota is a vehicle and can be operated on by all operations that are applicable to vehicles. (inclusion polymorphism)

  * Value sharing is a special case of parametric polymorphism. We could think of the symbol nil as being heavily overloaded, but this would be some strange kind of open-ended overloading, since nil is a valid element of an infinite collection of types which have not even been declared yet. Moreover, all the uses of nil denote the same value, which is not the common case for overloading. We could also think that there is a different nil for every type, but all the nils have the same representation and can be identified. The fact that an object having many types is uniformly represented for all types is characteristic of parametric polymorphism.

Universal polymorphism is considered true polymorphism, whereas ad-hoc polymorphism is some kind of apparent polymorphism whose polymorphic character disappears at close range.

Overloading is not true polymorphism; instead of a value having many types, we allow a symbol to have many types, but the values denoted by that symbol have distinct and possibly incompatible types. Similarly, coercions do not achieve true polymorphism: An operator may appear to accept values of many types, but the values must be converted to some representation before the operator can use them; hence that operator really works on (has) only one type. Moreover, the output type is no longer dependent on the input type, as is the case in parametric polymorphism.

In contrast to overloading and coercion, subtyping is an example of true polymorphism: Objects of a subtype can be uniformly manipulated as if belonging to their supertypes. In the implementation, the representations are chosen very carefully, so that no coercions are necessary when using an object of a subtype in place of an object of the supertype. In this sense the same object has many types (e.g., in Simula a member of a subclass may be a longer memory segment than a member of its superclass, and its initial segment has the same structure as the member of the superclass). Similarly, operations are careful to interpret the representations uniformly so that they can work uniformly on elements of subtypes and supertypes.

Parametric polymorphism is the purest form of polymorphism: the same object or function can be used uniformly in different type contexts without changes, coercions, or any kind of run-time tests or special encodings of representations. However, it should be noted that this uniformity of behavior requires that all data be represented, or somehow dealt with, uniformly.

ALGOL 68 is well known for its baroque coercion scheme. The ALGOL 68 experience suggests that coercions should generally be explicit, and this view has been taken in many later language designs.

We should mention generic procedures of the kind found in Ada, which are parameterized templates that must be instantiated with actual parameter values before they can be used. The polymorphism of Ada’s generic procedures is similar to the parametric polymorphism of languages like ML but is specialized to particular kinds of parameters. Parameters may be type parameters, procedure parameters, or value parameters. Generic procedures with type parameters are polymorphic in the sense that formal type parameters can take different actual types for different instantiations. Generic type polymorphism in Ada, however, is syntactic since generic instantiation   is performed at compile time with actual type values that must be determinable (manifest) at compile time. The semantics of generic procedures is macroexpansion driven by the type of the arguments. Thus generic procedures can be considered abbreviations for sets of monomorphic procedures. With respect to polymorphism, they have the advantage that specialized optimal code can be generated for the different forms of inputs. On the other hand, in true polymorphic systems, code is generated only once for every generic procedure.

### The Evolution of Types in Programming Languages ###

STONE AGE: no type, or, a single arithmetic type

FORTRAN: distinguish between integer and float-point numbers, by the first letter or their names

ALGOL 60: integer, real, boolean. ALGOL 60 was the first significant language to have an explicit notion of type and associated requirements for compile-time type checking.

PL/I: arrays, records, pointers, try to combine features of FORTRAN, ALGOL 60, COBOL, LISP. It has numerous type loopholes, such as not requiring the type of values pointed to by pointer variables to be specified, which weaken the effectiveness of compile-time type checking.

Pascal: arrays, records, pointers, user-defined types. Pascal does not define type equivalence, so that the question of when two type expressions denote the same type is implementation dependent.

ALGOL 68: ALGOL 68 has a more rigorous notion of type than Pascal, with a well-defined notion of type equivalence (structural equivalence). The notion of type (mode in ALGOL 68) is extended to include procedures as first-class values. Primitive modes include int, real, char, bool, string, bits, bytes, format, file, whereas mode include array, struct, proc, union, and ref for constructors (type constructors) constructing,  respectively, array types, record types, procedure types, union (variant) types, and pointer types. ALGOL 68 has carefully defined rules for coercion, using dereferencing, deproceduring, widening, rowing, uniting, and voiding to transform values to the type required for further computation. Type checking in ALGOL 68 is decidable, but the type-checking algorithm is so complex that questions of type equivalence and coercion cannot always be checked by the user. This complexity was felt by some to be a flaw, resulting in a reaction against complex type systems. Thus later languages like Ada had a simpler notion of type equivalence with severely restricted coercion.

Simula: Simula is the first object-oriented language. Its notion of type includes classes whose instances may be assigned as values of class-valued variables and may persist between the execution of the procedures they contain. Procedures and data declarations of a class constitute its interface and are accessible to users. Subclasses inherit declared entities in the interface of superclasses and may define additional operations and data that specialize the behavior of the subclass. Instances of a class are like data abstractions in having a declarative interface and a state that persists between invocation of operations, but lack the information-hiding mechanism of data abstractions. Subsequent object-oriented languages like Smalltalk and Loops combine the class concept derived from Simula with a stronger notion of information hiding.

Modula-2: Modula-2 [Wirth 1983] is the first widespread language to use modularization as a major structuring principle (these ideas were first developed in Mesa). Typed interfaces specify the types and operations available in a module; types in an interface can be made opaque to achieve data abstraction. An interface can be specified separately from its implementation,   thereby separating the specification and implementation tasks. Block-structured  scoping, preserved within modules, is abandoned at a more global level in favor of flexible intermodule visibility rules achieved by import and export lists. Module interfaces are similar to class declarations (except for the above-mentioned scoping rules), but unlike class instances, module instances are not first-class values. A linking phase is necessary to interconnect module instances for execution; this phase is specified by the module interfaces but is external to the language.

ML: ML has introduced the notion of parametric polymorphism in languages. ML types can contain type variables that are instantiated to different types in different contexts. Hence it is possible to partially specify type information and to write programs based on partially specified types that can be used on all the instances of those types. A way of partially specifying types is just to omit type declarations: The most general (less specific) types that fit a given situation are then automatically inferred.


Packages in Ada have an interface specification of named components that may be simple variables, procedures, exceptions, and even types. They may hide a local state either by a private data type or in the package body. Packages are like record instances in having a user interface of named components. Ada packages differ from records in that record components must be typed values, whereas package components may be procedures, exceptions, types, and other named entities. Since packages are not themselves types, they cannot be parameters, components of structures, or values of pointer variables [Wegner 1983]. Packages in Ada are second-class objects, whereas class instances in Simula or objects in object-oriented languages are first-class objects.

(J: type and data abstraction can be totally seperated in Ada.)

The differences in behavior between packages and records in Ada are avoided in object-oriented languages by extending the notion of type to procedures and data abstractions. In the context of this discussion it is useful to define object-oriented languages as extensions of procedure-oriented languages that support typed data abstractions with inheritance. Thus we say that a language is object oriented if and only if it satisfies the following requirements:

  * It supports objects that are data abstractions with an interface of named operations and a hidden local state.
  * Objects have an associated object type.
  * Types may inherit attributes from supertypes.

These requirements may be summarized as

  object oriented = data abstractions + object types + type inheritance.

Data abstraction by itself provides a way of organizing data with associated operations that differs considerably from the traditional methodology of procedure-oriented programming. The realization of data abstraction methodology was one of the primary objectives of Ada. However, Ada satisfies only the first of our three requirements for object-oriented programming,

The requirement that all objects have a type allows objects to be first-class values so that they can be managed as data structures within the language as well as used for computation. The requirement of type inheritance allows relations among types to be specified. Inheritance may be viewed as a type composition mechanism that allows the properties of one or more types to be reused in the definition of a new type. The specification B inherits A may be viewed as an abbreviation mechanism that avoids redefining the attributes of type A in the definition of type B. Inheritance, however, is more than a shorthand, since it imposes structure upon a collection of related types that can greatly reduce the conceptual complexity of a system specification. This is illustrated by the Smalltalk object hierarchy in Goldberg and Robson [1983].

The Smalltalk object hierarchy is a description of the Smalltalk programming environment in Smalltalk. It is conceptually similar to the LISP apply function, which describes the LISP language interpreter in LISP, but is a great deal more complex. It describes a collection of over 75 related system object types by an inheritance hierarchy. The object types include numerical, structured, input-output, concurrent, and display objects. The object hierarchy carefully factors out properties common to numeric objects into the supertype Number. It factors out properties common to different kinds of structured objects into the supertype Collection. It further factors out properties common to numbers, collections, and other kinds of objects into the supertype Object. In doing this the collection of over 75 object types that comprise the Smalltalk environment is described as a relatively simple structured hierarchy of object types. The shorthand provided by the object hierarchy in reusing superclasses whose attributes are shared by subclasses is clearly incidental to the conceptual parsimony achieved by imposing a coherent structure on the collection of object types.

The Smalltalk object hierarchy is also significant as an illustration of the power of polymorphism. We may characterize a polymorphic function as a function applicable to values of more than one type and inclusion polymorphism as a relation among types that allows operations to be applied to objects of different types related by inclusion. Objects are seen as collections of such polymorphic operations (attributes). This view emphasizes the sharing of operations by operands of many types as a primary feature of polymorphism.

(J: my thought was the primary feature of polymorphism is dispatching. These different views will lead to different programs, e.g. my view will lead to abusing of polymorphism.)

The Smalltalk object hierarchy realizes polymorphism in the above sense by factoring out attributes common to a collection of subtypes into a supertype. Thus polymorphism is intimately related to the notion of inheritance, and we can say that the expressive power of object-oriented type systems is due in large measure to the polymorphism they facilitate.

ML is an interactive functional programming language in which type specifications omitted by the user may be reintroduced by type inference. ML supports type inference not only for traditional  types but also for parametric (polymorphic) types, such as the length function for lists. If the user then enters “length[l; 2; 3]“, applying length to a list of integers, the system infers that length is to be specialized to the type “int list ---, int” and then applies the specialized function to the list of integers.

(J: not type check and apply the general function to the list of integers?)

When we say that a parametric function is applicable to lists of arbitrary type, we really mean that it may be specialized by (implicitly or explicitly) providing a type parameter T, and that the specialized function may then be applied to the specialized operands. There is an important distinction between the parametric function length for lists of arbitrary type and the specialized function for lists of type int. Functions like length are applicable to lists of arbitrary type because they have a uniform parametric representation that allows them to be specialized by supplying a type parameter. This distinction between a parametric function and its specialized versions is blurred in languages like ML because type parameters omitted by the user are automatically reintroduced by the type inference mechanism.

Supertypes in object-oriented languages may be viewed as parametric types whose parameter is omitted by the user. In order to understand the similarity between parametric types and supertypes, it is useful to introduce a notation in which supertype parameters must be explicitly supplied in specializing a supertype to a subtype. We shall see below that Fun has explicit type parameters for both parametric types and supertypes in order to provide a uniform model for both parametric and subtype polymorphism. This results in a uniform treatment of type inference when parameters are omitted in parametric types and supertypes.

### Type Expression Sublanguages ###

Type expression sublanguages generally include basic types like integer and boolean and composite types like arrays, records, and procedures constructed from basic types:

  Type ::= BasicType | ConstructedType
  BasicType ::= Int | Boo1 | ...
  ConstructedType ::= Array(Type) | Type -> Type | ...

The type expression sublanguage should be sufficiently rich to support types for all values with which we wish to compute, but sufficiently tractable to permit decidable and efficient type checking. One of the purposes of this paper is to examine trade-offs between richness and tractability for type expression sublanguages of strongly typed languages.

The type expression sublanguage can generally be specified by a context-free grammar. However, we are interested not only in the syntax of the type expression sublanguage, but also in its semantics. This is, we are interested in what types denote and in relations among type expressions. The most basic relation among type expressions is type equivalence. However, we are also interested in similarity relations among types that are weaker than equivalence, such as inclusion, which is related to subtypes.

Similarity relations among type expressions that permit a type expression to denote more than one type, or to be compatible with many types, are referred to as polymorphism.

The usefulness of a type system lies not only in the set of types that can be represented, but also in the kinds of relationships among types that can be expressed. The ability to express relations among types involves some ability to perform computations on types to determine whether they satisfy the desired relationship. Such computations could, in principle, be as powerful as computations performable on values.

### Preview of Fun ###

Fun is a lambda-calculus-based language that enriches the first-order typed lambda-calculus with second-order features designed to model polymorphism and object-oriented languages.

Section 2 reviews the untyped and typed lambda-calculus and develops first-order features of the Fun type expression sublanguage. Fun has the basic types Bool, Int, Real, String and constructed types for record, variant, function, and recursive types. This set of first-order types is used as a base for introducing parametric types, abstract data types, and type inheritance by means of second-order language features in subsequent sections.

Section 3 briefly reviews theoretical models of types related to features of Fun, especially models that view types as sets of values. Viewing types as sets allows us to define parametric polymorphism in terms of set intersection of associated types and inheritance polymorphism in terms of subsets of associated types. Data abstraction may also be defined in terms of set operations (in this case unions) on associated types.

Sections 4, 5, and 6, respectively, augment the first-order lambda-calculus with universal quantification for realizing parameterized types, existential quantification for realizing data abstraction, and bounded quantification for realizing type inheritance. The syntactic extensions of the type expression sublanguage determined by these features may be summarized as follows:

  Type ::= ... | QuantifiedType
  QuantifiedType ::=
                      Forall A. Type        |                             # Universal Quantification
                      Exists A. Type        |                             # Existential Quantification
                      Forall A < Type. Type | Exists A < Type. Type       # Bounded Quantification

Universal quantification enriches the first-order lambda-calculus with parameterized types that may be specialized by substituting actual type parameters for universally quantified parameters. Universally quantified types are themselves first-class types and may be actual parameters in such a substitution. Existential quantification enriches first-order features by allowing abstract data types with hidden representation.

Fun supports information hiding not only through existential quantification, but also through its let construct, which facilitates hiding of local variables of a module body. Hiding by means of let is referred to as first-order hiding because it involves hiding of local identifiers and associated values, whereas hiding by means of existential quantifiers is referred to as second-order hiding because it involves hiding of type representations.

Bounded quantification enriches the first-order lambda-calculus by providing explicit subtype parameters. Inheritance (i.e., subtypes and supertypes) is modeled by explicit parametric specialization of supertypes to the subtype for which the operations will actually be executed. In object-oriented languages every type is potentially a supertype for subsequently defined subtypes and should therefore be modeled by a bounded quantified type. Bounded quantification provides an explanatory mechanism for object-oriented polymorphism that is cumbersome to use explicitly but useful in illuminating the relation between parametric and inherited polymorphism.


## The Lambda Calculus


### The Untyped Lambda Calculus

e ::= x       -- a variable is a lambda-expression
e ::= fun(x)e -- functional abstraction of e
e ::= e(e)    -- operator e applied to operand e

value id = fun(x) x     -- identity function
value succ = fun(x) x+1 -- successor function (for integers)

The identity function may be applied to an arbitrary lambda-expression and always yields the lambda-expression itself. In order to define addition on integers in the pure lambda-calculus we pick a representation for integers and define the addition operation so that its effect on lambda-expressions representing the integers n and m is to produce the lambda-expression that represents n + m. The successor function should be applied only to lambda-expressions that represent integers and suggests a notion of typing. The infix notation x + 1 is an abbreviation for the functional notation + (x)(l). The symbols 1 and + above should in turn be viewed as abbreviations for a pure lambda-calculus expression for the number 1 and addition.

Correctness of integer addition requires no assumptions about what happens when the lambda-expression representing addition is applied to lambda-expressions that do not represent integers. However, if we want our notation to have good error-checking properties, it is desirable to define the effect of addition on arguments that are not integers as an error. This is accomplished in typed programming languages by type checking that eliminates, at compile time, the possibility of operations on objects of an incorrect type.

Type checking in the lambda-calculus, just as in conventional programming languages, has the effect that large classes of lambda-expressions legal in the untyped lambda-calculus become illegal. The class of illegally typed expressions type system that one adopts, and, although undesirable, it may a particular type-checking algorithm.

### The Typed Lambda Calculus

The typed lambda-calculus is like the lambda-calculus, except that every variable must be explicitly typed when introduced as a bound variable. Thus the successor function in the typed X-calculus has the following form:

  value succ = fun (x: Int) x+1

(J: type is an attribute of *bound variable*!!)

The function twice from integers to integers has a parameter f whose type is Int -> Int (the type of functions from integers to integers) and may be written as follows:

  value twice = fun(f: Int -> Int) fun (y:Int) f(f(y))

This notation approximates that of functional specification in typed programming languages but omits specification of the result type. We may denote the result type with a returns keyword as follows:

  value succ = fun(x: Int) (returns Int) x + 1

However, the type of the result can be determined from the form of the function body x + 1. We omit result type specifications in the interests of brevity. Type inference mechanisms that allow this information to be recovered during compilation are discussed in a later section.

Type declarations are introduced by the keyword type. Throughout this paper, type names begin with uppercase letters, whereas value and function names begin with lowercase letters:

  type IntPair = Int x Int
  type IntFun = Int -> Int

Type declarations introduce names (abbreviations) for type expressions; they do not create new types in any sense. This is sometimes expressed by saying that we used structural equivalence on types instead of name equivalence: Two types are equivalent when they have the same structure, regardless of the names we use as abbreviations. The fact that a *value* v has a type T is indicated by v : T:

  (3,4): IntPair
  succ: IntFun

We need not introduce variables by type declarations of the form var : T because the type of a variable may be determined from the form of the assigned value. For example, the fact that intPair below has the type IntPair can be determined by the fact that (3, 4) has type Int X Int, which has been declared equivalent to IntPair:

(J: which means if the variable is not assigned a value (unbind), you have to introduce type declaration for it)

  value intPair = (3,4)

However, if we want to indicate the type of a variable as part of its initialization, we can do so by the notation value var : T = value:

  value intPair: IntPair = (3,4)
  value succ: Int -> Int = fun(x: Int) x + 1


Local variables can be declared by the let-in construct, which introduces a new initialized variable (following let) in a local scope (an expression following in). The value of the construct is the value of that expression:

  let a = 3 in a + 1 -- yields 4

If we want to specify types, we can also write

  let a : Int = 3 in a + 1

The let-in construct can be defined in terms of basic fun expressions:

  let a : T = M in N  ===  (fun(a:T) N)(M)

### Basic Types, Structured Types, and Recursion

The typed X-calculus is usually augmented with various kinds of basic and structured types. For basic types we shall use

  Unit:   the trivial type, with only element ()
  Bool:   with an if-then-else operation
  Int:    with arithmetic and comparison operations
  Real:   with arithmetic and comparison operations
  String: with string concatenation (infix) ^

Structured types can be built up from these basic types by means of type constructors. The type constructors in our language include function spaces (->), Cartesian products (x), record types (also called labeled Cartesian products), and variant types (also called labeled disjoint sums).

A pair is an element of a Cartesian product type, for example,

  value p = 3,true : Int x Bool

Operations on pairs are selectors for the first and second components:

  fst(p) yields 3
  snd(p) yields true

A record is an unordered set of labeled values. Its type may be specified by indicating the type associated with each of its labels. A record type is denoted by a sequence of labeled types, separated by commas and enclosed in curly braces:

  type ARecordType = {a: Int, b: Bool, c: String}

A record of this type may be created by initializing each of the record labels to a value of the required type. It is written as a sequence of labeled values separated by commas and enclosed in curly braces:

  value r: ARecordType = { a = 3, b = true, c = “abed” }

The labels must be unique within any given record or record type. The only operation on records is field selection, denoted by the usual dot notation:

  r.b yields true

Since functions are first-class values, records may in general have function components:

  type FunctionRecordType = { fl : Int -> Int, f2: Real -> Real }
  value functionRecord = {fl = succ, 12 = sin}

A record type can be defined in terms of existing record types by an operator &, which concatenates two record types:

  type NewFunctionRecordType = FunctionRecordType & {f3: Bool -> Bool}

This is intended as an abbreviation, instead of writing the three fields f1, f2, and f3 explicitly. It is valid only when used on record types, and when no duplicated labels are involved.

A data structure can be made local and private to a collection of functions by let-in declarations. Records with function components are a particularly convenient way of achieving this; here is a private counter variable shared by an increment and a total function:

  value counter = let count = ref(0)
                  in {increment = fun(n:Int) count := count + n,
                      total = fun() count
                     }

  counter.increment(3)
  counter.total() yields 3

This example involves side effects, since the main use of private variables is to update them privately. The primitive ref returns an updatable reference to an object, and assignments are restricted to work on such references. This is a common form of information hiding that allows updating of local state by using static scoping to restrict visibility.

A variant type is also formed from an unordered set of labeled types, which are now enclosed in brackets:

  type AVariantType = [a: Int, b:Bool, c: String]

An element of this type can either be an integer labeled a, a Boolean labeled b, or a string labeled C:

  value vl = [a = 31
  value v2 = [b = true]
  value v3 = [c = “abed”]

The only operation on variants is case selection, A case statement for a variant of type AVariantType has the following form:

  case variant of
    [a = (variable) of type Int]    (action for case a)
    [b = (variable) of type Bool]   (action for case b)
    [c = (variable) of type String] (action for case c)

where in each case a new variable is introduced and bound to the respective contents of the variant. That variable can then be used in the respective action.

In the untyped lambda-calculus it is possible to express recursion operators and to use them to define recursive functions. However, all computations expressible in the typed lambda-calculus must terminate (roughly, the type of a function is always strictly more complex than the type of its result; hence, after some number of applications of the function, we obtain a basic type; moreover, we do not have nonterminating primitives). Hence, recursive definitions are introduced as a new primitive concept. The factorial function can be expressed as

  rec value fact = fun (n:Int) if n=O then 1 else n*fact(n-1)

For simplicity we assume that the only *values* that can be recursively defined are functions.

Finally, we introduce recursive type definitions. This allows us, for example, to define the type of integer lists out of record and variant types:

  rec type IntList = [nil:Unit, cons: {head: Int, tail: IntList}]


## Types Are Sets of Values

There is a universe V of all values, containing simple values like integers, data structures like pairs, records and variants, and functions. This is a complete partial order, built by using Scott’s techniques [Scott 19761,but in first approximation we can think of it as just a large set of all possible computable values.

A type is a set of elements of V. Not all subsets of V are legal types: They must obey some technical properties. The subsets of V obeying such properties are called ideals. All the types found in programming languages are ideals in this sense, so we do not have to worry too much about subsets of V that are not ideals.

Hence, a type is an ideal, which is a set of values. Moreover, the set of all types (ideals) over V, when ordered by set inclusion, forms a lattice. The top of this lattice is the type Top (the set of all values, i.e., V itself). The bottom of the lattice is, essentially, the empty set (actually, it is the singleton set containing the least element of V).

The phrase having a type is then interpreted as membership in the appropriate set. Because ideals over V may overlap, a value can have many types. The set of types of any given programming language is generally only a small subset of the set of all ideals over V. For example, any subset of the integers determines an ideal (and hence a type), and so does the set of all pairs with first element equal to 3. This generality is welcome because it allows one to accommodate many different type systems in the same framework. One has to decide exactly which ideals are to be considered interesting in the context of a particular language.

A particular type system is then a collection of ideals of V, which is usually identified by giving a language of type expressions and a mapping from type expressions to ideals. The ideals in this collection are elevated to the rank of types for a particular language. For example, we can choose the integers, integer pairs, and integer-to-integer functions as our type system. Different languages will have different type systems, but all these type systems can be built on top of the domain V (provided that V is rich enough to start with), using the same techniques.

A monomorphic type system is one in which each value belongs to at most one type (except for the least element of V which, by definition of ideal, belongs to all types). As types are sets, a value may belong to many types. A polymorphic type system is one in which large and interesting collections of values belong to many types. There is also a gray area of mostly monomorphic and almost polymorphic systems, so the definitions are left imprecise, but the important point is that the basic model of ideals over V can explain all these degrees of polymorphism.

Since types are sets, subtypes simply correspond to subsets. Moreover, the semantic assertion Tl is a subtype of T2 corresponds to the mathematical condition Tl is subset of T2 in the type lattice. This gives a very simple interpretation for subrange types and inheritance, as we shall see in later sections.

Finally, if we take our type system as consisting of the single set V, we have a type-free system in which all values have the same type. Hence we can express typed and untyped languages in the same semantic domain and compare them.

The type lattice contains many more points than can be named in any type language. In fact, it includes an uncountable number of points, since it includes every subset of the integers. The objective of a language for talking about types is to allow the programmer to name those types that correspond to interesting kinds of behavior. In order to do this, the language contains type constructors, including function type constructors (e.g., type T = Tl -> T2) for constructing a function type T from domain and range types Tl, T2. These constructors allow an unbounded number of interesting types to be constructed from a finite set of primitive types. However, there may be useful types of the type lattice that cannot be denoted using these constructors.

In the remaining sections of this paper we introduce more powerful type constructors that allow us to talk about types corresponding to infinite unions and intersections in the type lattice. In particular, universal quantification will allow us to name types whose lattice points are infinite intersections of types, while existential quantification will allow us to name types corresponding to infinite unions. Our reason for introducing universal and existential quantification is the importance of the resulting types in increasing the expressive power of typed programming languages. It is fortunate that these concepts are also mathematically simple and that they correspond to well-known mathematical constructions.

The ideal model is not the only model of types that has been studied. With respect to other denotational models, however, it has the advantage of explaining simple and polymorphic types in an intuitive way, namely, as sets of values, and of allowing a natural treatment of inheritance. Less satisfactory is its treatment of type parameterization, which is rather indirect since types cannot be values, and its treatment of type operators, which involves getting out of the model and considering functions over ideals. In view of this intuitive appeal, we have chosen the ideal model as our underlying view of types, but much of our discussion could be carried over, and sometimes even improved, if we chose to refer to other models.

The idea of types as parameters is fully developed in the second-order lambda-calculus [Bruce and Meyer 19841. The (only known) denotational models of the second-order lambda-calculus are retract models [Scott 19761. Here, types are not sets of objects but special functions (called retracts); these can be interpreted as identifying sets of objects but are objects themselves. Because of the property that types are objects, retract models can more naturally explain explicit type parameters, whereas ideal models can more naturally explain implicit type parameters.


## Universal Quantification


### Universal Quantification and Generic Functions

The typed lambda-calculus is sufficient to express monomorphic functions. However, it cannot adequately model polymorphic functions. The identity function can be defined only for specific types such as integers: fun(x:Int) x. We cannot capture the fact that its form does not depend on any specific type. We cannot express the idea of a functional form that is the same for a variety of types, and we must explicitly bind variables and values to a specific type at a time when such binding may be premature.

The fact that a given functional form is the same for all types may be expressed by universal quantification. In particular, the identity function may be expressed as follows:

  value id = all[a] fun(x:a) x

In this definition of id, a is a type variable and all[a] provides type abstraction for a so that id is the identity for all types. In order to apply this identity function to an argument of a specific type, we must first supply the type as a parameter and then the argument of the given type:

  id [Int] (3)

(We use the convention that type parameters are enclosed in square brackets, while typed arguments are enclosed in parentheses.)

We refer to functions like id, which require a type parameter before they can be applied to functions of a specific type as generic functions. id is the generic identity function. Note that all is a binding operator just like fun and requires a matching actual parameter to be supplied during function application. However, all[a] serves to bind a type, while fun(x:a) serves to bind a variable of a given (possibly generic) type.

Although types are applied, there is no implication that types can be manipulated as values: Types and values are still distinct, and type abstractions and application serve type-checking purposes only, with no run-time implications. In fact, we may decide to omit the type information in square brackets:

  value id = fun(x:a) x          -- where a is now afree type variable
  id(3)

(J: there's no type information at runtime, only values)

Here the type-checking algorithm has the task of recognizing that a is a free type variable and reintroducing the original all[a] and [lnt] information. This is part of what a polymorphic type checker can do, like the one used in the ML language. In fact, ML goes further and allows the programmer to omit even the remaining type information:

  value id = fun(x) x
  id(3)

ML has a type inference mechanism that allows the system to infer the types of both monomorphic and polymorphic expressions, so that type specifications omitted by the programmer can be reintroduced by the system. This has the advantage that the programmer can use the shorthand of the untyped lambda-calculus, while the system can translate the untyped input into fully typed expressions.

However, there are no known fully automatic type inference algorithms for the powerful type systems we are going to consider. In order for us to clarify what is happening without depending on the current state of type-checking technology, we shall always write down enough type information to make the type-checking task trivial.

Going back to the fully explicit language, let us extend our notation so that the type of a polymorphic function can be explicitly talked about. We denote the type of a generic function from an arbitrary type to itself by Va. a -> a:

  type GenericId = Va. a -> a
  id: GenericId

Here is an example of a function taking a parameter of a universally quantified type. The function inst takes a function of the above type and returns two instances of it, specialized for integers and Booleans:

  value inst = fun(f: Va. a -> a) (f[Int],f[Bool])
  value intid = fst(inst(id))   : Int  -> Int
  value boolid = snd(inst(id))  : Bool -> Bool

In general, function parameters of universally quantified types are most useful when they have to be used on different types in the body of a single function, for example, a list length function passed as a parameter and used on lists of different types. In order to show some of the freedom we have in defining polymorphic functions, we now write two versions of twice which differ in the way type parameters are passed.

The first version, twice1, has a function parameter f, which is of a universal type. The specification fun(f: Va. a -> a) body-of-function specifies the type of function parameter f to be generic and to admit functions from any given type into the same type. Applied instances of f in the body of twice1 must have a formal type parameter f [t] and require an actual type to be supplied when applying twice1. The full specification of twice1 requires binding of the type parameter t as a universally quantified type and binding of x to t:

  value twice1 = all[t] fun(f: Va. a -> a) fun(x:t) f[t](f[t](x))

Thus twice1 has three bound variables for which actual parameters must be supplied during function application:

  all[t]             -- requires an actual parameter which is a type
  fun(f: Va. a -> a) -- requires a function of the type Va. a + a
  fun(x: t)          -- requires an argument of the type substituted for t

An application of twice1 to the type Int, the function id, and the argument 3 is specified as follows:

  twice1[Int](id)(3)

Note that the third argument 3 has the type Int of the first argument and that the second argument id is of a universally quantified type. Note also that twicel[int](succ) would not be legal because succ does not have the type Va. a -> a.

The function twice2 below differs from twice1 in the type of the argument f, which is not universally quantified. Now we do not need to apply f [t] in the body of twice:

  value twice2 = all[t] fun(f: t -> t) fun(x: t) f(f(x))

  twice2[Int]      -- yields fun(f: Int -> Int) fun(x: Int) f(f(x))

It is now possible to compute twice of succ:

  twice2[Int](succ)    -- yields fun(x: Int) succ(succ(x))
  twice2[lnt](succ)(3) -- yields 5

An extra type application is required for twice2 of id, which has to be first specialized to Int

  twice2[Int](id[Int])(3)

Note that both lambda-abstraction (function abstraction) and universal quantification (generic type abstraction) are binding operators that require formal parameters to be replaced by actual parameters. Separation between types and values is achieved by having different binding operations for types and values and different parenthesis syntax when actual parameters are supplied.

The extension of the lambda-calculus to support two different kinds of binding mechanism, one for types and one for variables, is both practically useful in modeling parametric polymorphism and mathematically interesting in generalizing the lambda-calculus to model two qualitatively different kinds of abstraction in the same mathematical model. In the next few sections we introduce still a third kind of abstraction and associated binding mechanism, but first we have to introduce the notion of parametric types.

In Fun, types and values are rigorously distinguished (values are objects and types are sets). Hence we need two distinct binding mechanisms: fun and all. These two kinds of bindings can be unified in some type models where types are values, achieving some economy of concepts, but this unification does not fit our underlying semantics. In such models it is also possible to unify the parametric type-binding mechanism described in the next section with fun and all.

### Parametric Types ###

If we have two type definitions with similar structure, for example,

  type BoolPair = Bool x Bool
  type IntPair = Int x Int

we may want to factor the common structure in a single parametric definition and use the parametric type in defining other types:

 type Pair[T] = T x T
 type PairOfBool = Pair[Bool]
 type PairOfInt = Pair[Int]

A type definition simply introduces a new name for a type expression, and it is equivalent to that type expression in any context. A type definition does not introduce a new type. Hence, 3,4 is an IntPair because it has type Int x Int, which is the definition of IntPair.

A parametric type definition introduces a new type operator. Pair above is a type operator mapping any type T to a type T x T. Hence Pair[Int] is the type Int x Int, and it follows that 3, 4 has type Pair[Int].

*******************************************************************************************************************
Type operators are not types: They operate on types. In particular, one should not confuse the following notations:

   type A[T] = T -> T
   type B    = VT. T -> T

where A is a type operator that, when applied to a type T, gives the type of functions from T to T, and B is the
type of the identity function and is never applied to types.
*******************************************************************************************************************

(J: the difference between parametric type (aka. type operator) and universal quantification is interesting and delicate. Type operator, thus parametric type, IS NOT a type. There's no value has type A. Whenever you use a parametric type, you apply a specific type to it first, then you say the value has type A[T]. In contrast, there're values whose type is B, in other words, universally quantified type IS a type. They're the tools aim at the same thing but work in two different ways. The other thing I noticed is, universal quantification need support from both what I called 'value space' and 'type space', e.g., the all operator in value space and V quantifier in type space; while parametric type only need support from type space.)

(J: my feeling is, universally quantified type push the binding of type later than parametric type. Just a feeling.)

(J: here the author showed us how to define a general list, in two ways)

(Way One: use parametric type)

Type operators can be used in recursive definitions, as in the following definition of generic lists. Note that we cannot think of List[ltem] below as an abbreviation that has to be macroexpanded to obtain the real definition (this would cause an infinite expansion). Rather, we should think of List as a new type operator, which is recursively defined and maps any type to lists of that type:

  ret type List[Item] = [nil: Unit, cons: {head: Item, tail: List[Item]}]

(Way Two: use universal quantification)

A generic empty list can be defined, and then specialized, as

  value nil = all Item. [nil = ()]
  value intNil = nil[Int]
  value boolNil = nil[Bool]

Now, [nil = ()] has type List[Item], for any Item (as it matches the definition of List[Item]). Hence the types of the generic nil and its specializations are

  nil     : VItem. List[Item]
  intNil  : List[Int]
  boolNil : List[Bool]

Similarly, we can define a generic cons function and other list operations:

  value cons : VItem. (Item x List[ltem]) -> List[Item] = all Item. fun (h:Item, t:List[Item]) [cons = {head = h, tail = t}]

Note that cons can only build homogeneous lists, because of the way its arguments and result are related by the same Item type. We should mention that there are problems in deciding, in general, when two parametric recursive type definitions represent the same type. Solomon [1978] describes the problem and a reasonable solution, which involves restricting the form of parametric type definitions.


## Existential Quantification


Type specifications for variables of a universally quantified type have the following form for any type expression t(a):

  p: Va.t(a)     (e.g. id: Va. a -> a)

By analogy with universal quantification, we can try to give meaning to existentially quantified types. In general, for any type expression t(a),

  p: Ea.t(a)

has the property: For some type a, p has the type t(a)

For example,

  (3,4): Ea. a x a
  (3,4): Ea. a

where a = Int in the first case, and a = Int x Int in the second.

Thus we see that a given constant such as (3, 4) can satisfy many different existential types. (Warning: For didactic purposes we assign here existential types to ordinary values, like (3,4). Although this is conceptually correct, in later sections it is disallowed for type-checking purposes, and we require using particular constructs to obtain objects of existential type.)

Every value has type Ea. a because for every value there exists a type such that that value has that type. Thus the type Ea. a denotes the set of all values, which we sometimes call Top (the biggest type):

  type Top = Ea. a -- the type of any value whatsoever

(J: it seems only function can have a universal quantified type? except the residential of bottom type?)

The set of all ordered pairs may be denoted by the following existential type:

  Ea.Eb. a X b     -- the type of any pair whatsoever

This is the type of any pair p, q because, for some type a (take a type of p) and some type b (take a type of q), p, q has type a X b.

The type of any object, together with an integer-valued operation that can be applied to it may be denoted by the following existential type:

  Ea.a X (a -> Int)

The pair (3, succ) has this type, if we take a = Int. Similarly the pair ([l; 2; 3], length) has this type, if we take a = List[lnt].

Because the set of types includes not only simple types but also universal types and the type Top, existentially quantified types have some properties that may at first appear counterintuitive. The type Ea. a X a is not simply the type of pairs of equal type (e.g., 3, 4), as one might expect. In fact, even 3, true has this type. We know that both 3 and true have type Top; hence there is a type a = Top such that 3, true : a X a. Therefore, Ea. a x a is the type of all pairs whatsoever and is the same as Ea.Eb. a x b. Similarly, any function whatsoever has type Ea. a + a, if we take a = Top.

However, Ea. a x (a -> Int) forces a relation between the type of an object and the type of an associated integer-valued function. For example, (3, length) does not have this type (if we consider 3 as having type Top, then we would have to show that length has type Top + Int, but we only know that length : Va. List[a] -> a maps integer lists to integers, and we cannot assume that any arbitrary object of type Top will be mapped to an integer).

Not all existential types turn out to be useful. For example, if we have an (unknown) object of type Ea. a, we have absolutely no way of manipulating it (except passing it around) because we have no information about it. If we have an (unknown) object of type ga. a x a, we can assume that it is a pair and apply fst and snd to it, but then we are stuck because we have no information about a.

Existentially typed objects can be useful, however, if they are sufficiently structured. For example, x : Ea. a x (a -> Int) provides sufficient structure to allow us to compute with it. We can execute

  (snd(x)) (fst(x))

and obtain an integer.

Hence, there are useful existential types, which hide some of the structure of the objects they represent but show enough structure to allow manipulations of the objects through operations the objects themselves provide. These existential types can be used, for example, in forming apparently heterogeneous lists:

  [(3,succ); ([1;2;3],length)] : List[Ea. a x (a -> Int)]

We can later extract an element of this list and manipulate it, although we may not know which particular element we are using and what its exact type is. Of course, we can also form totally heterogeneous lists of type List[Ea.a], but these are quite unusable.

(J: this is the true 'programming to interface'!!! damn cool.)

### Existential Quantification and Information Hiding ###

The real usefulness of existential types becomes apparent only when we realize that Ea. a X (a -> Int) is a simple example of an abstract type packaged with its set of operations. The variable a is the abstract type itself, which hides a representation. The representation was Int and List[Int] in the previous examples.

Then a X (a -> Int) is the set of operators on that abstract type: a constant of type a and an operator of type a -> Int. These operators are unnamed, but we can have a named version by using record types instead of Cartesian products:

  x: Ea. {const: a, op: a -> Int)
  x.op(x.const)

As we do not know what the representation a really is (we only know that there is one), we cannot make assumptions about it, and users of x will be unable to take advantage of any particular implementation of a. As we announced earlier, we have been a bit liberal in applying various operators directly to objects of existential types (like x.op above). This will be disallowed from now on, for the sole purpose of making our formalism easier to type-check. Instead, we shall have explicit language constructs for creating and manipulating objects of existential types, just as we had type abstractions all[t] and type applications exp[t] for creating and using objects of universal types.

An ordinary object (3, succ) may be converted to an abstract object having type Ea. a x (a -> Int) by packaging it so that some of its structure is hidden. The operation pack below encapsulates the object (3, succ) so that the user knows only that an object of the type a x (a -> Int) exists without knowing the actual object. It is natural to think of the resulting object as having the existential type Ea. a x (a -> Int):

  value p = pack [a=Int in a x (a -> Int)] (3,succ) : Ea. a x (a -> Int)

(J: abstract object means we only know the type of the object, we have the object (value), but we don't know what the object (value) is.)

Packaged objects such as p are called packages. The value (3, succ) is referred to as the content of the package. The type a X (a -> Int) is the interface: It determines the structure specification of the contents and corresponds to the specification part of a data abstraction. The binding a = Int is the type representation: It binds the abstract data type to a particular representation Int, and corresponds to the hidden data type associated with a data abstraction.

The general form of the operation pack is as follows:

  pack[a = typerep in interface] (contents)

The operation pack is the only mechanism for creating objects of an existential type. Thus, if a variable of an existential type has been declared by a declaration such as

  p : Ea.a x (a -> Int)

then p can take only values created by a pack operation.

A package must be opened before it can be used:

  open p as x in (snd(x))(lst(x))

Opening a package introduces a name x for the contents of the package that can be used in the scope following in. When the structure of x is specified by labeled components, components of the opened package may be referred to by name:

  value p = pack [a = Int in (arg:a, op:a->Int)] (3, succ)
  open p as x in x.op(x.arg)

We may also need to refer to the (unknown) type hidden by the package. For example, suppose that we wanted to apply the second component of p to a value of the abstract type supplied as an external argument. In this case the unknown type b must be explicitly referred to and the following form can be used:

  open p as x [b] in fun(y:b) (snd(x))(y)

Here the type name b is associated with the hidden representation type in the scope following in. The type of the expression following in must not contain b, to prevent b from escaping its scope.

The function of open is mostly to bind names for representation types and to help the type checker in verifying type constraints. In many situations we may want to abbreviate open p as x in x.a to p.a. We are going to avoid such abbreviations to prevent confusion, but they are perfectly admissible.

Both pack and open have no run-time effect on data. Given a smart enough type checker, one could omit these constructs and revert to the notation used in the previous section.

### Packages and Abstract Data Types ###


