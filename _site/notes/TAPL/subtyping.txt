Subtyping
=========

Subtyping is a very fundamental extension, sometimes called subtype polymorphism. Unlike the features we have studied up to now, which could be formulated more or less orthogonally to each other, subtyping is a cross-cutting extension, interacting with most other language features in non-trivial ways. Subtyping is characteristically found in object-oriented languages and is often considered an essential feature of the object-oriented style. 

Extends lambda with record ({}) and subtyping (<:)
----------------------------------------------
t ::=                          terms
      x                        variable
      λx:T.t                   abstraction
      t t                      application
      {li=ti,i<-1..n}          record
      t.l                      projection
      <l=t> (no as)            tagging

v ::=                          values
      λx:T.t                   abstraction value
      {li=vi,i<-1..n}          record value

T ::=                          types
      Top                      maximum type
      Bot                      minimum type
      T->T                     type of functions
      {li:Ti,i<-1..n}          type of records

Г ::=                          contexts
      /                        empty context
      Г,x:T                    term variable binding

Evaluation                     t -> t'

t1 -> t1'
---------------                E-APP1
t1 t2 -> t1' t2

t2 -> t2'
---------------                E-APP2
v1 t2 -> v1 t2'

(λx:T11.t12)v2 -> [x -> v2]t12 E-APPABS

{li=vi,i<-1..n}.lj -> vj       E-PROJRCD

t1 -> t1'
-------------                  E-PROJ
t1.l -> t1'.l

tj -> tj'
-----------------------------------------------------------------------------------------    E-RCD
{li=vi,i<-1..j-1,  lj=tj,  lk=tk,k<-j+1..n} -> {li=vi,i<-1..j-1, lj=tj', lk=tk,k<-j+1..n}

|- v1:T
-------------                  E-DOWNCAST
v1 as T -> v1

Subtyping                      S <: T

S <: S                         S-REFL

S <: U    U <: T
----------------               S-TRANS
S <: T

S <: Top                       S-TOP

Bot <: S                       S-BOT

T1 <: S1    S2 <: T2
--------------------           S-ARROW
S1->S2 <: T1->T2

{li:Ti,i<-1..n+k} <: {li:Ti,i<-1..n}    S-RCDWIDTH

for each i Si <: Ti
----------------------------------    S-RCDDEPTH
{li:Si,i<-1..n} <: {li:Ti,i<-1..n}

{kj:Sj,j<-1..n} is a permutation of {li:Ti,i<-1..n}
---------------------------------------------------    S-RCDPERM
{kj:Sj,j<-1..n} <: {li:Ti,i<-1..n}

<li:Ti,i<-1..k> <: <li:Ti,i<-1..n+k>    S-VARIANTWIDTH

for each i Si <: Ti
----------------------------------    S-VARIANTDEPTH
<li:Si,i<-1..n> <: <li:Ti,i<-1..n>

<kj:Sj,j<-1..n> is a permutation of <li:Ti,i<-1..n>
---------------------------------------------------    S-VARIANTPERM
<kj:Sj,j<-1..n> <: <li:Ti,i<-1..n>

Typing                         Г |- t:T

x:T belongs to Г
----------------               T-VAR
Г |- x:T

Г,x:T1 |- t2:T2
--------------------           T-ABS
Г |- λx:T1.t2:T1->T2

Г |- t1:T11->T12    Г |- t2:T11
-------------------------------    T-APP
Г |- t1 t2:T12

Г |- t:S    S <: T
------------------             T-SUB
Г |- t:T

Г |- t1:T
--------------                 T-ASCRIBE
Г |- t1 as T:T

Г |- t1:S
--------------                 T-DOWNCAST
Г |- t1 as T:T

for each i Г |- ti:Ti
------------------------------------    T-RCD
Г |- {li=ti,i<-1..n}:{li:Ti,i<-1..n}

Г |- t1:{li:Ti,i<-1..n}
-----------------------        T-PROJ
Г |- t1.lj:Tj

Г |- t1:T1
--------------------           T-VARIANT
Г |- <l1=t1>:<l1:T1>

Subsumption
-----------

The goal of subtyping is to 'loose' the typing rules. We accomplish this by formalizing the intuition that some types are more informative than others: we say that S is a subtype of T, written S <: T (T is a supertype of S), to mean that any term of type S can safely be used in a context where a term of type T is expected. This view of subtyping is often called the principle of safe substitution.

A simpler intuition is to read S <: T as "every value described by S is also described by T," that is, "the elements of S are a subset of the elements of T (T-SUB).


Subtype Relation
----------------

The subtype relation is formalized as a collection of inference rules for deriving statements of the form S <: T, We consider each form of type (function types, record types, etc.) separately; for each one, we introduce one or more rules formalizing situations when it is safe to allow elements of one type of this form to be used where another is expected.

We have two general stipulations: S-REFL (subtyping should be reflexive) and S-TRANS (subtyping should be transitive). These rules follow directly from the intuition of safe substitution.


For record types, we have

  {li:Ti,i<-1..n+k} <: {li:Ti,i<-1..n}    (S-RCDWIDTH, width subtyping)

  for each i Si <: Ti
  ----------------------------------    (S-RCDDEPTH, depth subtyping)
  {li:Si,i<-1..n} <: {li:Ti,i<-1..n}

  {kj:Sj,j<-1..n} is a permutation of {li:Ti,i<-1..n}
  ---------------------------------------------------    (S-RCDPERM)
  {kj:Sj,j<-1..n} <: {li:Ti,i<-1..n}

It may seem surprising that the "smaller" type-the subtype-is the one with more fields. The easiest way to understand this is regarding a record type {x:Nat} as describing "the set of all records with at least a field x of type Nat". So a longer record constitutes a more demanding-i.e., more informative-specification, and so describes a smaller set of values.

S-RCDPERM arises from the observation that the order of fields in a record does not make any difference to how we can safely use it, since the only thing that we can do with records once we've built them-i.e., projecting their fields-is insensitive to the order of fields.

S-RCDWIDTH, S-RCDDEPTH, and S-RCDPERM each embody a different sort of flexibility in the use of records. For purposes of discussion, it is useful to present them as three separate rules. In particular, there are languages that allow some of them but not others; for example, most variants of Abadi and Cardelli's object calculus (1996) omit width subtyping. However, for purposes of implementation it is more convenient to combine them into a single macro-rule that does all three things at once.


For function types, we have

  T1 <: S1    S2 <: T2
  --------------------    (S-ARROW)
  S1->S2 <: T1->T2

Notice that the sense of the subtype relation is reversed (contravariant) for the argument types in the left-hand premise, while it runs in the same direction (covariant) for the result types as for the function types themselves. The intuition is that, if we have a function f of type S1->S2, then we know that f accepts elements of type S1; clearly, f will also accept elements of any subtype T1 of S1. The type of f also tells us that it returns elements of type S2; we can also view these results belonging to any supertype T2 of S2. That is, any function f of type S1->S2 can also be viewed as having type T1->T2.

An alternative view is that it is safe to allow a function of one type S1->S2 to be used in a context where another type T1->T2 is expected as long as none of the arguments that may be passed to the function in this context will surprise it (T1 <: S1) and none of the results that it returns will surprise the context (S2 <: T2).


Finally, it is convenient to have a type that is a supertype of every type. We introduce a new type constant Top, plus a rule that makes Top a maximum element of the subtype relation (S-TOP). Note that the presence of the reflexivity and transitivity rules means that the subtype relation is clearly a preorder; however, because of the record permutation rule, it is not a partial order: there are many pairs of distinct types where each is a subtype of the other. The maximal type Top is not a necessary part of the simply typed lambda-calculus with subtyping; it can be removed without damaging the properties of the system. However, it is included in most presentations, for several reasons. First, it corresponds to the type Object found in most object-oriented languages. Second, Top is a convenient technical device in more sophisticated systems combining subtyping and parametric polymorphism.

It is natural to ask whether we can also complete the subtype relation with a minimal element - a type Bot that is a subtype of every type. The answer is that we can: see T-BOT and S-BOT. The first thing to notice is that Bot is empty - there are no closed values of type Bot. If there were one, say v, then the subsumption rule plus S-Bot would allow us to derive |- v:Top->Top, from which the canonical forms lemma tells us that v must have the form λx:S1.t2 for some S1 and t2. On the other hand, by subsumption, we also have |- v:{}, from which the canonical forms lemma tells us that v must be a record. The syntax makes it clear that v cannot be both a function and a record, and so assuming that |- v:Bot has led us to a contradiction. The emptiness of Bot does not make it useless. On the contrary: Bot provides a very convenient way of expressing the fact that some operations (in particular, throwing an exception or invoking a continuation) are not intended to return. Giving such expressions the type Bot has two good effects: first, it signals to the programmer that no result is expected (since if the expression did return a result, it would be a value of type Bot); second, it signals to the typechecker that such an expression can safely be used in a context expecting any type of value. For example, if the exception-raising term error is given type Bot, then a term like

   λx:T.
     if <check that x is reasonable> then
       <compute result>
     else
       error

will be well typed because, no matter what the type of the normal result is, the term error can always be given the same type by subsumption, so the two branches of the if are compatible, as required by T-If. Unfortunately, the presence of Bot significantly complicates the problem of building a typechecker for the system. A simple typechecking algorithm for a language with subtyping needs to rely on inferences like "if an application t1 t2 is well typed, then t1 must have an arrow type." In the presence of Bot, we must refine this to "if t1 t2 is well typed, then t1 must have either an arrow type or type Bot". So adding Bot is a more serious step than adding Top.

In languages with polymorphism, such as ML, we can also use "X.X as a result type for error and similar constructs. This achieves the same effect as Bot by different means: instead of giving error a type that can be promoted to any type, we give it a type scheme that can be instantiated to any type. Though they rest on different foundations, the two solutions are quite similar: in particular, the type "X.X is also empty.

Properties of Subtyping and Typing
----------------------------------

* Inversion of the Subtype Relation Lemma

  1. If S <: T1->T2, then S has the form S1->S2, with T1 <: S1 and S2 <: T2.
  2. If S <: {li:Ti,i<-1..n}, then S has the form {kj:Sj,j<-1..m}, with at least the labels {li,i<-1..n} - i.e., {li,i<-1..n} included by {kj,j<-1..m} - and with Sj <: Ti for each common label li = kj.

* Lemma

  1. If Г |- λx:S1.s2:T1->T2, then T1 <: S1 and Г, x:S1 |- s2:T2.
  2. If Г |- {ka=sa,a<-1..m}:{li:Ti,i<-1..n}, then {li,i<-1..n} included by {ka,a<-1..m} and Г |- sa:Ti for each common label ka = li.

* Substitution Lemma: If Г, x:S |- t:T and Г |- s:S, then Г |- [x|->s]t:T.

** Preservation Theorem: If Г |- t:T and t -> t′, then Г |- t′:T.

* Canonical Forms Lemma

  1. If v is a closed value of type T1->T2, then v has the form λx:S1.t2.
  2. If v is a closed value of type {li:Ti,i<-l..n}, then v has the form {kj=vj,j<-l..m}, with {li,i<-l..n} included by {kj,j<-l..m}.

** Progress Theorem: If t is a closed, well-typed term, then either t is a value or else there is some t′ with t -> t′.


Subtyping and Other Features
----------------------------

As we extend our simple calculus with subtyping toward a full-blown programming language, each new feature must be examined carefully to see how it interacts with subtyping.

* Ascription and Casting

The ascription operator t as T was introduced as a form of checked documentation, allowing the programmer to record in the text of the program the assertion that some subterm of a complex expression has some particular type. In the examples in this book, ascription is also used to control the way in which types are printed, forcing the typechecker to use a more readable abbreviated form instead of the type that it has actually calculated for a term.

In languages with subtyping such as Java and C++, ascription becomes quite a bit more interesting. It is often called casting in these languages, and is written (T)t. There are actually two quite different forms of casting - so-called up-casts and down-casts. The former are straightforward; the latter, which involve dynamic type-testing, require a significant extension.

Up-casts, in which a term is ascribed a supertype of the type that the typechecker would naturally assign it, are instances of the standard ascription operator. We give a term t and a type T at which we intend to "view" t. The typechecker verifies that T is indeed one of the types of t by attempting to build a derivation

...         ...
--------    ------
Г |- t:S    S <: T
-------------------    T-SUB
Г |- t:T
-------------------    T-ASCRIBE
Г |- t as T:T

Up-casts can be viewed as a form of abstraction - a way of hiding the existence of some parts of a value so that they cannot be used in some surrounding context. For example, if t is a record (or, more generally, an object), then we can use an up-cast to hide some of its fields (methods).

A down-cast, on the other hand, allows us to assign types to terms that the typechecker cannot derive statically. To allow down-casts, we make a somewhat surprising change to the typing rule for as T-DOWNCAST

That is, we check that t1 is well typed (i.e., that it has some type S) and then assign it type T, without making any demand about the relation between S and T. For example, using down-casting we can write a function f that takes any argument whatsoever, casts it down to a record with an a field containing a number, and returns this number:

  f = λ(x:Top) (x as {a:Nat}).a;

In effect, the programmer is saying to the typechecker, "I know (for reasons that are too complex to explain in terms of the typing rules) that f will always be applied to record arguments with numeric a fields; I want you to trust me on this one."

Of course, blindly trusting such assertions will have a disastrous effect on the safety of our language: if the programmer somehow makes a mistake and applies f to a record that does not contain an a field, the results might (depending on the details of the compiler) be completely arbitrary! Instead, our motto should be "trust, but verify." At compile time, the typechecker simply accepts the type given in the down-cast. However, it inserts a check that, at run time, will verify that the actual value does indeed have the type claimed. In other words, the evaluation rule for ascriptions should not just discard the annotation, but should first compare the actual (run-time) type of the value with the declared type (E-DOWNCAST)

For example, if we apply the function f above to the argument {a=5,b=true}, then this rule will check (successfully) that |- {a=5,b=true}:{a:Nat}. On the other hand, if we apply f to {b=true}, then the E-DOWNCAST rule will not apply and evaluation will get stuck at this point. This run-time check recovers the type preservation property.

Of course, we lose progress, since a well-typed program can certainly get stuck by attempting to evaluate a bad down-cast. Languages that provide down-casts normally address this in one of two ways: either by making a failed down-cast raise a dynamic exception that can be caught and handled by the program or else by replacing the down-cast operator by a form of dynamic type test:

Г |- t1:S    Г,x:T |- t2:U    Г |- t3:U
---------------------------------------    (T-TYPETEST)
Г |- if t1 in T then x->t2 else t3:U

|- v1:T
-------------------------------------------    (E-TYPETEST)
if v1 in T then x->t2 else t3->[x |-> v1]t2

|/- v1:T
---------------------------------    (E-TYPETEST2)
if v1 in T then x->t2 else t3->t3

Uses of down-casts are actually quite common in languages like Java. In particular, down-casts support a kind of "poor-man's polymorphism." For example, "collection classes" such as Set and List are monomorphic in Java: instead of providing a type List T (lists containing elements of type T) for every type T, Java provides just List, the type of lists whose elements belong to the maximal type Object. Since Object is a supertype of every other type of objects in Java, this means that lists may actually contain anything at all: when we want to add an element to a list, we simply use subsumption to promote its type to Object. However, when we take an element out of a list, all the typechecker knows about it is that it has type Object. This type does not warrant calling most of the methods of the object, since the type Object mentions only a few very generic methods for printing and such, which are shared by all Java objects. In order to do anything useful with it, we must first downcast it to some expected type T.

It has been argued-for example, by the designers of Pizza (Odersky and Wadler, 1997), GJ (Bracha, Odersky, Stoutamire, and Wadler, 1998), PolyJ (Myers, Bank, and Liskov, 1997), and NextGen (Cartwright and Steele, 1998)-that it is better to extend the Java type system with real polymorphism (cf. Chapter 23), which is both safer and more efficient than the down-cast idiom, requiring no run-time tests. On the other hand, such extensions add significant complexity to an already-large language, interacting with many other features of the language and type system (see Igarashi, Pierce, and Wadler, 1999, Igarashi, Pierce, and Wadler, 2001, for example); this fact supports a view that the down-cast idiom offers a reasonable pragmatic compromise between safety and complexity.

Down-casts also play a critical role in Java's facilities for reflection. Using reflection, the programmer can tell the Java run-time system to dynamically load a bytecode file and create an instance of some class that it contains. Clearly, there is no way that the typechecker can statically predict the shape of the class that will be loaded at this point (the bytecode file can be obtained on demand from across the net, for example), so the best it can do is to assign the maximal type Object to the newly created instance. Again, in order to do anything useful, we must downcast the new object to some expected type T, handle the run-time exception that may result if the class provided by the bytecode file does not actually match this type, and then go ahead and use it with type T.

To close the discussion of down-casts, a note about implementation is in order. It seems, from the rules we have given, that including down-casts to a language involves adding all the machinery for typechecking to the run-time system. Worse, since values are typically represented differently at run time than inside the compiler (in particular, functions are compiled into byte-codes or native machine instructions), it appears that we will need to write a different typechecker for calculating the types needed in dynamic checks. To avoid this, real languages combine down-casts with type tags-single-word tags (similar in some ways to ML's datatype constructors and the variant tags) that capture a run-time "residue" of compile-time types and that are sufficient to perform dynamic subtype tests.

* Variants

The subtyping rules for variants are nearly identical to the ones for records; the only difference is that the width rule S-VARIANTWIDTH allows new variants to be added, not dropped, when moving from a subtype to a supertype. The intuition is that a tagged expression <l=t> belongs to a variant type <li:Ti,i<-1..n> if its label l is one of the possible labels {Ti} listed in the type; adding more labels to this set decreases the information it gives us about its elements. A singleton variant type <l1:T1> tells us precisely what label its elements are tagged with; a two-variant type <l1:T1 , l2:T2> tells us that its elements have either label l1 or label l2, etc. Conversely, when we use variant values, it is always in the context of a case statement, which must have one branch for each variant listed by the type-listing more variants just means forcing case statements to include some unnecessary extra branches.

Another consequence of combining subtyping and variants is that we can drop the annotation from the tagging construct, writing just <l=t> instead of <l=t> as <li:Ti,i<-1..n>, and changing the typing rule for tagging so that it assigns <l1=t1> the precise type <l1:T1>. We can then use subsumption plus S-VARIANTWIDTH to obtain any larger variant type.

* List

We have seen a number of examples of covariant type constructors (records and variants, as well as function types, on their right-hand sides) and one contravariant constructor (arrow, on the left-hand side). The List constructor is also covariant: if we have a list whose elements have type S1, and S1 <: T1, then we can safely regard our list as having elements of type T1.

S1 <: T1
------------------    S-LIST
List S1 <: List T1

* References

Not all type constructors are covariant or contravariant. The Ref constructor, for example, must be taken to be invariant in order to preserve type safety.

S1 <: T1    T1 <: S1
--------------------    S-REF
Ref S1 <: Ref T1

For Ref S1 to be a subtype of Ref T1, we demand that S1 and T1 be equivalent under the subtype relation-each a subtype of the other. This gives us the flexibility to reorder the fields of records under a Ref constructor-for example, Ref {a:Bool,b:Nat} <: Ref {b:Nat,a:Bool} - but nothing more.

The reason for this very restrictive subtyping rule is that a value of type Ref T1 can be used in a given context in two different ways: for both reading (!) and writing (:=). When it is used for reading, the context expects to obtain a value of type T1, so if the reference actually yields a value of type S1 then we need S1 <: T1 to avoid violating the context's expectations. On the other hand, if the same reference cell is used for writing, then the new value provided by the context will have type T1. If the actual type of the reference is Ref S1, then someone else may later read this value and use it as an S1; this will be safe only if T1 <: S1.

A more refined analysis of references, first explored by Reynolds in the language Forsythe (1988), can be obtained by introducing two new type constructors, Source and Sink. Intuitively, Source T is thought of as a capability to read values of type T from a cell (but which does not permit assignment), while Sink T is a capability to write to a cell. Ref T is a combination of these two capabilities, giving permission both to read and to write.

The typing rules for dereferencing and assignment are modified so that they demand only the appropriate capability.

Г|E |- t1:Source T11
----------------------    T-DEREF
Г|E |- !t1:T11

Г|E |- t1:Sink T11    Г|E |- t2:T11
-----------------------------------    T-ASSIGN
Г|E |- t1:=t2:Unit

Now, if we have only the capability to read values from a cell and if these values are guaranteed to have type S1, then it is safe to "downgrade" this to a capability to read values of type T1, as long as S1 is a subtype of T1. That is, the Source constructor is covariant.

S1 <: T1
----------------------    S-SOURCE
Source S1 <: Source T1

Conversely, a capability to write values of type S1 to a given cell can be downgraded to a capability to write values of some smaller type T1: the Sink constructor is contravariant.

T1 <: S1
------------------    S-SINK
Sink S1 <: Sink T1

Finally, we express the intuition that Ref T1 is a combination of read and write capabilities by two subtyping rules that permit a Ref to be downgraded to either a Source or a Sink.

Ref T1 <: Source T1   S-REFSOURCE
Ref T1 <: Sink T1     S-REFSINK

* Arrays

Clearly, the motivations behind the invariant subtyping rule for references also apply to arrays, since the operations on arrays include forms of both dereferencing and assignment.

S1 <: T1    T1 <: S1
--------------------    S-ARRAY
Array S1 <: Array T1

Interestingly, Java actually permits covariant subtyping of arrays:

S1 <: T1
--------------------    S-ARRAYJAVA
Array S1 <: Array T1

(in Java syntax, S1[] <: T1[]). This feature was originally introduced to compensate for the lack of parametric polymorphism in the typing of some basic operations such as copying parts of arrays, but is now generally considered a flaw in the language design, since it seriously affects the performance of programs involving arrays. The reason is that the unsound subtyping rule must be compensated with a run-time check on every assignment to any array, to make sure the value being written belongs to (a subtype of) the actual type of the elements of the array.

* Channels

The same intuitions (and identical subtyping rules) form the basis for the treatment of channel types in recent concurrent programming languages such as Pict (Pierce and Turner, 2000; Pierce and Sangiorgi, 1993). The key observation is that, from the point of view of typing, a communication channel behaves exactly like a reference cell: it can be used for both reading and writing, and, since it is difficult to determine statically which reads correspond to which writes, the only simple way to ensure type safety is to require that all the values passed along the channel must belong to the same type. Now, if we pass someone only the capability to write to a given channel, then it is safe for them to pass this capability to someone else who promises to write values of a smaller type - the "output channel" type constructor is contravariant. Similarly, if we pass just the capability to read from a channel, then this capability can safely be downgraded to a capability for reading values of any larger type - the "input channel" constructor is covariant.

* Base Types

In a full-blown language with a rich set of base types, it is often convenient to introduce primitive subtype relations among these types. For example, in many languages the boolean values true and false are actually represented by the numbers 1 and 0. We can, if we like, expose this fact to the programmer by introducing a subtyping axiom Bool <: Nat. Now we can write compact expressions like 5*b instead of if b then 5 else 0.


Problems with the Subset Semantics
----------------------------------

Throughout this chapter, our intuition has been that subtyping is "semantically insignificant." The presence of subtyping does not change the way programs are evaluated; rather, subtyping is just a way of obtaining additional flexibility in typing terms. This interpretation is simple and natural, but it carries some performance penalties-particularly for numerical calculations and for accessing record fields - that may not be acceptable in high-performance implementations.


As we saw in above section, it can be convenient to allow subtyping between different base types. But some "intuitively reasonable" inclusions between base types may have a detrimental effect on performance. For example, suppose we introduce the axiom Int <: Float, so that integers can be used in floating-point calculations without writing explicit coercions-allowing us to write, for example, 4.5 + 6 instead of 4.5 + intToFloat(6). Under the subset semantics, this implies that the set of integer values must literally be a subset of the set of floats. But, on most real machines, the concrete representations of integers and floats are entirely different: integers are usually represented in twos-complement form, while floats are divided up into mantissa, exponent, and sign, plus some special cases such as NaN (not-a-number).

To reconcile these representational differences with the subset semantics of subtyping, we can adopt a common tagged (or boxed) representation for numbers: an integer is represented as a machine integer plus a tag (either in a separate header word or in the high-order bits of the same word as the actual integer), and a float is represented as a machine float plus a different tag. The type Float then refers to the entire set of tagged numbers (floats and ints), while Int refers just to the tagged ints.

This scheme is not unreasonable: it corresponds to the representation strategy actually used in many modern language implementations, where the tag bits (or words) are also needed to support garbage collection. The downside is that every primitive operation on numbers must actually be implemented as a tag check on the arguments, a few instructions to unbox the primitive numbers, one instruction for the actual operation, and a couple of instructions for re-boxing the result. Clever compiler optimizations can eliminate some of this overhead, but, even with the best currently available techniques, it significantly degrades performance, especially in heavily numeric code such as graphical and scientific calculations.

A different performance problem arises when records are combined with subtyping-in particular, with the permutation rule. Our simple evaluation rule for field projection E-PROJRCD can be read as "search for lj among the labels of the record, and yield the associated value vj." But, in a real implementation, we certainly do not want to perform a linear search at run time through the fields of the record to find the desired label. In a language without subtyping (or with subtyping but without the permutation rule), we can do much better: if the label lj appears third in the type of the record, then we know statically that all run-time values with this type will have lj as their third field, so at run time we do not need to look at the labels at all (in fact, we can omit them completely from the run-time representation, effectively compiling records into tuples). To obtain the value of the lj field, we generate an indirect load through a register pointing to the start of the record, with a constant offset of 3 words. The presence of the permutation rule foils this technique, since knowing that some record value belongs to a type where lj appears as the third field tells us nothing at all, now, about where the lj field is actually stored in the record. Again, clever optimizations and run-time tricks can palliate this problem, but in general field projection can require some form of search at run time.

Similar observations apply to accessing fields and methods of objects, in languages where object subtyping allows permutation. This is the reason that Java, for example, restricts subtyping between classes so that new fields can only be added at the end. Subtyping between interfaces (and between classes and interfaces) does allow permutation - if it did not, interfaces would be of hardly any use - and the manual explicitly warns that looking up a method from an interface will in general be slower than from a class. (I think the author is talking about no multi inheritance allowed for classes here)


Coercion Semantics for Subtyping
--------------------------------

We can address both of these problems by adopting a different semantics, in which we "compile away" subtyping by replacing it with run-time coercions. If an Int is promoted to a Float during typechecking, for example, then at run time we physically change this number's representation from a machine integer to a machine float. Similarly, a use of the record permutation subtyping rule will be compiled into a piece of code that literally rearranges the order of the fields. Primitive numeric operations and field accesses can now proceed without the overhead of unboxing or search.

Intuitively, the coercion semantics for a language with subtyping is expressed as a function that transforms terms from this language into a lower-level language without subtyping. Ultimately, the low-level language might be machine code for some concrete processor. For purposes of illustration, however, we can keep the discussion on a more abstract level. For the source language, we choose the one we have been using for most of the chapter-the simply typed lambda-calculus with subtyping and records. For the low-level target language, we choose the pure simply typed lambda-calculus with records and a Unit type (which we use to interpret Top).

Formally, the compilation consists of three translation functions-one for types, one for subtyping, and one for typing. For types, the translation just replaces Top with Unit. We write this function as〚-〛.

  〚Top〛 = Unit
  〚T1->T2〛 = 〚T1〛->〚T2〛
  〚{li:Ti,i<-1..n}〛= {li:〚Ti〛i<-1..n}

For example, 〚Top->{a:Top,b:Top}〛 = Unit->{a:Unit,b:Unit}. (The other translations will also be written 〚-〛; the context will always make it clear which one we are talking about.)

To translate a term, we need to know where subsumption is used in typechecking it, since these are the places where run-time coercions will be inserted. One convenient way of formalizing this observation is to give the translation as a function on derivations of typing statements. Similarly, to generate a coercion function transforming values of type S to type T, we need to know not just that S is a subtype of T, but also why. We accomplish this by generating coercions from subtyping derivations.

A little notation for naming derivations is needed to formalize the translations. Write C :: S <: T to mean "C is a subtyping derivation tree whose conclusion is S <: T." Similarly, write D :: Г |- t:T to mean "D is a typing derivation whose conclusion is Г |- t:T.

Let us look first at the function that, given a derivation C for the subtyping statement S <: T, generates a coercion 〚C〛. This coercion is nothing but a function (in the target language of the translation) from type 〚S〛 to type 〚T〛. The definition goes by cases on the final rule used in C.

〚------〛 = λx:〚T〛.x
  T <: T

〚--------〛 = λx:〚S〛.unit
  S <: Top

  C1 :: S <: U    C2 :: U <: T
〚----------------------------〛 = λx:〚S〛.〚C2〛(〚C1〛x)
  S <: T

  C1 :: T1 <: S1    C2 :: S2 <: T2
〚--------------------------------〛 = λf:〚S1->S2〛.λx:〚T1〛.〚C2〛(f(〚C1〛x))
  S1->S2 <: T1->T2

〚------------------------------------〛 = λr:{li:〚Ti〛,i<-1..n+k}.{li=r.li,i<-1..n}
  {li:Ti,i<-1..n+k} <: {li:Ti,i<-1..n}

  for each i    Ci :: Si <: Ti
〚----------------------------------〛 = λr:{li:〚Si〛,i<1..n}.{li=〚Ci〛(r.li),i<-1..n}
  {li:Si,i<-1..n} <: {li:Ti,i<-1..n}

  {kj:Sj,j<-1..n} perm.of {li:Ti,i<-1..n}
〚---------------------------------------〛 = λr:{kj:〚Sj〛,j<-1..n}.{li=r.li,i<-1..n}
  {kj:Sj,j<-1..n} <: {li:Ti,i<-1..n}

Lemma: If C :: S <: T, then |- 〚C〛 : 〚S〛->〚T〛

Typing derivations are translated in a similar way. If D is a derivation of the statement Г |- t:T, then its translation 〚D〛 is a target-language term of type 〚T〛. This translation function is often called the Penn translation, after the group at the University of Pennsylvania that first studied it (Breazu-Tannen, Coquand, Gunter, and Scedrov, 1991).

  x:T <- Г
〚--------〛= x
  Г |- x:T

  D2 :: Г,x:T1 |- t2:T2
〚---------------------〛= λx:〚T1〛.〚D2〛
  Г |- λx:T1:T1->T2

  D1 :: Г |- t1::T11->T12    D2 :: Г |- t2:T11
〚--------------------------------------------〛=〚D1〛〚D2〛
  Г |- t1 t2:T12

  for each i Di :: Г |- ti:Ti
〚------------------------------------〛= {li=〚Di〛,i<-1..n}
  Г |- {li=ti,i<-1..n}:{li:Ti,i<-1..n}

  D1 :: Г |- t1:{li:Ti,i<-1..n}
〚-----------------------------〛=〚D1〛.lj
  Г |- t1.lj:Tj

  D :: Г |- t:S    C :: S <: T
〚----------------------------〛=〚C〛〚D〛
  Г |- t:T

theorem: If D :: Г |- t : T, then 〚Г〛 |- 〚D〛:〚T〛, where 〚Г〛 is the point-wise extension of the type translation to contexts: 〚ø〛 = ø and 〚Г, x:T〛 = 〚Г〛, x:〚T〛.

Having defined these translations, we can drop the evaluation rules for the high-level language with subtyping, and instead evaluate terms by typechecking them (using the high-level typing and subtyping rules), translating their typing derivations to the low-level target language, and then using the evaluation relation of this language to obtain their operational behavior. This strategy is actually used in some high-performance implementations of languages with subtyping, such as the Yale compiler group's experimental Java compiler (League, Shao, and Trifonov, 1999; League, Trifonov, and Shao, 2001).


Coherence
---------

When we give a coercion semantics for a language with subtyping, there is a potential pitfall that we need to be careful to avoid. Suppose, for example, that we extend the present language with the base types Int, Bool, Float, and String. The following primitive coercions might all be useful:

  〚Bool <: Int〛 = λb:Bool.if b then 1 else 0
  〚Int <: String〛 = intToString
  〚Bool <: Float〛 = λb:Bool.if b then 1.0 else 0.0
  〚Float <: String〛 = floatToString

The functions intToString and floatToString are primitives that construct string representations of numbers. For the sake of the example, suppose that intToString(1) = "1", while floatToString(1.0) = "1.000". Now, suppose we are asked to evaluate the term (λx:String.x)true using the coercion semantics. This term is typable, given the axioms above for the primitive types. In fact, it is typable in two distinct ways: we can either use subsumption to promote Bool to Int and then to String, to show that true is an appropriate argument to a function of type String->String, or we can promote Bool to Float and then to String. But if we translate these derivations into λ->, we get different behaviors. If we coerce true to type Int, we get 1, from which intToString yields the string "1". But if we instead coerce true to a float and then, using floatToString, to a String (following the structure of a typing derivation in which true : String is proved by going via Float), we obtain "1.000". But "1" and "1.000" are very different strings: they do not even have the same length. In other words, the choice of how to prove |- (λx:String. x)true:String affects the way the translated program behaves! But this choice is completely internal to the compiler-the programmer writes only terms, not derivations-so we have designed a language in which programmers cannot control or even predict the behavior of the programs they write.

The appropriate response to such problems is to impose an additional requirement, called coherence, on the definition of the translation functions: A translation 〚-〛 from typing derivations in one language to terms in another is coherent if, for every pair of derivations D1 and D2 with the same conclusion Г |- t:T, the translations 〚D1〛 and 〚D2〛 are behaviorally equivalent terms of the target language.

In particular, the translations given above (with no base types) are coherent. To recover coherence when we consider base types (with the axioms above), it suffices to change the definition of the floatToString primitive so that floatToString(0.0) = "0" and floatToString(1.0) = "1".


Intersection and Union Types
----------------------------

The inhabitants of the intersection type T1∧T2 are terms belonging to both S and T-that is, T1∧T2 is an order-theoretic meet (greatest lower bound) of T1 and T2.

  T1∧T2 <: T1    (S-INTER1)

  T1∧T2 <: T2    (S-INTER2)

  S <: T1    S <: T2
  ------------------    (S-INTER3)
  S <: T1∧T2

  S->T1∧S->T2 <: S->(T1∧T2)    (S-INTER4)

The intuition behind this rule is that, if we know a term has the function types S→T1 and S→T2, then we can certainly pass it an S and expect to get back both a T1 and a T2.

The power of intersection types is illustrated by the fact that, in a call-by-name variant of the simply typed lambda-calculus with subtyping and intersections, the set of untyped lambda-terms that can be assigned types is exactly the set of normalizing terms - i.e., a term is typable iff its evaluation terminates! This immediately implies that the type reconstruction problem for calculi with intersections is undecidable.

More pragmatically, the interest of intersection types is that they support a form of finitary overloading. For example, we might assign the type (Nat->Nat->Nat)∧(Float->Float->Float) to an addition operator that can be used on both natural numbers and floats (using tag bits in the runtime representation of its arguments, for example, to select the correct instruction).

Unfortunately, the power of intersection types raises some difficult pragmatic issues for language designers. So far, only one full-scale language, Forsythe (Reynolds, 1988), has included intersections in their most general form. A restricted form known as refinement types may prove more manageable (Freeman and Pfenning, 1991; Pfenning, 1993b; Davies, 1997).

The dual notion of union types, T1VT2, also turns out to be quite useful. Unlike sum and variant types (which, confusingly, are sometimes also called "unions"), T1VT2 denotes the ordinary union of the set of values belonging to T1 and the set of values belonging to T2, with no added tag to identify the origin of a given element. Thus, NatVNat is actually just another name for Nat. Non-disjoint union types have long played an important role in program analysis (Palsberg and Pavlopoulou, 1998), but have featured in few programming languages (notably Algol 68; cf. van Wijngaarden et al., 1975); recently, though, they are increasingly being applied in the context of type systems for processing of "semistructured" database formats such as XML (Buneman and Pierce, 1998; Hosoya, Vouillon, and Pierce, 2001).

The main formal difference between disjoint and non-disjoint union types is that the latter lack any kind of case construct: if we know only that a value v has type T1 V T2, then the only operations we can safely perform on v are ones that make sense for both T1 and T2. (For example, if T1 and T2 are records, it makes sense to project v on their common fields.) The untagged union type in C is a source of type safety violations precisely because it ignores this restriction, allowing any operation on an element of T1 V T2 that makes sense for either T1 or T2.


o
The idea of subtyping in programming languages goes back to the 1960s, in Simula (Birtwistle, Dahl, Myhrhaug, and Nygaard, 1979) and its relatives. The first formal treatments are due to Reynolds (1980) and Cardelli (1984).

The typing and-especially-subtyping rules dealing with records are somewhat heavier than most of the other rules we have seen, involving either variable numbers of premises (one for each field) or additional mechanisms like permutations on the indices of fields. There are many other ways of writing these rules, but all either suffer from similar complexity or else avoid it by introducing informal conventions. An alternative treatment of records based on row-variable polymorphism has been developed by (Wand 1987, 1988, 1989b), Rémy (1990, 1989, 1992), and others, and forms the basis for the object-oriented features of OCaml (Rémy and Vouillon, 1998; Vouillon, 2000).

The fundamental problem addressed by a type theory is to insure that programs have meaning. The fundamental problem caused by a type theory is that meaningful programs may not have meanings ascribed to them. The quest for richer type systems results from this tension.
  - Mark Mannasse
