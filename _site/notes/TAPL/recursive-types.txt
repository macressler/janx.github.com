Recursive Types
===============

List, queue, binary tree, label tree, abstract syntax tree etc. are representations of a large class of common structures that may grow to arbitrary size, but that have a simple, regular structure.

Clearly, it does not make sense to provide every one of these structures as a separate, primitive language feature. Instead, we need a general mechanism with which they can be defined from simpler elements, as needed. This mechanism is called recursive types.

We can consider a list of numbers as:

  NatList = <nil:Unit, cons:{Nat,NatList}>

it is convenient to make this equation into a proper definition by moving the "loop" over to the right-hand side of the =. We do this by introducing an explicit recursion operator μ for types:

  NatList = μX. <nil:Unit, cons:{Nat,X}>

Intuitively, this definition is read, "Let NatList be the infinite type satisfying the equation X = <nil:Unit, cons:{Nat,X}>. There are actually two somewhat different ways of formalizing recursive types - the so-called equi-recursive and iso-recursive presentations - differing in the amount of help that the programmer is expected to give to the typechecker in the form of type annotations. 

To define a List, we need nil, cons, isnil, hd and tl. We've defined these as built-in operations before, now we'll define them from simper parts:

    nil = <nil=unit> as NatList;

    cons = λn:Nat. λl:NatList. <cons={n,l}> as NatList;

    isnil = λl:NatList. case l of
                        <nil=u> => true
                      | <cons=p> => false;

    hd = λl:NatList. case l of <nil=u> => 0 | <cons=p> => p.1;

    tl = λl:NatList. case l of <nil=u> => l | <cons=p> => p.2;

We've arbitrarily decided to define hd of an empty list to be 0 and tl of the empty list to be the empty list. We might alternatively have raised exceptions in these cases.


Hungry
------

Another example illustrating a somewhat trickier use of recursive types is a type of "hungry functions" that can accept any number of numeric arguments and always return a new function that is hungry for more:

  Hungry = μA. Nat->A;

An element of this type can be defined using the fix operator:

    f = fix (λf: Nat->Hungry. λn:Nat. f);
    > f : Hungry

    f 0 1 2 3 4 5;
    > <fun> : Hungry


Stream
------

A more useful variant of the Hungry type above is the type Stream of functions that can consume an arbitrary number of unit values, each time returning a pair of a number and a new stream.

(Jan: Stream is the opposite side of Hungry. Hungry is like a blackhole while Stream is a whitehole, or you can see them as /dev/null and /dev/urandom)

  Stream = μA. Unit->{Nat,A};

We can define two "destructors" for streams; if s is a stream, then hd s is the first number it returns when we pass it unit.

  hd = λs:Stream. (s unit).1;
  > hd : Stream -> Nat

Similarly, tl s is the new stream that we obtain when we pass unit to s.

  tl = λs:Stream. (s unit).2;
  > tl : Stream -> Stream

To construct a stream, we use fix as above:

  upfrom0 = fix (λf: Nat->Stream. λn:Nat. λ_:Unit. {n,f (succ n)}) 0;
  > upfrom0 : Stream

  hd upfrom0;
  > 0 : Nat

  hd (tl (tl (tl upfrom0)));
  > 3 : Nat


Process
-------

Streams can be further generalized to a simple form of processes-functions that accept a number and return a number and a new process.

(Jan: Process is some like Hungry+Stream, or Continuation)

  Process = μA. Nat->{Nat,A};

For example, here is a process that, at each step, returns the sum of all the numbers it has been given so far:

  p = fix (λf: Nat->Process. λacc:Nat. λn:Nat.
                             let newacc = plus acc n in
                             {newacc, f newacc}) 0;
  > p : Process

As we did for streams, we can define auxiliary functions for interacting with processes:

  curr = λs:Process. (s 0).1;
  > curr : Process -> Nat

  send = λn:Nat. λs:Process. (s n).2;
  > send : Nat -> Process -> Process

If we send the process p the numbers 5, 3, and 20, the number it returns in response to the last interaction is 28.

  curr (send 20 (send 3 (send 5 p)));
  > 28 : Nat


Objects
-------

A slight rearrangement of the last example gives us another familiar idiom of interacting with data: objects. For instance, here is the type of counter objects that keep track of a number and allow us to either query or increment it:

  Counter = μC. {get:Nat, inc:Unit->C};

Note that our treatment of objects here is purely functional: sending a counter object the inc message does not cause this object to mutate its state internally; instead, the operation returns a new counter object with incremented internal state. The use of recursive types here allows us to specify that the returned object has exactly the same type as the original.

The only difference between these objects and the processes discussed above is that an object is a recursively defined record (containing a function), whereas a process was a recursively defined function (returning a tuple). The reason this change in point of view is useful is that we can extend our record to include more than one function-for example, a decrement operation:

  Counter = μC. {get:Nat, inc:Unit->C, dec:Unit->C};

To create a counter object, we use the fixed-point combinator, as we did above.

  c = let create = fix (λf: {x:Nat}->Counter. λs: {x:Nat}.
                                              {get = s.x, inc = λ_:Unit. f {x=succ(s.x)}, dec = λ_:Unit. f {x=pred(s.x)} })
      in create {x=0};
  > c : Counter


Recursive Values From Recursive Types
-------------------------------------

A more surprising use of recursive types-and one that clearly reveals their expressive power-is a well-typed implementation of the fixed-point combinator. For any type T, we can define a fixed-point constructor for functions on T as follows.

  fixT  = λf:T->T. (λx:(μA.A->T). f (x x)) (λx:(μA.A->T). f (x x));
  > fixT : (T->T) -> T

Note that, if we erase types, this term is precisely the untyped fixed point combinator that we saw before.

The key trick here is using a recursive type to type the two occurrences of the subexpression x x. As we observed before, typing this term requires that x have an arrow type whose domain is the type of x itself. Clearly, there is no finite type with this property, but the infinite type μA.A->T does the job perfectly.

A corollary of this example is that the presence of recursive types breaks the strong normalization property: we can use the fixT combinator to write a well-typed term whose evaluation (when applied to unit) will diverge.

  divergeT = λ_:Unit. fixT (λx:T. x);
  > divergeT : Unit -> T

Moreover, since we can can obtain such terms for every type, it follows that every type in this system is inhabited.


Untyped Lambda-Calculus, Redux
------------------------------

Perhaps the best illustration of the power of recursive types is the fact that we can embed the whole untyped lambda-calculus-in a well-typed way-into a statically typed language with recursive types. This fact makes systems with recursive types useless as logics: if we interpret types as logical propositions following the Curry-Howard correspondence and read "type T is inhabited" as "proposition T is provable," then the fact that every type is inhabited means that every proposition in the logic is provable-that is, the logic is inconsistent.

Let D be the following type:

  D = μX.X->X;

Define an "injection function" lam mapping functions from D to D into elements of D as follows:

  lam = λf:D->D. f as D;
  > lam : D

To apply one element of D to another, we simply unfold the type of the first, yielding a function, and apply this to the second:

  ap = λf:D. λa:D. f a;
  > ap : D

Now, suppose M is a closed lambda-term involving just variables, abstractions, and applications. Then we can construct an element of D representing M, written M*, in a uniform way as follows:

  x*        =  x
  (λx.M)*   =  lam (λx:D. M*)
  (MN)*     =  ap M* N*

For example, here is the untyped fixed point combinator expressed as an element of D:

  fixD = lam (λf:D. ap (lam (λx:D. ap f (ap x x))) (lam (λx:D. ap f (ap x x))));
  > fixD : D

This embedding of the pure lambda-calculus can be extended to include features such as numbers. We change the definition of D to a variant type with one tag for numbers and one for functions:

  D = μX. <nat:Nat, fn:X->X>;

That is, an element of D is either a number or a function from D to D, tagged nat or fn, respectively. The implementation of the lam constructor is essentially the same as before:

  lam = λf:D->D. <fn=f> as D;
  > lam : (D->D) -> D

The implementation of ap, though, is different in an interesting way:

  ap = λf:D. λa:D.  case f of
                    <nat=n> => divergeD unit
                  | <fn=f> => f a;
  > ap : D -> D -> D

Before we can apply f to a, we need to extract a function from f with a case. This forces us to specify how application behaves when f is not a function. (In this example, we just diverge; we could also raise an exception.) Note how closely the tag-checking here resembles the run-time tag checking in an implementation of a dynamically typed language such as Scheme. In this sense, typed computation may be said to "include" untyped or dynamically typed computation.

Similar tag checking is needed in order to define the successor function on elements of D:

  suc = λf:D. case f of
              <nat=n> => (<nat=succ n> as D)
            | <fn=f> => divergeD unit;
  > suc : D -> D

The injection of 0 into D is trivial:

  zro = <nat=0> as D;
  >  zro : D


Formalities
-----------

There're two basic approaches to recursive types. The essential difference between them is captured in their response to a simple question: What is the relation between the type μX.T and its one-step unfolding? For example, what is the relation between NatList and <nil:Unit,cons:{Nat,NatList}>?

1. The equi-recursive approach takes these two type expressions as definitionally equal—interchangeable in all contexts - since they stand for the same infinite tree. It is the typechecker's responsibility to make sure that a term of one type will be allowed as an argument to a function expecting the other, etc.

The pleasant thing about the equi-recursive treatment is that allowing type expressions to be infinite is the only alteration to the declarative presentations of the systems we already understand. Existing definitions, safety theorems, and proofs remain unchanged, as long as they do not depend on induction on type expressions (which naturally no longer works).

Of course, the implementation of equi-recursive types requires some work, since typechecking algorithms cannot work directly with infinite structures.

2. The iso-recursive approach, on the other hand, takes a recursive type and its unfolding as different, but isomorphic.

Formally, the unfolding of a recursive type μX.T is the type obtained by taking the body T and replacing all occurrences of X by the whole recursive type - i.e., using the standard notation for substitution, it is [X |-> (μX.T)]T. For example, the type NatList, i.e.,

  μX.<nil:Unit,cons:{Nat,X}>,

unfolds to

  <nil:Unit, cons:{Nat, μX.<nil:Unit,cons:{Nat,X}>}>.

In a system with iso-recursive types, we introduce, for each recursive type μX.T, a pair of functions

  unfold[μX.T]   :  μX.T -> [X |-> μX.T]T
  fold[μX.T]     :  [X |-> μX.T]T -> μX.T

that "witness the isomorphism" by mapping values back and forth between the two types. The fold and unfold maps are provided as primitives by the language. The fact that they form an isomorphism is captured by the evaluation rule E-UNFLDFLD, which annihilates a fold when it meets a corresponding unfold. (The evaluation rule does not require the type annotations on the fold and the unfold to be the same, since we would have to invoke the typechecker at run time to verify such a constraint. However, in the evaluation of a well-typed program, these two type annotations will be equal whenever E-UNFLDFLD is applied.)

μ                               extends lambda
----------------------------------------------
t ::= ...                       terms
      fold[T] t                 folding
      unfold[T] t               unfolding

v ::= ...                       values
      fold[T] v                 folding

T ::= ...                       types
      X                         type variable
      μX.T                      recursive type

unfold[S] (fold[T] v1) -> v1    E-UNFLDFLD

t1 -> t1'
-------------------------       E-FLD
fold[T] t1 -> fold[T] t1'

t1 -> t1'
-----------------------------   E-UNFLD
unfold[T] t1 -> unfold[T] t1'

U = μX.T1    Г |- t1:[X |-> U]T1
--------------------------------    T-FLD
Г |- fold[U] t1:U

U = μX.T1    Г |- t1:U
-----------------------------   T-UNFLD
Г |- unfold[U] t1:[X |-> U]T1

Both approaches are widely used in both theoretical studies and programming language designs. The equi-recursive style is arguably more intuitive, but places stronger demands on the typechecker, which must effectively infer the points where fold and unfold annotations should occur. Moreover, the interactions between equi-recursive types and other advanced typing features such as bounded quantification and type operators can be quite complex, leading to significant theoretical difficulties or even undecidable typechecking problems.

The iso-recursive style is notationally somewhat heavier, requiring programs to be decorated with fold and unfold instructions wherever recursive types are used. In practice, however, these annotations can often be "hidden" by coalescing them with other annotations. In languages in the ML family, for example, every datatype definition implicitly introduces a recursive type. Each use of one of the constructors to build a value of the datatype implicitly includes a fold, and each constructor appearing in a pattern match implicitly forces an unfold. Similarly, in Java each class definition implicitly introduces a recursive type, and invoking a method on an object involves an implicit unfold. This felicitous overlap of mechanisms makes the iso-recursive style quite palatable in practice.

For example, here is the NatList example in iso-recursive form. First, it is convenient to define an abbreviation for the unfolded form of NatList:

  NLBody = <nil:Unit, cons:{Nat,NatList}>;

Now, nil is defined by building a variant, of type NLBody, and then folding it up as a NatList; cons is similar.

  nil = fold [NatList] (<nil=unit> as NLBody);
  cons = λn:Nat. λl:NatList. fold [NatList] <cons={n,l}> as NLBody;

Conversely, the definitions of the isnil, hd, and tl operations need to take a NatList and consider it as a variant so that they can perform a case analysis on its tag. This is achieved by unfolding the argument l:

  isnil = λl:NatList. case unfold [NatList] l of
                           <nil=u> => true
                         | <cons=p> => false;

  hd = λl:NatList. case unfold [NatList] l of
                        <nil=u> => 0
                      | <cons=p> => p.1;

  tl = λl:NatList. case unfold [NatList] l of
                        <nil=u> => l
                      | <cons=p> => p.2;


Subtyping
---------

The final question that we need to address in this chapter concerns the combination of recursive types with the other major refinement of the simply typed lambda-calculus that we have seen so far-subtyping. For example, supposing that the type Even is a subtype of Nat, what should be the relation between the types μX.Nat->(Even × X) and μX.Even->(Nat × X)? (× is type of Pair, see simple-type-extensions.txt)

The simplest way to think through such questions is to view them "in the limit"-i.e., using an equi-recursive treatment of recursive types. In the present example, the elements inhabiting both types can be thought of as simple reactive processes (see above, processes is CPS): given a number, they return another number plus a new process that is ready to receive a number, and so on. So those two type become:

  f1:Nat->{Even, μX.Nat->(Even × X)}
  f2:Even->{Nat, μX.Even->(Nat × X)}

Processes belonging to the first type always yield even numbers and are capable of accepting arbitrary numbers. Those belonging to the second type yield arbitrary numbers, but expect always to be given even numbers. The constraints both on what arguments the function must accept and on what results it may return are more demanding for the first type, so intuitively we expect the first to be a subtype of the second. 
