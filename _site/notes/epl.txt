Type Theory
-----------

  T f (S x) { B; return y; }

The declaration introduces a function called f that takes an argument, x of type S, performs calculations indicated as B, and returns a result, y of type T . If there is a type theory associated with the language, we should be able to prove a theorem of the following form:

  If x has type S then the evaluation of f (x) yields a value of type T .

The reasoning used in the proof is based on the syntactic form of the function body, B. If we can do this for all legal programs in a language L, we say that L is statically typed . If L is indeed statically typed:

  A compiler for L can check the type correctness of all programs.
  A program that is type-correct will not fail because of a type error when it is executed.

A program that is type-correct may nevertheless fail in various ways. Type checking does not usually detect errors such as division by zero. (In general, most type checking systems assume that functions are total : a function with type S -> T , given a value of type S, will return a value of type T. Some commonly used functions are partial : for example x/y ≡ divide(x,y) is undefined for y = 0.) A program can be type-correct and yet give completely incorrect answers. In general:

  Most PLs are not statically typed in the sense defined above, although the number of loopholes in the type system may be small.
  A type-correct program may give incorrect answers.
  Type theories for modern PLs, with objects, classes, inheritance, overloading, genericity, dynamic binding, and other features, are very complex.

Type theory leads to attractive and interesting mathematics and, consequently, tends to be overvalued by the theoretical Computer Science community. The theory of program correctness is more difficult and has attracted less attention, although it is arguably more important.


Regular Languages
-----------------

In the formal study of regular languages, we begin with an alphabet, Σ, which is a finite set of symbols. For example, we might have Σ = { 0, 1 }. The set of all finite strings, including the empty string, constructed from the symbols of Σ is written Σ*. We then introduce a variety of regular expressions, which we will refer to as RE here. (The abbreviation RE is also used to stand for “recursively enumerable”, but in this section it stands for “regular expression”.) Each form of RE is defined by the set of strings in Σ* that it generates. This set is called a “regular language”.

  1. O/ is RE and denotes the empty set.
  2. e (the string containing no symbols) is RE and denotes the set {e}.
  3. For each symbol x ∈ Σ, x is RE and denotes the set { x }.
  4. If r is RE with language R, then r* is RE and denotes the set R* (where R* = { x1 x2 . . . xn | n ≥ 0 ∧ xi ∈ R } ).
  5. If r and s are RE with languages R and S respectively, then (r + s) is RE and denotes the set R U S.
  6. If r and s are RE with languages R and S respectively, then (rs) is RE and denotes the set RS (where RS = { rs | r ∈ R ∧ s ∈ S } ).

We add two further notations that serve solely as abbreviations.

  1. The expression a^n represents the string aa . . . a containing n occurrences of the symbol a.
  2. The expression a+ is an abbreviation for the RE aa∗ that denotes the set { a, aa, aaa, . . . }.

We can use regular languages to describe the tokens (or lexemes) of most PLs. For example:

  UC = A + B + ··· + Z
  LC = a + b + ··· + z
  LETTER = UC + LC
  DIGIT = 0 + 1 + ··· + 9
  IDENTIFIER = LETTER (LETTER + DIGIT)*
  INTCONST = DIGIT+
  FLOATCONST = DIGIT+ . DIGIT*
  ....

We can use a program such as lex to construct a lexical analyzer (scanner) from a regular expression that defines the tokens of a PL.

  RE               Control Structure
  -----------------------------------------------------------
  x                statement
  rs               statement; statement
  r*               while expression do statement
  r+s              if condition then statement else statement

  RE               Data Structure
  ----------------------------------
  x                int n;
  rs               struct { int n; float y; }
  r^n              int a[n];
  r*               int a[]
  r+s              union { int n; float y; }

These analogies suggest that the mechanisms for constructing REs — concatenation, alternation, and repetition — are somehow fundamental. In particular, the relation between the standard control structures and the standard data structures can be helpful in programming. In Jackson Structured Design (JSD), for example, the data structures appropriate for the application are selected first and then the program is constructed with the corresponding control structures. The limitations of REs are also interesting. When we move to the next level of the Chomsky hierarchy, Context Free Languages (CFLs), we obtain the benefits of recursion. The corresponding control structure is the procedure and the corresponding data structures are recursive structures such as lists and trees (Wirth 1976, page 163).


Context Free Grammers
---------------------

Context free grammars for PLs are usually written in BNF (Backus-Naur Form). A grammar rule, or production, consists of a non-terminal symbol (the symbol being defined), a connector (usually ::= or ->), and a sequence of terminal and non-terminal symbols. The syntax for the assignment, for example, might be defined:

  ASSIGNMENT -> VARIABLE “ := ” EXPRESSION

Basic BNF provides no mechanism for choice: we must provide one rule for each possibility. Also, BNF provides no mechanism for repetition; we must use recursion instead. The following example illustrates both of these limitations.

  SEQUENCE -> EMPTY
  SEQUENCE -> STATEMENT SEQUENCE

BNF has been extended in a variety of ways. With a few exceptions, most of the extended forms can be described simply: the sequence of symbols on the right of the connector is replaced by a regular expression. The extension enables us to express choice and repetition within a single rule. In grammars, the symbol | rather than + is used to denote choice.

  STATEMENT = ASSIGNMENT | CONDITIONAL | · · ·
  SEQUENCE = ( STATEMENT )*

Extended BNF (EBNF) provides a more concise way of describing grammars than BNF. Just as parsers can be constructed from BNF grammars, they can be constructed from EBNF grammars.


The Procedural Paradigm
-----------------------

The introduction of the von Neumann architecture was a crucial step in the development of electronic computers. The basic idea is that instructions can be encoded as data and stored in the memory of the computer. The first consequence of this idea is that changing and modifying the stored program is simple and efficient. In fact, changes can take place at electronic speeds, a very different situation from earlier computers that were programmed by plugging wires into panels. The second, and ultimately more far-reaching, consequence is that computers can process programs themselves, under program control. In particular, a computer can translate a program from one notation to another. Thus the stored-program concept led to the development of programming “languages”.

The first PLs evolved from machine code. The first programs used numbers to refer to machine addresses. One of the first additions to programming notation was the use of symbolic names rather than numbers to represent addresses. This establishes the principle that a variable name stands for a memory location, a principle that influenced the subsequent development of PLs and is now known — perhaps inappropriately — as value semantics.

The importance of subroutines and subroutine libraries was recognized before high-level programming languages had been developed, as the following quotation shows. The following advantages arise from the use of such a library:

  1. It simplifies the task of preparing problems for the machine;
  2. It enables routines to be more readily understood by other users, as conventions are standardized and the units of a routine are much larger, being subroutines instead of individual orders;
  3. Library subroutines may be used directly, without detailed coding and punching;
  4. Library subroutines are known to be correct, thus greatly reducing the overall chance of error in a complete routine, and making it much easier to locate errors.

Another difficulty arises from the fact that, although it is desirable to have subroutines available to cover all possible requirements, it is also undesirable to allow the size of the resulting library to increase unduly. However, a subroutine can be made more versatile by the use of parameters associated with it, thus reducing the total size of the library.

We may divide the parameters associated with subroutines into two classes.

  EXTERNAL parameters, i.e. parameters which are fixed throughout the solution of a problem and arise solely from the use of the library;
  INTERNAL parameters, i.e. parameters which vary during the solution of the problem.

Subroutines may be divided into two types, which we have called OPEN and CLOSED. An open subroutine is one which is included in the routine as it stands whereas a closed subroutine is placed in an arbitrary position in the store and can be called into use by any part of the main routine. (Wheeler 1951)

Machine code is a sequence of “orders” or “instructions” that the computer is expected to execute. The style of programming that this viewpoint developed became known as the “imperative” or “procedural” programming paradigm. In these notes, we use the term “procedural” rather than “imperative” because programs resemble “procedures” (in the English, non-technical sense) or recipes rather than “commands”. Confusingly, the individual steps of procedural PLs, such as Pascal and C, are often called “statements”, although in logic a “statement” is a sentence that is either true or false.

By default, the commands of a procedural program are executed sequentially. Procedural PLs provide various ways of escaping from the sequence. The earliest mechanisms were the “jump” command, which transferred control to another part of the program, and the “jump and store link” command, which transferred control but also stored a “link” to which control would be returned after executing a subroutine.

The data structures of these early languages were usually rather simple: typically primitive values (integers and floats) were provided, along with single- and multi-dimensioned arrays of primitive values.

* FORTRAN: 1957. for IBM 704; Formula Translator; like assembly + math expressions; no recursion; no structure; unsafe memory allocation;

* Algol 60: Independent of any company/computer; ACM require algorithm submissions described in Algol; Block structure and stacked Activation Record; Dynamic Arrays on stack; Call by Name translate parameter to trunk (procedure without param); Own variables (like static vars in C)

* COBOL: Structured data and Data processing, the first time PL dealing with non-numbers, like text; implicit type conversion;

* PL/I: Enhanced implicit type conversion (like js); storage class (control variable extent); explict allocation on heap; simple exception handling; Dommed to failure: "CobAlgoltran", universal language for all kinds of programming; a programmer could learn/use a subset of the language for a particular problem

* Algol 68: Orthogonality; Specification described completely in formal language; Operator overloading, even priority can be altered; uniform notation for declarations and other entiies; collateral clause in which expressions will be evaluated in any order or concurrently; Collateral clauses provide a good, and early, example of the idea that a PL specification should intentionally leave some implementation details undefined; operator 'ref' for reference (* and & in C)

* Pascal: demonstrate PL can be simple but powerful; Type system based on primitives and mechanisms for building structured types; data type declaration; static type checking; recursive data type; no implicit type conversion; development by stepwise refinement (Wirth); monolithic structure prevents independt complilation

* Modula-2: learned from Pascal by Wirth; introduce module with sperated interface and implementation; coroutines

* C: low level; simple; concise; pointer; system programming

* Ada: the last major effort in procedural language design; a large and complex language that combines then-known programming features with little attempt at consolidation; the first widely-used language to provide full support for concurrency, with interactions checked by the compiler


The Functional Paradigm
-----------------------

Procedural programming is based on instructions (“do something”) but, inevitably, procedural PLs also provide expressions (“calculate something”). The key insight of functional programming (FP) is that everything can be done with expressions: the commands are unnecessary.

This point of view has a solid foundation in theory. Turing (1936) introduced an abstract model of “programming”, now known as the Turing machine. Kleene (1936) and Church (1941) introduced the theory of recursive functions. The two theories were later shown (by Kleene) to be equivalent: each had the same computational power. Other theories, such as Post production systems, were shown to have the same power. This important theoretical result shows that FP is not a complete waste of time but it does not tell us whether FP is useful or practical. To decide that, we must look at the functional programming languages (FPLs) that have actually been implemented.

Most functional language support high order functions. Roughly, a high order function is a function that takes another function as a parameter or returns a function. More precisely:

  A zeroth order expression contains only variables and constants.
  A first order expression may also contain function invocations, but the results and parameters of functions are variables and constants (that is, zeroth order expressions).
  In general, in an n-th order expression, the results and parameters of functions are (n−1)th order expressions.

A high order expression is an n-th order expression with n ≥ 2. The same conventions apply in logic with “function” replaced by “function or predicate”. In first-order logic, quantifiers can bind variables only; in a high order logic, quantifiers can bind predicates.

In procedural PLs, a name denotes a storage location (value semantics). In LISP, a name is a reference to an object, not a location (reference semantics). (consequence is there will be garbages - objects with no reference). A PL in which variable names are references to objects in memory is said to have reference semantics. All FPLs and most OOPLs have reference semantics.

Note that reference semantics is not the same as “pointers” in languages such as Pascal and C. A pointer variable stands for a location in memory and therefore has value semantics; it just so happens that the location is used to store the address of another object.

* LISP: 1958. provide list processing (like IPL or FLPL); prefix notation; high order functions and notation for functions; garbage collector; first compiler written in the language it compiles; lambda expressions to denote functions; dynamic scoping; interpretation

        The close relationship between code and data in LISP mimics the von Neumann architecture at a higher level of abstraction; 

        Another way to show that LISP was neater than Turing machines was to write a universal LISP function and show that it is briefer and more comprehensible than the description of a universal Turing machine. This was the LISP function eval [e, a], which computes the value of a LISP expression e, the second argument a being a list of assignments of values to variables. . . . Writing eval required inventing a notation for representing LISP functions as LISP data, and such a notation was devised for the purpose of the paper with no thought that it would be used to express LISP programs in practice. (McCarthy 1978)

        The lambda expression itself cannot be evaluated. Consequently, LISP had to resort to programming tricks to make higher order functions work. For example, if we want to pass the squaring function as an argument to another function, we must wrap it up in a “special form”

        Although dynamic scoping is natural for an interpreter, it is inefficient for a compiler. Interpreters are slow anyway, and the overhead of searching a linear list for a variable value just makes them slightly slower still. A compiler, however, has more efficient ways of accessing variables, and forcing it to maintain a linear list would be unacceptably inefficient. Consequently, early LISP systems had an unfortunate discrepancy: the interpreters used dynamic scoping and the compilers used static scoping. Some programs gave one answer when interpreted and another answer when compiled!

* Scheme: 1975. actors model (alpha operation); prove alpha == lambda; closure, is the value of function; lexical/static scoping; continuations; higher order functions + control of state = simple form of OOP

* SASL: 1976. compile by combinator reduction; call by name optimization (call by need/lazy evaluation); short-circuit is lazy eval in fact

* SML: "Standard" ML; "metalanguage", ML; reasoning about programs; statically typed; inferred types; function declaration by cases (pattern matching); function composition; currying


Object Oriented Paradigm
------------------------

OOP provides a way of building programs by incremental modification. Programs can often be extended by adding new code rather than altering existing code. The mechanism for incremental modification without altering existing code is inheritance. However, there are several different ways in which inheritance can be defined and used, and some of them conflict.

Class-based OOPLs can be designed so as to support information hiding (encapsulation) — the separation of interface and implementation. However, there may be conflicts between inheritance and encapsulation. An interesting, but rarely noted, feature of OOP is that the call graph is created and modified during execution. This is in contrast to procedural languages, in which the call graph can be statically inferred from the program text.

* Simula: 1965 (Simula I). coroutine; multiple stacks; classes; inheritance (prefix block); garbage collector

    Deleting the procedure definitions and final statement from an Algol block gives a pure data record.
    Deleting the final statement from an Algol block, leaving procedure definitions and data declarations, gives an abstract data object.
    Adding coroutine constructs to Algol blocks provides quasi-parallel programming capabilities.
    Adding a prefix mechanism to Algol blocks provides an abstraction mechanism (the class hierarchy).

    Note that the class is syntactically more like a procedure than a data object: it has an (optional) formal parameter and it must be called with an actual parameter. It differs from an ordinary Algol procedure in that its AR remains on the heap after the invocation.

* Smalltalk: 1972 (first compiler, based on BASIC). inspired by LISP and Simula; block; private data/public function; message exchange; single root inheritance; coroutines; garbage collector

    Six principles:
    1. Everything is an object.
    2. Objects communicate by sending and receiving messages (in terms of objects).
    3. Objects have their own memory (in terms of objects).
    4. Every object is an instance of a class (which must be an object).
    5. The class holds the shared behaviour for its instances (in the form of objects in a program list).
    6. To [evaluate] a program list, control is passed to the first object and the remainder is treated as its message.

    Compare to Simula:
    1. Simula distinguishes primitive types, such as integer and real, from class types. In Smalltalk, “everything is an object”.
    2. In particular, classes are objects in Smalltalk. To create a new instance of a class, you send a message to it. Since a class object must belong to a class, Smalltalk requires metaclasses.
    3. Smalltalk effectively eliminates passive data. Since objects are “active” in the sense that they have methods, and everything is an object, there are no data primitives.
    4. Smalltalk is a complete environment, not just a compiler. You can edit, compile, execute, and debug Smalltalk programs without ever leaving the Smalltalk environment.

* CLU: 1974. information hiding; ADT; up/down (cast); mutable/immutable; modules/classes (clusters); safe encapsulation; generic cluster; exception handling

    An abstract data type (ADT) specifies a type, a set of operations that may be performed on instances of the type, and the effects of these operations. The implementation of an ADT provides the actual operations that have the specified effects and also prevents programmers from doing anything else. The word “abstract” in CLU means “user defined”, or not built in to the language:

    "I referred to the types as “abstract” because they are not provided directly by a programming languages but instead must be implemented by the user. An abstract type is abstract in the same way that a procedure is an abstract operation." (Liskov 1996, page 473) (J: what a bad name!)

    This is not a universally accepted definition. Many people use the term ADT in the following sense: An abstract data type is a system consisting of three constituents:
    1. some sets of objects;
    2. a set of syntactic descriptions of the primitive functions;
    3. a semantic description — that is, a sufficiently complete set of relationships that specify how the functions interact with each other. (Martin 1986)

    The difference is that an ADT in CLU provides a particular implementation of the type (and, in fact, the implementation must be unique) whereas Martin’s definition requires only a specification.

    Compare Simula to CLU, Simula:
    1. does not provide encapsulation: clients could directly access the variables, as well as the functions, of a class;
    2. does not provide “type generators” or, as we would say now, “generic types” (“templates” in C++);
    3. associates operations with objects rather than types;
    4. handles built-in and user-defined types differently — for example, instances of user-defined classes are always heap-allocated.

    Heap is used for objects because:
    1. Declarations are easy to implement because, for all types, space is allocated for a pointer. Stack allocation breaks encapsulation because the size of the object (which should be an implementation “secret”) must be known at the point of declaration.
    2. Variable declaration is separated from object creation. This simplifies safe initialization.
    3. Variable and object lifetimes are not tied. When the variable goes out of scope, the object continues to exist (there may be other references to it).
    4. The meaning of assignment is independent of type. The statement x := E means simply that x becomes a reference (implemented as a pointer, of course) to E. There is no need to worry about the semantics of copying an object. Copying, if needed, should be provided by a member function of the class.
    5. Reference semantics requires garbage collection. Although garbage collection has a run-time overhead, efficiency was not a primary goal of the CLU project. Furthermore, garbage collection eliminates memory leaks and dangling pointers, notorious sources of obscure errors.

    Whereas Simula provides tools for the programmer and supports a methodology, CLU provides the same tools and enforces the methodology. The cost is an increase in complexity. Several keywords (rep, cvt, down, and up) are needed simply to maintain the distinction between abstract and representation types. In Simula, three concepts — procedures, classes, and records — are effectively made equivalent, resulting in significant simplification. In CLU, procedures, records, and clusters are three different things. The argument in favour of CLU is that the compiler will detect encapsulation and other errors. But is it the task of a compiler to prevent the programmer doing things that might be completely safe? Or should this role be delegated to a style checker, just as C programmers use both cc (the C compiler) and lint (the C style checker)?

    The designers of CLU advocate defensive programming: "Of course, checking whether inputs [i.e., parameters of functions] are in the permitted subset of the domain takes time, and it is tempting not to bother with the checks, or to use them only while debugging, and suppress them during production. This is generally an unwise practice. It is better to develop the habit of defensive programming, that it writing each procedure to defend itself against errors. . . .  Defensive programming makes it easier to debug programs. . . ." (Liskov and Guttag 1986, page 100) (J: so static type and type checker is good)

C++: superset of C; hybrid (imperative and OO); emphasize stack than heap; multiple inheritance; genericity; exception handling; no GC

Effiel: strictly OO; multiple inheritance; repeated inheritance; programming by contract; unusual excpetion handling; generic classes; may provide GC

    Programming by Contract is an extension of defensive programming. Effiel use require/ensure clauses to constitute a contract between function and caller. Require is like a assertion. If argument supplied by caller doesn't meet contract, the function can return anything. Effiel also has class invariation, which is a predicate over the variables of an instance of the class. It must be true when the object is created and after each function of the class has executed. Functions in a child class must provide a contract at least as strong as that of the corresponding function in the parent class.

    Repeated inheritance is like virutal inheritance in C++, but on attribute level instead of class level.

    An Eiffel function must either fulfill its contract or report failure. If a function contains a rescue clause, this clause is invoked if an operation within the function reports failure. A return from the rescue clause indicate that the function has failed. However, a rescue clause may perform some cleaning-up actions and then invoke retry to attempt the calculation again. The mechanism seems harder to use than other, more conventional, exception handling mechanisms. It is not obvious that there are many circumstances in which it makes sense to “retry” a function.

Java: byte codes; portability; security; single inheritance; single root; interfaces for multiple inheritance; exception handling; primitive values and wrapper classes; reference semantics; GC; applets; just-in-time compilation;

Kevo: All of the OOPLs previously described in this section are class-based languages. In a class-based language, the programmer defines one or more classes and, during execution, instances of these classes are created and become the objects of the system. (In Smalltalk, classes are run-time objects, but Smalltalk is nevertheless a class-based language.) There is another, smaller, family of OOPLs that use prototypes rather than classes. Although there are several prototype-based OOPLs, we discuss only one of them here. Antero Taivalsaari (1993) has given a thorough account of the motivation and design of his language, Kevo. Taivalsaari (1993, page 172) points out that class-based OOPLs, starting with Simula, are based on an Aristotelian view in which the world is seen as a collection of objects with well-defined properties arranged [according] to a hierarchical taxonomy of concepts. The problem with this approach is that there are many classes which people understand intuitively but which are not easily defined in taxonomic terms. Common examples include “book”, “chair”, “game”, etc. Prototypes provide an alternative to classes. In a prototype-based OOPL, each new type of object is introduced by defining a prototype or exemplar , which is itself an object. Identical copies of a prototype — as many as needed — are created by cloning . Alternatively, we can clone a prototype, modify it in some way, and use the resulting object as a prototype for another collection of identical objects. The modified object need not be self-contained. Some of its methods will probably be the same as those of the original object, and these methods can be delegated to the original object rather than replicated in the new object. Delegation thus plays the role of inheritance in a prototype-based language and, for this reason, prototype-based languages are sometimes called delegation languages. This is misleading, however, because delegation is not the only possible mechanism for inheritance with prototypes.

CLOS: The Common LISP Object System is an extension to LISP that provides OOP. It has a number of interesting features, including “multi-methods” — the choice of a function is based on the class of all of its arguments rather than its first (possibly implicit) argument. CLOS also provides “before” and “after” methods — code defined in a superclass that is executed before or after the invocation of the function in a class.

Self: Self is the perhaps the best-known prototype language. Considerable effort has been put into efficient implementation, to the extent that Self programs run at up to half of the speed of equivalent C++ programs. This is an impressive achievement, given that Self is considerably more flexible than C++.


Backtracking Languages
----------------------

Backtracking languages are designed to solve problems with multiple answers, and to find as many answers as required.

Constraint PLs are attracting increasing attention. A program is a set of constraints, expressed as equalities and inequalities, that must be satisfied. Prolog is strong in logical deduction but weak in numerical calculation; constraint PLs attempt to correct this imbalance.

Prolog: first logic PL; "pure" Prolog programme is inefficient; cut; occurs check; GC

    In the declarative reading, Prolog finds multiple results simply because they are there. In the procedure reading, multiple results are obtained by backtracking . Every point in the program at which there is more than one choice of a variable binding is called a choice point. The choice points define a tree: the root of the tree is the starting point of the program (the main goal) and each leaf of the tree represents either success (the goal is satisfied with the chosen bindings) or failure (the goal cannot be satisfied with these bindings). Prolog performs a depth-first search of this tree.

    In Prolog, proving not(P) means “P cannot be proved” which means “P cannot be inferred from the facts and rules in the database”.

    The Prolog system proceeds top-down. It attempts to match the entire goal to one of the rules and, having found a match, applies the same idea to the parts of the matched formulas. The “matching” process is called unification.

    Prolog syntax corresponds to a restricted form of first-order predicate calculus called clausal form logic. It is not practical to use the full predicate calculus as a basis for a PL because it is undecidable. Clausal form logic is semi-decidable: there is an algorithm that will find a proof for any formula that is true in the logic. If a formula is false, the algorithm may fail after a finite time or may loop forever. The proof technique, called SLD resolution, was introduced by Robinson (1965). SLD stands for “S electing a literal, using a Linear strategy, restricted to Definite clauses”.

    The proof of validity of SLD resolution assumes that unification is implemented with the occurs check. Unfortunately, the occurs check is expensive to implement and most Prolog systems omit it. Consequently, most Prolog systems are technically unsound, although problems are rare in practice. A Prolog program apparently has a straightforward interpretation as a statement in logic, but the interpretation is slightly misleading. For example, since Prolog works through the rules in the order in which they are written, the order is significant. Since the logical interpretation of a rule sequence is disjunction, it follows that disjunction in Prolog does not commute.

Alma-0: start from Modula-2, add many logic PL features;

    (J: really interesting and powerful)
    BES (boolean expression as statement)
    SBE (statement sequence as boolean expression)
    EITHER-ORELSE statement
    SOME statement
    COMMIT statement (like cut in prolog)
    FORALL statement
    EQ statement
    MIX: call by mixed form, a new param passing mechanism (J: I guess this is a bug trigger)
    KNOWN statement

    The language Alma-0 has a declarative semantics in which the constructs described above correspond to logical formulas. It also has a procedural semantics that describes how the constructs can be efficiently executed, using backtracking. Alma-0 demonstrates that, if appropriate care is taken, it is possible to design PLs that smoothly combine paradigms without losing the advantages of either paradigm.

    The interesting feature of Alma-0 is that it demonstrates that it is possible to start with a simple, well-defined language (Modula-2), remove features that are not required for the current purpose, and add a small number of simple, new features that provide expressive power when used together.


Implementation
--------------

A compiler translates source language to machine language. Since practical compilation systems allow programs to be split into several components for compiling, there are usually several phases.

  1. The components of the source program is compiled into object code.
  2. The object code units and library functions required by the program are linked to form an executable file.
  3. The executable file is loaded into memory and executed.

All compilers perform some checks to ensure that the source code that they are translating is correct. 

  * A FORTRAN compiler provides no checking between components. When the compiled components are linked, the linker will report subroutines that have not been defined but it will not detect discrepancies in the number of type of arguments. The result of such a discrepancy is, at best, run-time failure.

  * C and C++ rely on the programmer to provide header files containing declarations and implementation files containing definitions. The compiler checks that declarations and definitions are compatible. Overloaded functions in C++ present a problem because one name may denote several functions. A key design decision in C++, however, was to use the standard linker (Stroustrup 1994, page 34). The solution is “name mangling” — the C++ compiler alters the names of functions by encoding their argument types in such a way that each function has a unique name.

  * Modula-2 programs consist of interface modules and implementation modules. When the compiler compiles an implementation module, it reads all of the relevant interfaces and performs full checking. In practice, the interface modules are themselves compiled for efficiency, but this is not logically necessary.

  * Eiffel programs consist of classes that are compiled separately. The compiler automatically derives an interface from the class declaration and uses the generated interfaces of other classes to perform full checking. Other OOPLs that use this method include Blue, Dee, and Java.

An interpreter accepts source language and run-time input data and executes the program directly. It is possible to interpret without performing any translation at all, but this approach tends to be inefficient. Consequently, most interpreters do a certain amount of translation before executing the code. For example, the source program might be converted into an efficient internal representation, such as an abstract syntax tree, which is then “executed”. Interpreting is slower than executing compiled code, typically by a factor of 10 or more.

Some programming environments, particularly those for LISP, provide both interpretation and compilation. In fact, some LISP environments allow source code and compiled code to be freely mixed.

Byte Codes. Another mixed strategy consists of using a compiler to generate byte codes and an interpreter to execute the byte codes. The “byte codes”, as the name implies, are simply streams of bytes that represent the program: a machine language for an “abstract machine”. One of the first programming systems to be based on byte codes was the Pascal “P” compiler.

    The advantage of byte code programs is that they can, in principle, be run on any machine. Thus byte codes provide a convenient way of implementing “portable” languages. The Pascal “P” compiler provided a simple method for porting Pascal to a new kind of machine. The “Pascal P package” consisted of the P-code compiler written in P-code. A Pascal implementor had to perform the following steps:

      1. Write a P-code interpreter. Since P-code is quite simple, this is not a very arduous job.
      2. Use the P-code interpreter to interpret the P-code compiler. This step yields a P-code program that can be used to translate Pascal programs to P-code. At this stage, the implementor has a working Pascal system, but it is inefficient because it depends on the P-code interpreter.
      3. Modify the compiler source so that the compiler generates machine code for the new machine. This is a somewhat tedious task, but it is simplified by the fact that the compiler is written in Pascal.
      4. Use the compiler created in step 2 to compile the modified compiler. The result of this step is a P-code program that translates Pascal to machine code.
      5. Use the result of step 4 to compile the modified compiler. The result is a machine code program that translates Pascal to machine code — in other words, a Pascal compiler for the new machine.

    With the exercise of due caution, the process described above (called bootstrapping ) can be simplified. For example, the first version of the compiler, created in step 2 does not have to compile the entire Pascal language, but only those parts that are needed by the compiler itself. The complete sequence of steps can be used to generate a Pascal compiler that can compile itself, but may be incomplete in other respects. When that compiler is available, it can be extended to compile the full language.

Just In Time. “Just in time” (JIT) compilation is a recent strategy that is used for Java and a few research languages. The idea is that the program is interpreted, or perhaps compiled rapidly without optimization. As each statement is executed, efficient code is generated for it. This approach is based on the following assumptions:

    1. in a typical run, much of the code of a program will not be executed;
    2. if a statement has been executed once, it is likely to be executed again.

    JIT provides fast response, because the program can be run immediately, and efficient execution, because the second and subsequent cycles of a loop are executed from efficiently compiled code. Thus JIT combines some of the advantages of interpretation and compilation.

Interactive Systems: A PL can be designed for writing large programs that run autonomously or for interacting with the user. The categories overlap: PLs that can be used interactively, such as BASIC, LISP, SML, and Smalltalk, can also be used to develop large programs. The converse is not true: an interactive environment for C++ would not be very useful. An interactive PL is usually interpreted, but interpretation is not the only possible means of implementation. PLs that start out with interpreters often acquire compilers at a later stage — BASIC, LISP, and a number of other interactive PLs followed this pattern of development. A PL that is suitable for interactive use if programs can be described in small chunks. FPLs are often supported interactively because a programmer can build up a program as a collection of function definitions, where most functions are fairly small (e.g., they fit on a screen). Block structured languages, and modular languages, are less suitable for this kind of program development.

There is a sharp distinction: either a PL is implemented with garbage collection (GC) or it is not. The implementor does not really have a choice. An implementation of LISP, Smalltalk, or Java without GC would be useless — programs would exhaust memory in a few minutes at most.  Conversely, it is almost impossible to implement C or C++ with GC, because programmers can perform arithmetic on pointers.

The imperative PL community has been consistently reluctant to incorporate GC: there is no widely-used imperative PL that provides GC. The functional PL community has been equally consistent, but in the opposite direction. OOPL community is divided on this, but most of them provide GC.

Conservative GC depends on the fact that it is possible to guess with a reasonable degree of accuracy whether a particular machine word is a pointer (Boehm and Weiser 1988). In typical modern processors, a pointer is a 4-byte quantity, aligned on a word boundary, and its value is a legal address. By scanning memory and noting words with these properties, the GC can identify regions of memory that might be in use. The problem is that there will be occasionally be words that look like pointers but actually are not: this results in about 10% of garbage being retained. The converse, and more serious problem, of identifying live data as garbage cannot occur: hence the term “conservative”.


Abstraction
-----------

Abstraction is one of the most important mental tools in Computer Science. The biggest problems that we face in Computer Science are concerned with complexity; abstraction helps us to manage complexity. Abstraction is particularly important in the study of PLs because a PL is a two-fold abstraction.

A PL must provide an abstract view of the underlying machine. The values that we use in programming — integers, floats, booleans, arrays, and so on — are abstracted from the raw bits of computer memory. The control structures that we use — assignments, conditionals, loops, and so on — are abstracted from the basic machine instructions. The first task of a PL is to hide the low-level details of the machine and present a manageable interface, preferably an interface that does not depend on the particular machine that is being used.

A PL must also provide ways for the programmer to abstract information about the world. Most programs are connected to the world in one way or another, and some programs are little more than simulations of some aspect of the world. For example, an accounting program is, at some level, a simulation of tasks that used to be performed by clerks and accountants. The second task of a PL is to provide as much help as possible to the programmer who is creating a simplified model of the complex world in a computer program.

In the evolution of PLs, the first need came before the second. Early PLs emphasize abstraction from the machine: they provide assignments, loops, and arrays. Programs are seen as lists of instructions for the computer to execute. Later PLs emphasize abstraction from the world: they provide objects and processes. Programs are seen as descriptions of the world that happen to be executable.

An abstraction mechanism provides a way of naming and parameterizing a program entity.

Procedures: The earliest abstraction mechanism was the procedure, which exists in rudimentary form even in assembly language. Procedural abstraction enables us to give a name to a statement, or group of statements, and to use the name elsewhere in the program to trigger the execution of the statements. Parameters enable us to customize the execution according to the particular requirements of the caller.

Functions: The next abstraction mechanism, which also appeared at an early stage was the function. Functional abstraction enables us to give a name to an expression, and to use the name to trigger evaluation of the expression.

    In many PLs, procedures and functions are closely related. Typically, they have similar syntax and similar rules of construction. The reason for this is that an abstraction mechanism that works only for simple expressions is not very useful. In practice, we need to abstract form arbitrarily complex programs whose main purpose is to yield a single value.

    The difference between procedures and functions is most evident at the call site, where a procedure call appears in a statement context and a function call appears in an expression context.

    FORTRAN provides procedures (called “subroutines”) and functions. Interestingly, COBOL does not provide procedure or function abstraction, although the PERFORM verb provides a rather restricted and unsafe way of executing statements remotely. LISP provides function abstraction. The effect of procedures is obtained by writing functions with side-effects. Algol 60, Algol 68, and C take the view that everything is an expression. (The idea seems to be that, since any computation leaves something in the accumulator, the programmer might as well be allowed to use it.) Thus statements have a value in these languages. It is not surprising that this viewpoint minimizes the difference between procedures and functions. In Algol 68 and C, a procedure is simple a function that returns a value of type void. Since this value is unique, it requires log 1 = 0 bits of storage and can be optimized away by the compiler.

Data Types

Classes


Computational Model
-------------------

There is an abstraction that is useful for describing a PL in general terms, without the details of every construct, and that is the computational model (also sometimes called the semantic model ). The computational model (CM) is an abstraction of the operational semantics of the PL and it describes the effects of various operations without describing the actual implementation. Programmers who use a PL acquire intuitive knowledge of its CM. When a program behaves in an unexpected way, the implementation has failed to respect the CM.

A simple CM does not imply easy implementation. Often, the opposite is true.

There are two ways in which an OOPL can treat primitive entities such as booleans, characters, and integers. Primitive values can have a special status that distinguishes them from objects. A language that uses this policy is likely to be efficient, because the compiler can process primitives without the overhead of treating them as full-fledged objects. On the other hand, it may be confusing for the user to deal with two different kinds of variable — primitive values and objects.  Primitive values can be given the same status as objects. This simplifies the programmer’s task, because all variables behave like objects, but a naive implementation will probably be inefficient.

A solution for this dilemma is to define a CM for the language in which all variables are objects. An implementor is then free to implement primitive values efficiently provided that the behaviour predicted by the CM is obtained at all times. In Smalltalk, every variable is an object. Early implementations of Smalltalk were inefficient, because all variables were actually implemented as objects. Java provides primitives with efficient implementations but, to support various OO features, also has classes for the primitive types.

FPLs are based on the theory of partial recursive functions. They attempt to use this theory as a CM. The advantage of this point of view is that program text resembles mathematics and, to a certain extent, programs can be manipulated as mathematical objects. The disadvantage is that mathematics and computation are not the same. Some operations that are intuitively simple, such as maintaining the balance of a bank account subject to deposits and withdrawals, are not easily described in classical mathematics. The underlying difficulty is that many computations depend on a concept of mutable state that does not exist in mathematics.

FPLs have an important property called referential transparency . An expression is referentially transparent if its only attribute is its value. The practical consequence of referential transparency is that we can freely substitute equal expressions.

Early CMs: modelled programs as code acting on data; structured programs by recursive decomposition of code.
Later CMs: modelled programs as packages of code and data; structured programs by recursive decomposition of packages of code and data.
The CM should: help programmers to reason about programs; help programmers to read and write programs; constrain but not determine the implementation.


Names and Binding
-----------------

free name: n+1
bound name: for n in (1..5); puts n; end

Expression with bounded name has definite value, change the name doesn't change the value.

In predicate calculus, predicates may contain free names, as in: n mod 2 = 0 => n^2 mod 2 = 0. Names are bound by the quantifiers ∀ (for all) and ∃ (there exists). We say that the formula ∀n . n mod 2 = 0 => n^2 mod 2 = 0 is closed because it contains no free variables. Strictly, we should specify the range of values that n is allowed to assume. We could do this implicitly: for example, it is very likely that n refers to integers. Alternatively, we could define the range of values explicitly, as in ∀n ∈ Z . n mod 2 = 0 => n^2 mod 2 = 0.

Precisely analogous situations occur in programming. In the function

  int f (int n)
  {
    return k + n;
  }

k occurs free and n occurs bound. Note that we cannot tell the value of n from the definition of the function but we know that n will be given a value when the function is called. In most PLs, a program with free variables will not compile. A C compiler, for example, will accept the function f defined above only if it is compiled in a scope that contains a declaration of k. Some early PLs, such as FORTRAN and PL/I, provided implicit declarations for variables that the programmer did not declare; this is now understood to be error-prone and is not a feature of recent PLs.

In mathematics, a variable normally has only one attribute: its value. In (1), n is bound to the values 0, 1, . . . , 5. Sometimes, we specify the domain from which these values are chosen, as in n ∈ Z. In programming, a name may have several attributes, and they may be bound at different times.  For example, in the sequence

  int n;
  n = 6;

the first line binds the type int to n and the second line binds the value 6 to n. The first binding occurs when the program is compiled. (The compiler records the fact that the type of n is int; neither this fact nor the name n appears explicitly in the compiled code.) The second binding occurs when the program is executed. This example shows that there are two aspects of binding that we must consider in PLs: the attribute that is bound, and the time at which the binding occurs. The attributes that may be bound to a name include: type, address, and value. The times at which the bindings occur include: compile time, link time, load time, block entry time, and statement execution time.

Definition. A binding is static if it occurs during before the program is executed: during compilation or linking. A binding is dynamic if it occurs while the program is running: during loading, block entry, or statement execution.

An anonymous contributor to the Encyclopedia of Computer Science wrote: Broadly speaking, the history of software development is the history of ever-later binding time.

  Early binding => efficiency
  Late binding => flexibility

Variable Addressing:

  In FORTRAN, addresses are bound to variable names at compile time. The result is that, in the compiled code, variables are addressed directly, without any indexing or other address calculations. (In reality, the process is somewhat more complicated. The compiler assigns an address relative to a compilation unit. When the program is linked, the address of the unit within the program is added to this address. When the program is loaded, the address of the program is added to the address. The important point is that, by the time execution begins, the “absolute” address of the variable is known.) FORTRAN is efficient, because absolute addressing is used. It is inflexible, because all addresses are assigned at load time. This leads to wasted space, because all local variables occupy space whether or not they are being used, and also prevents the use of direct or indirect recursion.

  In languages of the Algol family (Algol 60, Algol 68, C, Pascal, etc.), local variables are allocated on the run-time stack. When a function is called, a stack frame (called, in this context, an activation record or AR) is created for it and space for its parameters and local variables is allocated in the AR. The variable must be addressed by adding an offset (computed by the compiler) to the address of the AR, which is not known until the function is called. Algol-style is slightly less efficient than FORTRAN because addresses must be allocated at block-entry time and indexed addressing is required. It is more flexible than FORTRAN because inactive functions do not use up space and recursion works.

  Languages that provide dynamic allocation, such as Pascal and C and most of their successors, have yet another way of binding an address to a variable. In these languages, a statement such as new(p) allocates space on the “heap” and stores the address of that space in the pointer p. Accessing the variable requires indirect addressing, which is slightly slower then indexed addressing, but greater flexibility is obtained because dynamic variables do not have to obey stack discipline (last in, first out).

Function Addressing:

  When the compiler encounters a function definition, it binds an address to the function name. (As above, several steps must be completed before the absolute address of the function is determined, but this is not relevant to the present discussion.) When the compiler encounters a function invocation, it generates a call to the address that it has assigned to the function. This statement is true of all PLs developed before OOP was introduced. OOPLs provide “virtual” functions. A virtual function name may refer to several functions. The actual function referred to in a particular invocation is not in general known until the call is executed.

  Thus in non-OO PLs, functions are statically bound, with the result that calls are efficiently executed but no decisions can be made at run-time. In OOPLs, functions are dynamically bound. Calls take longer to execute, but there is greater flexibility because the decision as to which function to execute is made at run-time. The overhead of a dynamically-bound function call depends on the language. In Smalltalk, the overhead can be significant, because the CM requires a search. In practice, the search can be avoided in up to 95% of calls by using a cache. In C++, the compiler generates “virtual function tables” and dynamic binding requires only indirect addressing. Note, however, that this provides yet another example of the principle: Smalltalk binds later than C++, runs more slowly, but provides greater flexibility.

Definition. An entity can be named if the PL provides a definitional mechanism that associates a name with an instance of the entity. An entity can be denoted if the PL provides ways of expressing instances of the entity as expressions.

In C, we can name functions but we cannot denote them. The definition

  int f (int x)
  {
    B
  }

introduces a function with name f , parameter x, and body B. Thus functions in C can be named. There is no way that we can write a function without a name in C. Thus functions in C cannot be denoted.

We have seen that most PLs choose between two interpretations of a variable name. In a PL with value semantics, variable names denote memory locations. Assigning to a variable changes the contents of the location named by the variable. In a PL with reference semantics, variable names denote objects in memory. Assigning to a variable, if permitted by the language, changes the denotation of the name to a different object. Reference semantics is sometimes called “pointer semantics”. This is reasonable in the sense that the implementation of reference semantics requires the storage of addresses — that is, pointers. It is misleading in that providing pointers is not the same as providing reference semantics. The distinction is particularly clear in Hoare’s work on PLs. Hoare (1974) has this to say about the introduction of pointers into PLs.

  Many language designers have preferred to extend [minor, localized faults in Algol 60 and other PLs] throughout the whole language by introducing the concept of reference, pointer, or indirect address into the language as an assignable item of data. This immediately gives rise in a high-level language to one of the most notorious confusions of machine code, namely that between an address and its contents. Some languages attempt to solve this by even more confusing automatic coercion rules. Worse still, an indirect assignment through a pointer, just as in machine code, can update any store location whatsoever, and the damage is no longer confined to the variable explicitly names as the target of assignment. For example, in Algol 68, the assignment x := y always changes x, but the assignment p := y + 1 may, if p is a reference variable, change any other variable (of appropriate type) in the whole machine. One variable it can never change is p! . . . . References are like jumps, leading wildly from one part of a data structure to another. Their introduction into high-level languages has been a step backward from which we may never recover.

One year later, Hoare (1975) provided his solution to the problem of references in high-level PLs:

  In this paper, we will consider a class of data structures for which the amount of storage can actually vary during the lifetime of the data, and we will show that it can be satisfactorily accommodated in a high-level language using solely high-level problem-oriented concepts, and without the introduction of references.

The implementation that Hoare proposes in this paper is a reference semantics with types. Explicit types make it possible to achieve

  . . . . a significant improvement on the efficiency of compiled LISP, perhaps even a factor of two in space-time cost for suitable applications. (Hoare 1975)

All FPLs and most OOPLs (the notable exception, of course, being C++) use reference semantics. There are good reasons for this choice, but the reasons are not the same for each paradigm.

In a FPL, all (or at least most) values are immutable. If X and Y are have the same value, the program cannot tell whether X and Y are distinct objects that happen to be equal, or pointers to the same object. It follows that value semantics, which requires copying, would be wasteful because there is no point in making copies of immutable objects. (LISP provides two tests for equality. (eq x y) is true if x and y are pointers to the same object. (equal x y) is true if the objects x and y have the same extensional value. These two functions are provided partly for efficiency and partly to cover up semantic deficiencies of the implementation. Some other languages provide similar choices for comparison.)

One of the important aspects of OOP is object identity . If a program object X corresponds to a unique entity in the world, such as a person, it should be unique in the program too. This is most naturally achieved with a reference semantics.


Polymorphism
------------

The word “polymorphism” is derived from Greek and means, literally, “many shapes”. In PLs, “polymorphism” is used to describe a situation in which one name can refer to several different entities. The most common application is to functions. There are several kinds of polymorphism; the terms “ad hoc polymorphism” and “parametric polymorphism” are due to Christopher Strachey.

Ad Hoc Polymorphism:

  In general, ad hoc polymorphism refers to the use of a single function name to refer to two or more distinct functions. Typically the compiler uses the types of the arguments of the function to decide which function to call. Ad hoc polymorphism is also called “overloading”. Almost all PLs provide ad hoc polymorphism for built-in operators such as “+”, “−”, “*”, etc. Ada, C++, and other recent languages also allow programmers to overload functions. (Strictly, we should say “overload function names” but the usage “overloaded functions” is common.) In general, all that the programmer has to do is write several definitions, using the same function name, but ensuring that the either the number or the type of the arguments are different.

Parameterized Polymorphism:

  Suppose that a language provides the type list as a parameterized type. That is, we can make declarations such as these:

    list(int)
    list(float)

  Suppose also that we have two functions: sum computes the sum of the components of a given list, and len computes the number of components in a given list. There is an important difference between these two functions. In order to compute the sum of a list, we must be able to choose an appropriate “add” function, and this implies that we must know the type of the components. On the other hand, there seems to be no need to know the type of the components if all we need to do is count them. A function such as len, which counts the components of a list but does not care about their type, has parametric polymorphism.

Object Polymorphism:

  In OOPLs, there may be many different objects that provide a function called, say, f . However, the effect of invoking f may depend on the object. The details of this kind of polymorphism, which are discussed in Section 6, depend on the particular OOPL. This kind of polymorphism is a fundamental, and very important, aspect of OOP.


Assignment
----------

Consider the assignment x := E. Whatever the PL, the semantics of this statement will be something like:

  evaluate the expression E;
  store the value obtained in x.

The assignment is unproblematic if x and E have the same type. But what happens if they have different types? There are several possibilities:

  * The statement is considered to be an error. This will occur in a PL that provides static type checking but does not provide coercion. For example, Pascal provides only a few coercions (subrange to integer, integer to real, etc.) and rejects other type differences.
  * The compiler will generate code to convert the value of expression E to the type of x. This approach was taken to extremes in COBOL and PL/I. It exists in C and C++, but C++ is stricter than C.
  * The value of E will be assigned to x anyway. This is the approach taken by PLs that use dynamic type checking. Types are associated with objects rather than with names.


Scope and Extent
----------------

The scope of a name is the region of the program text in which a name may be used. In C++, for example, the scope of a local variable starts at the declaration of the variable and ends at the end of the block in which the declaration appears. Scope is a static property of a name that is determined by the semantics of the PL and the text of the program.

  * Global Scope. The name is visible throughout the program.
  * Local Scope. In block-structured languages, names declared in a block are local to the block. There is a question as to whether the scope starts at the point of definition or is the entire block. (In other words, does the PL require “declaration before use”?)
  * Qualified Scope. The components of a structure, such as a Pascal record or a C struct, have names. These names are usually hidden, but can be made visible by the name of the object.
  * Import and Export. In modular languages, a name or group of names can be brought into a scope with an import clause. The module that provides the definitions must have a corresponding export clause.
  * Namespaces. The mechanisms above are inadequate for very large programs developed by large teams. Problems arise when a project uses several libraries that may have conflicting names. Namespaces, provided by PLs such as C++ and Common LISP, provide a higher level of name management than the regular language features.

  * Nested scopes are convenient in small regions, such as functions. The advantage of nested scopes for large structures, such as classes or modules, is doubtful.
  * Nesting is a form of scope management. For large structures, explicit control by name qualification may be better than nesting.
  * Qualified names work well at the level of classes and modules, when the source of names is obvious.
  * The import mechanism has the problem that the source of a name is not obvious where it appears: users must scan possibly remote import declarations.
  * Qualified names are inadequate for very large programs.
  * Library designers can reduce the potential for name conflicts by using distinctive prefixes. For example, all names supplied by the commercial graphics library FastGraph are have fg_ as a prefix.
  * Namespaces provide a better solution than prefixes.

Scope management is important because the programmer has no “work arounds” if the scoping mechanisms provided by the PL are inadequate. This is because PLs typically hide the scoping mechanisms. If a program is compiled, names will normally not be accessible at run-time unless they are included for a specific purpose such as debugging. In fact, one way to view compiling is as a process that converts names to numbers. Interpreters generally have a repository that contains the value of each name currently accessible to the program, but programmers may have limited access to this repository.

The extent, also called lifetime, of a name is the period of time during program execution during which the object corresponding to the name exists. Understanding the relation between scope and extent is an important part of understanding a PL.

Global names usually exist for the entire lifetime of the execution: they have global extent. In FORTRAN, local variables also exist for the lifetime of the execution. Programmers assume that, on entry to a subroutine, its local variables will not have changed since the last invocation of the subroutine.

In Algol 60 and subsequent stack-based languages, local variables are instantiated whenever control enters a block and they are destroyed (at least in principle) when control leaves the block. It is an error to create an object on the stack and to pass a reference to that object to an enclosing scope. Some PLs attempt to detect this error (e.g., Algol 68), some try to prevent it from occurring (e.g., Pascal), and others leave it as a problem for the programmer (e.g., C and C++).

In PLs that use a reference model, objects usually have unlimited extent, whether or not their original names are accessible. Examples include Simula, Smalltalk, Eiffel, CLU, and all FPLs. The advantage of the reference model is that the problems of disappearing objects (dangling pointers) and inaccessible objects (memory leaks) do not occur. The disadvantage is that GC and the accompanying overhead are more or less indispensable.

The separation of extent from scope was a key step in the evolution of post-Algol PLs.


Structure
---------

The earliest programs had little structure. Although a FORTRAN program has a structure — it consists of a main program and subroutines — the main program and the subroutines themselves are simply lists of statements.

Block Structure:

  Algol 60 was the first major PL to provide recursive, hierarchical structure. It is probably significant that Algol 60 was also the first language to be defined, syntactically at least, with a context-free grammar. It is straightforward to define recursive structures using context-free grammars — specifically, a grammar written in Backus-Naur Form (BNF) for Algol 60. The basis of Algol 60 syntax can be written in extended BNF as follows:

          PROG -> BLOCK
         BLOCK -> "begin" { DECL } { STMT } "end"
          STMT -> BLOCK | · · ·
          DECL -> FUNCDECL | · · ·
      FUNCDECL -> HEAD BLOCK

With blocks come nested scopes. Earlier languages had local scope (for example, FORTRAN subroutines have local variables) but scopes were continuous (no “holes”) and not nested. It was not possible for one declaration to override another declaration of the same name. In Algol 60, however, an inner declaration overrides an outer declaration.

Modules:

  By the early 70s, people realized that the “monolithic” structure of Algol-like programs was too restricted for large and complex programs. Furthermore, the “separate compilation” model of FORTRAN — and later C — was seen to be unsafe. The new model that was introduced by languages such as Mesa and Modula divided a large program into modules.

    * A module is a meaningful unit (rather than a collection of possibly unrelated declarations) that may introduce constants, types, variables, and functions (called collectively features).
    * A module may import some or all of its features.
    * In typical modular languages, modules can export types, functions, and variables.

  Parnas (1972) wrote a paper that made a number of important points:

    * A large program will be implemented as a set of modules.
    * Each programmer, or programming team, will be the owners of some modules and users of other modules.
    * It is desirable that a programmer should be able to change the implementation of a module without affecting (or even informing) other programmers.
    * This implies that a module should have an interface that is relatively stable and an implementation that changes as often as necessary. Only the owner of a module has access to its implementation; users do not have such access.

  Parnas introduced these ideas as the “principle of information hiding”. Today, it is more usual to refer to the goals as encapsulation. In order to use the module correctly without access to its implementation, users require a specification written in an appropriate language. The specification answer the question “What does this module do?” and the implementation answers the question “How does this module work?” Most people today accept Parnas’s principle, even if they do not apply it well. It is therefore important to appreciate how radical it was at the time of its introduction.

    "[Parnas] has proposed a still more radical solution. His thesis is that the programmer is most effective if shielded from, rather than exposed to, the details of construction of system parts other than his own. This presupposes that all interfaces are completely and precisely defined. While that is definitely sound design, dependence upon its perfect accomplishment is a recipe for disaster. A good information system both exposes interface errors and stimulates their correction." (Brooks 1975, page 78)

  Brooks wrote this in the first edition of The Mythical Man Month in 1975. Twenty years later, he had changed his mind.

    "Parnas was right, and I was wrong. I am now convinced that information hiding, today often embodied in object-oriented programming, is the only was of raising the level of software design." (Brooks 1995, page 272)

  Parnas’s paper was published in 1972, only shortly after Wirth’s (1971) paper on program development by stepwise refinement. Yet Parnas’s approach is significantly different from Wirth’s. Whereas Wirth assumes that a program can be constructed by filling in details, perhaps with occasional backtracking to correct an error, Parnas sees programming as an iterative process in which revisions and alterations are part of the normal course of events. This is a more “modern” view and certainly a view that reflects the actual construction of large and complex software systems.

Control Structures:

  Early PLs, such as FORTRAN, depended on three control structures:

    sequence;
    transfer (goto);
    call/return.

  These structures are essentially those provided by assembly language. The first contribution of PLs was to provide names for variables and infix operators for building expressions. The “modern” control structures first appeared in Algol 60. The most significant contribution of Algol 60 was the block : a program unit that could contain data, functions, and statements. Algol 60 also contributed the familiar if-then-else statement and a rather complex loop statement. However, Algol 60 also provided goto statements and most programmers working in the sixties assumed that goto was essential.

  The storm of controversy raised by Dijkstra’s (1968) letter condemning the goto statement indicates the extent to which programmers were wedded to the goto statement. Even Knuth (1974) entered the fray with arguments supporting the goto statement in certain circumstances, although his paper is well-balanced overall. The basic ideas that Dijkstra proposed were as follows.

    * There should be one flow into, and one flow out of, each control structure.
    * All programming needs can be met by three structures satisfying the above property: the sequence; the loop with preceding test (while-do); and the conditional (if-then-else).

  Dijkstra’s proposals had previously been justified in a formal sense by Bohm and Jacopini (1966). This paper establishes that any program written with goto statements can be rewritten as an equivalent program that uses sequence, conditional, and loop structures only; it may be necessary to introduce Boolean variables. Although the result is interesting, Dijkstra’s main point was not to transform old, goto-ridden programs but to write new programs using simple control structures only.

  Despite the controversy, Dijkstra’s arguments had the desired effect. In languages designed after 1968, the role of goto was down-played. Both C and Pascal have goto statements but they are rarely used in well-written programs. C, with break, continue, and return, has even less need for goto than Pascal. Ada has a goto statement because it was part of the US DoD requirements but, again, it is rarely needed. The use of control structures rather than goto statements has several advantages:

    * It is easier to read and maintain programs. Maintenance errors are less common.
    * Precise reasoning (for example, the use of formal semantics) is simplified when goto statements are absent.
    * Certain optimizations become feasible because the compiler can obtain more precise information in the absence of goto statements.

  The control structure that has elicited the most discussion is the loop. The basic loop has a single test that precedes the body: while E do S; It is occasionally useful to perform the test after the body: Pascal provides repeat/until and C provides do/while. This is not quite enough, however, because there are situations in which it is useful to exit from a loop from within the body. Consider, for example, the problem of reading and adding numbers, terminating when a negative number is read without including the negative number in the sum. Some recent languages, recognizing this problem, provide only one form of loop statement consisting of an unconditional repeat and a conditional exit that can be used anywhere in the body of the loop.

  Some languages, such as Ada and Pascal, distinguish the terms procedure and function. Both names refer to “subroutines” — program components that perform a specific task when invoked from another component of the program. A procedure is a subroutine that has some effect on the program data but does not return a result. A function is a subroutine that returns a result; a function may have some effect on program data, called a “side effect”, but this is often considered undesirable.

  Languages that make a distinction between procedures and functions usually also make a distinction between “actions” and “data”. For example, at the beginning of the first Pascal text (Jensen and Wirth 1976), we find the following statement: "An algorithm or computer program consists of two essential parts, a description of actions which are to be performed, and a description of the data, which are manipulated by the actions. Actions are described be so-called statements, and data are described by so-called declarations and definitions."

  In other languages, such as Algol and C, the distinction between “actions” and “data” is less emphasized. In these languages, every “statement” has a value as well as an effect. Consistently, the distinction between “procedure” and “function” also receives less emphasis. C, for example, does not have keywords corresponding to procedure and function. There is a single syntax for declaring and defining all functions and a convention that, if a function does not return a result, it is given the return type void. We can view the progression here as a separation of concerns. The “concerns” are: whether a subroutine returns a value; and whether it has side-effects. Ada and Pascal combine the concerns but, for practical reasons, allow “functions with side-effects”. In C, there are two concerns: “having a value” and “having an effect” and they are essentially independent.

  The basic control structures are fine for most applications provided that nothing goes wrong. Although in principle we can do everything with conditions and loops, there are situations in which these constructs are inadequate. PL/I (Section 4.5) was the first major language to provide exception handling. C provided primitives (setjmp/longjmp) that made it possible to implement exceptin handling.


Issues in OOP
-------------

Algorithms + Data Structures = Programs ?

  The distinction between code and data was made at an early stage. It is visible in FORTRAN and reinforced in Algol 60. Algol provided the recursive block structure for code but almost no facilities for data structuring. Algol 68 and Pascal provided recursive data structures but maintained the separation between code and data. LISP brought algorithms (in the form of functions) and data structures together, but: 1. the mutability of data was downplayed; 2. subsequent FPLs reverted to the Algol model, separating code and data. Simula started the move back towards the von Neumann Architecture at a higher level of abstraction than machine code: the computer was recursively divided into smaller computers, each with its own code and stack. This was the feature of Simula that attracted Alan Kay and led to Smalltalk (Kay 1996). Many OOPLs since Smalltalk have withdrawn from its radical ideas. C++ was originally called “C with classes” (Stroustrup 1994) and modern C++ remains an Algol-like systems programming language that supports OOP. Ada has shown even more reluctance to move in the direction of OOP, although Ada 9X has a number of OO features.

Valuses vs Objects

  Some data behave like mathematical objects and other data behave like representations of physical objects (MacLennan 1983).

  Consider the sets { 1, 2, 3 } and { 2, 3, 1 }. Although these sets have different representations (the ordering of the members is different), they have the same value because in set theory the only important property of a set is the members that it contains: “two sets are equal if and only if they contain the same members”. Consider two objects representing people:

    [name="Peter", age=55]
    [name="Peter", age=55]

  Although these objects have the same value, they might refer to different people who have the same name and age. The consequences of assuming that these two objects denote the same person could be serious: in fact, innocent people have been arrested and jailed because they had the misfortune to have the same name and social security number as a criminal.

  Values:
  * Integers have values. For example, the integer 6 is a value.
  * Values are abstract. The value 7 is the common property shared by all sets with cardinality 7.
  * Values are immutable (i.e., do not change).
  * It does not make sense to change a value. If we change the value 3, it is no longer 3.
  * Values need representations. We need representations in order to compute with values. The representations 7, VII, sept, sju, and a particular combination of bits in the memory of a computer are different representations of the same abstract value.
  * It does not make sense to talk about the identity of a value. Suppose we evaluate 2 + 3, obtaining 5, and then 7 − 2, again obtaining 5. Is it the same 5 that we obtain from the second calculation?
  * We can copy a representation of a value, but copying a value is meaningless.
  * We consider different representations of the same value to be equal.
  * Aliasing is not a problem for values. It is immaterial to the programmer, and should not even be detectable, whether two instances of a value are stored in the same place or in different places.
  * Functional and logical PLs usually provide values.
  * Programs that use values only have the property of referential transparency.
  * Small values can be held in machine words. Large values are best represented by references (pointers to the data structure representing the value.
  * The assignment operator for values should be a reference assignment. There is no need to copy values, because they do not change.

  Objects:
  * Objects possess values as attributes. For example, a counter might have 6 as its current value.
  * Objects are concrete. An object is a particular collection of sub-objects and values. A counter has one attribute: the value of the count.
  * Objects are usually, but not necessarily, mutable (i.e., may change).
  * It is meaningful, and often useful, to change the value of an object. For example, we can increment a counter.
  * Objects need representations. An object has subobjects and values as attributes.
  * Identity is an important attribute of objects. Even objects with the same attributes may have their own, unique identities: consider identical twins.
  * Copying an object certainly makes sense. Sometimes, it may even be useful, as when we copy objects from memory to disk, for example. However, multiple copies of an object with a supposedly unique identity can cause problems, known as “integrity violations” in the database community.
  * An object is equal to itself. Whether two distinct objects with equal attributes are equal depends on the application. A person who is arrested because his name and social insurance number is the same as those of a wanted criminal may not feel “equal to” the criminal.
  * All references to a particular object should refer to the same object. This is sometimes called “aliasing”, and is viewed as harmful, but it is in fact desirable for objects with identity.
  * Object oriented PLs should provide objects.
  * Programs that provide objects do not provide referential transparency, but may have other desirable properties such as encapsulation.
  * Very small objects may be held in machine words. Most objects should be represented by references (pointers to the data structure holding the object.
  * The assignment operator for objects should be a reference assignment. Objects should not normally be copied, because copying endangers their integrity.

  Conclusions:
  * The CM must provide values, may provide objects.
  * What is “small” and what is “large”? This is an implementation issue: the compiler must decide.
  * What is “primitive” and what is “user defined”? The PL should minimize the distinction as much as possible.
  * Strings should behave like values. Since they have unbounded size, many PLs treat them as objects.
  * Copying and comparing are semantic issues. Compilers can provide default operations and “building bricks”, but they cannot provide fully appropriate implementations.

Classes vs Prototypes

  The most widely-used OOPLs — C++, Smalltalk, and Eiffel — are class-based. Classes provide a visible program structure that seems to be reassuring to designers of complex software systems. Prototype languages are a distinct but active minority. The relation between classes and prototypes is somewhat analogous to the relation between procedural (FORTRAN and Algol) and functional (LISP) languages in the 60s. Prototypes provide more flexibility and are better suited to interactive environments than classes. Although large systems have been built using prototype languages, these languages seem to be more often used for exploratory programming.

Types vs Objects

  In most class-based PLs, functions are associated with objects. This has the effect of making binary operations asymmetric. For example, x = y is interpreted as x.equal(y) or, approximately, “send the message equal with argument y to the object x”. It is not clear whether, with this interpretation, y = x means the same as x = y. The problem is complicated further by inheritance: if class C is a subclass of class P , can we compare an instance of C to an instance of P , and is this comparison commutative? Some of the problems can be avoided by associating functions with types rather than with objects. This is the approach taken by CLU and some of the “functional” OOPLs. An expression such as x = y is interpreted in the usual mathematical sense as equal(x,y). However, even this approach has problems if x and y do not have the same type.

Pure vs Hybrid

  Smalltalk, Eiffel, and Java are pure OOPLs: they can describe only systems of objects and programmers are forced to express everything in terms of objects. C++ is a hybrid PL in that it supports both procedural and object styles. Once the decision was made to extend C rather than starting afresh (Stroustrup 1994, page 43), it was inevitable that C++ could be a hybrid language. In fact, its precursor, “C with classes”, did not even claim to be object oriented. A hybrid language can be dangerous for inexperienced programmers because it encourages a hybrid programming style. OO design is performed perfunctorily and design problems are solved by reverting to the more general, but lower level, procedural style. (This is analogous to the early days of “high level” programming when programmers would revert to assembler language to express constructs that were not easy or possible to express at a high level.) For experienced programmers, and especially those trained in OO techniques, a hybrid language is perhaps better than a pure OOPL. These programmers will use OO techniques most of the time and will only resort to procedural techniques when these are clearly superior to the best OO solution. As OOPLs evolve, these occasions should become rarer. Perhaps “pure or hybrid” is the wrong question. A better question might be: suppose a PL provides OOP capabilities. Does it need to provide anything else? If so, what? One answer is: FP capabilities. The multi-paradigm language LEDA (Budd 1995) demonstrates that it is possible to provide functional, logic, and object programming in a single language. However, LEDA provides these in a rather ad hoc way. It more tightly-integrated design would be preferable, but is harder to accomplish.

Closures vs Classes

  The most powerful programming paradigms developed so far are those that do not separate code and data but instead provide “code and data” units of arbitrary size. This can be accomplished by high order functions and mutable state, as in Scheme (Section 5.2) or by classes. Although each method has advantages for certain applications, the class approach seems to be better in most practical situations.

Inheritance

  Inheritance is one of the key features of OOP and at the same time one of the most troublesome. Conceptually, inheritance can be defined in a number of ways and has a number of different uses. In practice, most (but not all!) OOPLs provide a single mechanism for inheritance and leave it to the programmer to decide how to use the mechanism. Moreover, at the implementation level, the varieties of inheritance all look much the same — a fact that discourages PL designers from providing multiple mechanisms. One of the clearest examples of the division is Meyer’s (1988) “marriage of convenience” in which the class ArrayStack inherits from Array and Stack. The class ArrayStack behaves like a stack — it has functions such as push, pop, and so on — and is represented as an array. There are other ways of obtaining this effect — notably, the class ArrayStack could have an array object as an attribute — but Meyer maintains that inheritance is the most appropriate technique, at least in Eiffel.

  Whether or not we agree with Meyer, it is clear that there two different relations are involved. The relation ArrayStack ~ Stack is not the same as the relation ArrayStack ~ Array. Hence the question: should an OOPL distinguish between these relations or not? Based on the “marriage of convenience” the answer would appear to be “yes” — two distinct ends are achieved and the means by which they are achieved should be separated. A closer examination, however, shows that the answer is by no means so clear-cut. There are many practical programming situations in which code in a parent class can be reused in a child class even when interface consistency is the primary purpose of inheritance.

A Critique of C++

  C++ is both the most widely-used OOPL and the most heavily criticized. Further criticism might appear to be unnecessary. The purpose of this section is to show how C++ fails to fulfill its role as the preferred OOPL. The first part of the following discussion is based on a paper by LaLonde and Pugh (1995) in which they discuss C++ from the point of view of a programmer familiar with the principles of OOP, exemplified by a language such as Smalltalk, but who is not familiar with the details of C++.

  We begin by writing a specification for a simple BankAccount class with an owner name, a balance, and functions for making deposits and withdrawals. Data members are prefixed with “p” to distinguish them from parameters with similar names. The instance variable pTrans is a pointer to an array containing the transactions that have taken place for this account. Functions that operate on this array have been omitted. We can create instances of BankAccount on either the stack or the heap:

    BankAccount account1("John"); // on the stack
    BankAccount *account2 = new BankAccount("Jane"); // on the heap

  (J: object on stack ... I think it's cool)

  The first account, account1, will be deleted automatically at the end of the scope containing the declaration. The second account, account2, will be deleted only when the program executes a delete operation. In both cases, the destructor will be invoked. The notation for accessing functions depends on whether the variable refers to the object directly or is a pointer to the object.

    account1.balance()
    account2->balance()

  A member function can access the private parts of another instance of the same class which comes as a surprise to Smalltalk programmers because, in Smalltalk, an object can access only its own components. The next step is to define the member functions of the class BankAccount and write a simple test of the class BankAccount. This test introduces two bugs. The array account1.pTrans is lost because it is over-written by account2.pTrans. At the end of the test, account2.pTrans will be deleted twice, once when the destructor for account1 is called, and again when the destructor for account2 is called. Although most introductions to C++ focus on stack objects, it is more interesting to consider heap objects.

  The code creating three special accounts is rather specialized. We generalizes it so that we can enter the account owner’s names interactively. The problem with this code is that all of the bank accounts have the same owner: that of the last name entered. This is because each account contains a pointer to the unique buffer. We can fix this by allocating a new buffer for each account. This version works, but is poorly designed. Deleting the owner string should be the responsibility of the object, not the client. The destructor for BankAccount should delete the owner string. After making this change, we can remove the destructor for owner. Unfortunately, this change introduces another problem. Suppose we use the loop to create two accounts and then create the third account with this statement:

    accounts[2] = new BankAccount("Tina");

  The account destructor fails because "Tina" was not allocated using new. To correct this problem, we must modify the constructor for BankAccount. The rule we must follow is that data must be allocated with the constructor and deallocated with the destructor, or controlled entirely outside the object. The next problem occurs when we try to use the overloaded function owner() to change the name of an account owner.

    accounts[2]->owner("Fred");

  Once again, we have introduced two bugs: 1. The original owner of accounts[2] will be lost. 2. When the destructor is invoked for accounts[2], it will attempt to delete "Fred", which was not allocated by new. To correct this problem, we must rewrite the member function owner(). We now have bank accounts that work quite well, provided that they are allocated on the heap. If we try to mix heap objects and stack objects, however, we again encounter difficulties. The error occurs because transfer expects the address of an account rather than an actual account.  We correct it by introducing the reference operator, &.

    account1->transfer(&account3, 10.0);

  We could make things easier for the programmer who is using accounts by declaring an overloaded version of transfer() as shown below. Note that, we must use the operator “.” rather than “->”. This is because other behaves like an object, although in fact an address has been passed. The underlying problem here is that references (&) work best with stack objects and pointers (*) work best with heap objects. It is awkward to mix stack and heap allocation in C++ because two sets of functions are needed. If stack objects are used without references, the compiler is forced to make copies. To implement the code (a function pass in and return object on stack), C++ uses the (default) copy constructor for BankAccount twice: first to make a copy of account2 to pass to test() and, second, to make another copy of account2 that will become myAccount. At the end of the scope, these two copies must be deleted. The destructors will fail because the default copy constructor makes a shallow copy — it does not copy the pTrans fields of the accounts. The solution is to write our own copy constructor and assignment overload for class BankAccount. The function pDuplicate() makes a deep copy of an account the function; it must make copies of all of the components of a back account object and destroy all components of the old object. The function pDelete has the same effect as the destructor. It is recommended practice in C++ to write these functions for any class that contains pointers.

  LaLonde and Pugh continue with a discussion of infix operators for user-defined classes. If the objects are small — complex numbers, for example — it is reasonable to store the objects on the stack and to use call by value. C++ is well-suited to this approach: storage management is automatic and the only overhead is the copying of values during function call and return. If the objects are large, however, there are problems. Suppose that we want to provide the standard algebraic operations (+, −, *) for 4 × 4 matrices. (This would be useful for graphics programming, for example.) Since a matrix occupies 4×4×8 = 128 bytes, copying a matrix is relatively expensive and the stack approach will be inefficient. But if we write functions that work with pointers to matrices, it is difficult to manage allocation and deallocation. Even a simple function call such as project(A + B); will create a heap object corresponding to A + B but provides no opportunity to delete this object. (The function cannot delete it, because it cannot tell whether its argument is a temporary.) The same problem arises with expressions such as A = B + C * D; It is possible to implement the extended assignment operators, such as +=, without losing objects. But this approach reduces us to a kind of assembly language style of programming.

  These observations by LaLonde and Pugh seem rather hostile towards C++. They are making a comparison between C++ and Smalltalk based only on ease of programming. The comparison is one-sided because they do not discuss factors that might make C++ a better choice than Smalltalk in some circumstances, such as when efficiency is important. Nevertheless, the discussion makes an important point: the “stack model” (with value semantics) is inferior to the “heap model” (with reference semantics) for many OOP applications. It is significant that most language designers have chosen the heap model for OOP (Simula, Smalltalk, CLU, Eiffel, Sather, Dee, Java, Blue, amongst others) and that C++ is in the minority on this issue.

  Another basis for criticizing C++ is that is has become, perhaps not intentionally, a hacker’s language. A sign of good language design is that the features of the language are used most of the time to solve the problems that they were intended to solve. A sign of a hacker’s language is that language “gurus” introduce “hacks” that solve problems by using features in bizarre ways for which they were not originally intended.

  Input and output in C++ is based on “streams”. A typical output statement looks something like this:

    cout << "The answer is " << answer << ’.’ << endl;

  The operator << requires a stream reference as its left operand and an object as its right operand. Since it returns a stream reference (usually its left operand), it can be used as an infix operator in compound expressions such as the one shown. The notation seems clear and concise and, at first glance, appears to be an elegant way of introducing output and input (with the operator >>) capabilities.

  Obviously, << is heavily overloaded. There must be a separate function for each class for which instances can be used as the right operand. (<< is also overloaded with respect to its left operand, because there are different kinds of output streams.) In the example above, the first three right operands are a string, a double, and a character. The first problem with streams is that, although they are superficially object oriented (a stream is an object), they do not work well with inheritance. Overloading provides static polymorphism which is incompatible with dynamic polymorphism. It is possible to write code that uses dynamic binding but only by providing auxiliary functions.

  The final operand, endl, writes end-of-line to the stream and flushes it. It is a global function with signature ios & endl (ios & s) A programmer who needs additional “manipulators” of this kind has no alternative but to add them to the global name space. Things get more complicated when formatting is added to streams. For example, we could change the example above to:

    cout << "The answer is " << setprecision(8) << answer << ’.’ << endl;

  The first problem with this is that it is verbose. (The verbosity is not really evident in such a small example, but it is very obvious when streams are used, for example, to format a table of heterogeneous data.) The second, and more serious problem, is the implementation of setprecision. The way this is done is (roughly) as follows (Teale 1993, page 171). First, a class for manipulators with integer arguments is declared, and the inserter function is defined. Second, the function that we need can be defined. It is apparent that a programmer must be quite proficient in C++ to understand these definitions, let alone design similar functions. To make matters worse, the actual definitions are not as shown here, because that would require class declarations for many different types, but are implemented using templates. There are many other tricks in the implementation of streams. For example, we can use file handles as if they had boolean values. It seems that there is a boolean value, infile, that we can negate, obtaining !infile. In fact, infile can be used as a conditional because there is a function that converts infile to (void *), with the value NULL indicating that the file is not open, and !infile can be used as a conditional because the operator ! is overloaded for class ifstream to return an int which is 0 if the file is open.

  These are symptoms of an even deeper malaise in C++ than the problems pointed out by authors of otherwise excellent critiques such as (Joyner 1992; Sakkinen 1992). Most experienced programmers can get used to the well-known idiosyncrasies of C++, but few realize that frequently-used and apparently harmless constructs are implemented by subtle and devious tricks. Programs that look simple may actually conceal subtle traps.


Conclusion
----------

Values and Objects. Mathematically oriented views of programming favour values. Simulation oriented views favour objects. Both views are useful for particular applications. A PL that provides both in a consistent and concise way might be simple, expressive, and powerful. The “logical variable” (a variable that is first unbound, then bound, and later may be unbound during backtracking) is a third kind of entity, after values and objects. It might be fruitful to design a PL with objects, logical variables, unification, and backtracking. Such a language would combine the advantages of OOP and logic programming.

Making Progress. It is not hard to design a large and complex PL by throwing in numerous features. The hard part of design is to find simple, powerful abstractions — to achieve more with less. In this respect, PL design resembles mathematics. The significant advances in mathematics are often simplifications that occur when structures that once seemed distinct are united in a common abstraction. Similar simplifications have occurred in the evolution of PLs: for example, Simula and Scheme. But programs are not mathematical objects. A rigorously mathematical approach can lead all too rapidly to the “Turing tar pit”.

Practice and Theory. The evolution of PLs shows that, most of the time, practice leads theory. Designers and implementors introduce new ideas, then theoreticians attempt to what they did and how they could have done it better. There are a number of PLs that have been based on purely theoretical principles but few, if any, are in widespread use. Theory-based PLs are important because they provide useful insights. Ideally, they serve as testing environments for ideas that may eventually be incorporated into new mainstream PLs. Unfortunately, it is often the case that they show retroactively how something should have been done when it is already too late to improve it.

Multiparadigm Programming. PLs have evolved into several strands: procedural programming, now more or less subsumed by object oriented programming; functional programming; logic programming and its successor, constraint programming. Each paradigm performs well on a particular class of applications. Since people are always searching for “universal” solutions, it is inevitable that some will try to build a “universal” PL by combining paradigms. Such attempts will be successful to the extent that they achieve overall simplification. It is not clear that any future universal language will be accepted by the programming community; it seems more likely that specialized languages will dominate in the future.

