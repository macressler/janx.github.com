Type Theory
-----------

  T f (S x) { B; return y; }

The declaration introduces a function called f that takes an argument, x of type S, performs calculations indicated as B, and returns a result, y of type T . If there is a type theory associated with the language, we should be able to prove a theorem of the following form:

  If x has type S then the evaluation of f (x) yields a value of type T .

The reasoning used in the proof is based on the syntactic form of the function body, B. If we can do this for all legal programs in a language L, we say that L is statically typed . If L is indeed statically typed:

  A compiler for L can check the type correctness of all programs.
  A program that is type-correct will not fail because of a type error when it is executed.

A program that is type-correct may nevertheless fail in various ways. Type checking does not usually detect errors such as division by zero. (In general, most type checking systems assume that functions are total : a function with type S -> T , given a value of type S, will return a value of type T. Some commonly used functions are partial : for example x/y ≡ divide(x,y) is undefined for y = 0.) A program can be type-correct and yet give completely incorrect answers. In general:

  Most PLs are not statically typed in the sense defined above, although the number of loopholes in the type system may be small.
  A type-correct program may give incorrect answers.
  Type theories for modern PLs, with objects, classes, inheritance, overloading, genericity, dynamic binding, and other features, are very complex.

Type theory leads to attractive and interesting mathematics and, consequently, tends to be overvalued by the theoretical Computer Science community. The theory of program correctness is more difficult and has attracted less attention, although it is arguably more important.


Regular Languages
-----------------

In the formal study of regular languages, we begin with an alphabet, Σ, which is a finite set of symbols. For example, we might have Σ = { 0, 1 }. The set of all finite strings, including the empty string, constructed from the symbols of Σ is written Σ*. We then introduce a variety of regular expressions, which we will refer to as RE here. (The abbreviation RE is also used to stand for “recursively enumerable”, but in this section it stands for “regular expression”.) Each form of RE is defined by the set of strings in Σ* that it generates. This set is called a “regular language”.

  1. O/ is RE and denotes the empty set.
  2. e (the string containing no symbols) is RE and denotes the set {e}.
  3. For each symbol x ∈ Σ, x is RE and denotes the set { x }.
  4. If r is RE with language R, then r* is RE and denotes the set R* (where R* = { x1 x2 . . . xn | n ≥ 0 ∧ xi ∈ R } ).
  5. If r and s are RE with languages R and S respectively, then (r + s) is RE and denotes the set R U S.
  6. If r and s are RE with languages R and S respectively, then (rs) is RE and denotes the set RS (where RS = { rs | r ∈ R ∧ s ∈ S } ).

We add two further notations that serve solely as abbreviations.

  1. The expression a^n represents the string aa . . . a containing n occurrences of the symbol a.
  2. The expression a+ is an abbreviation for the RE aa∗ that denotes the set { a, aa, aaa, . . . }.

We can use regular languages to describe the tokens (or lexemes) of most PLs. For example:

  UC = A + B + ··· + Z
  LC = a + b + ··· + z
  LETTER = UC + LC
  DIGIT = 0 + 1 + ··· + 9
  IDENTIFIER = LETTER (LETTER + DIGIT)*
  INTCONST = DIGIT+
  FLOATCONST = DIGIT+ . DIGIT*
  ....

We can use a program such as lex to construct a lexical analyzer (scanner) from a regular expression that defines the tokens of a PL.

  RE               Control Structure
  -----------------------------------------------------------
  x                statement
  rs               statement; statement
  r*               while expression do statement
  r+s              if condition then statement else statement

  RE               Data Structure
  ----------------------------------
  x                int n;
  rs               struct { int n; float y; }
  r^n              int a[n];
  r*               int a[]
  r+s              union { int n; float y; }

These analogies suggest that the mechanisms for constructing REs — concatenation, alternation, and repetition — are somehow fundamental. In particular, the relation between the standard control structures and the standard data structures can be helpful in programming. In Jackson Structured Design (JSD), for example, the data structures appropriate for the application are selected first and then the program is constructed with the corresponding control structures. The limitations of REs are also interesting. When we move to the next level of the Chomsky hierarchy, Context Free Languages (CFLs), we obtain the benefits of recursion. The corresponding control structure is the procedure and the corresponding data structures are recursive structures such as lists and trees (Wirth 1976, page 163).


Context Free Grammers
---------------------

Context free grammars for PLs are usually written in BNF (Backus-Naur Form). A grammar rule, or production, consists of a non-terminal symbol (the symbol being defined), a connector (usually ::= or ->), and a sequence of terminal and non-terminal symbols. The syntax for the assignment, for example, might be defined:

  ASSIGNMENT -> VARIABLE “ := ” EXPRESSION

Basic BNF provides no mechanism for choice: we must provide one rule for each possibility. Also, BNF provides no mechanism for repetition; we must use recursion instead. The following example illustrates both of these limitations.

  SEQUENCE -> EMPTY
  SEQUENCE -> STATEMENT SEQUENCE

BNF has been extended in a variety of ways. With a few exceptions, most of the extended forms can be described simply: the sequence of symbols on the right of the connector is replaced by a regular expression. The extension enables us to express choice and repetition within a single rule. In grammars, the symbol | rather than + is used to denote choice.

  STATEMENT = ASSIGNMENT | CONDITIONAL | · · ·
  SEQUENCE = ( STATEMENT )*

Extended BNF (EBNF) provides a more concise way of describing grammars than BNF. Just as parsers can be constructed from BNF grammars, they can be constructed from EBNF grammars.


The Procedural Paradigm
-----------------------

The introduction of the von Neumann architecture was a crucial step in the development of electronic computers. The basic idea is that instructions can be encoded as data and stored in the memory of the computer. The first consequence of this idea is that changing and modifying the stored program is simple and efficient. In fact, changes can take place at electronic speeds, a very different situation from earlier computers that were programmed by plugging wires into panels. The second, and ultimately more far-reaching, consequence is that computers can process programs themselves, under program control. In particular, a computer can translate a program from one notation to another. Thus the stored-program concept led to the development of programming “languages”.

The first PLs evolved from machine code. The first programs used numbers to refer to machine addresses. One of the first additions to programming notation was the use of symbolic names rather than numbers to represent addresses. This establishes the principle that a variable name stands for a memory location, a principle that influenced the subsequent development of PLs and is now known — perhaps inappropriately — as value semantics.

The importance of subroutines and subroutine libraries was recognized before high-level programming languages had been developed, as the following quotation shows. The following advantages arise from the use of such a library:

  1. It simplifies the task of preparing problems for the machine;
  2. It enables routines to be more readily understood by other users, as conventions are standardized and the units of a routine are much larger, being subroutines instead of individual orders;
  3. Library subroutines may be used directly, without detailed coding and punching;
  4. Library subroutines are known to be correct, thus greatly reducing the overall chance of error in a complete routine, and making it much easier to locate errors.

Another difficulty arises from the fact that, although it is desirable to have subroutines available to cover all possible requirements, it is also undesirable to allow the size of the resulting library to increase unduly. However, a subroutine can be made more versatile by the use of parameters associated with it, thus reducing the total size of the library.

We may divide the parameters associated with subroutines into two classes.

  EXTERNAL parameters, i.e. parameters which are fixed throughout the solution of a problem and arise solely from the use of the library;
  INTERNAL parameters, i.e. parameters which vary during the solution of the problem.

Subroutines may be divided into two types, which we have called OPEN and CLOSED. An open subroutine is one which is included in the routine as it stands whereas a closed subroutine is placed in an arbitrary position in the store and can be called into use by any part of the main routine. (Wheeler 1951)

Machine code is a sequence of “orders” or “instructions” that the computer is expected to execute. The style of programming that this viewpoint developed became known as the “imperative” or “procedural” programming paradigm. In these notes, we use the term “procedural” rather than “imperative” because programs resemble “procedures” (in the English, non-technical sense) or recipes rather than “commands”. Confusingly, the individual steps of procedural PLs, such as Pascal and C, are often called “statements”, although in logic a “statement” is a sentence that is either true or false.

By default, the commands of a procedural program are executed sequentially. Procedural PLs provide various ways of escaping from the sequence. The earliest mechanisms were the “jump” command, which transferred control to another part of the program, and the “jump and store link” command, which transferred control but also stored a “link” to which control would be returned after executing a subroutine.

The data structures of these early languages were usually rather simple: typically primitive values (integers and floats) were provided, along with single- and multi-dimensioned arrays of primitive values.

* FORTRAN: 1957. for IBM 704; Formula Translator; like assembly + math expressions; no recursion; no structure; unsafe memory allocation;

* Algol 60: Independent of any company/computer; ACM require algorithm submissions described in Algol; Block structure and stacked Activation Record; Dynamic Arrays on stack; Call by Name translate parameter to trunk (procedure without param); Own variables (like static vars in C)

* COBOL: Structured data and Data processing, the first time PL dealing with non-numbers, like text; implicit type conversion;

* PL/I: Enhanced implicit type conversion (like js); storage class (control variable extent); explict allocation on heap; simple exception handling; Dommed to failure: "CobAlgoltran", universal language for all kinds of programming; a programmer could learn/use a subset of the language for a particular problem

* Algol 68: Orthogonality; Specification described completely in formal language; Operator overloading, even priority can be altered; uniform notation for declarations and other entiies; collateral clause in which expressions will be evaluated in any order or concurrently; Collateral clauses provide a good, and early, example of the idea that a PL specification should intentionally leave some implementation details undefined; operator 'ref' for reference (* and & in C)

* Pascal: demonstrate PL can be simple but powerful; Type system based on primitives and mechanisms for building structured types; data type declaration; static type checking; recursive data type; no implicit type conversion; development by stepwise refinement (Wirth); monolithic structure prevents independt complilation

* Modula-2: learned from Pascal by Wirth; introduce module with sperated interface and implementation; coroutines

* C: low level; simple; concise; pointer; system programming

* Ada: the last major effort in procedural language design; a large and complex language that combines then-known programming features with little attempt at consolidation; the first widely-used language to provide full support for concurrency, with interactions checked by the compiler


The Functional Paradigm
-----------------------

Procedural programming is based on instructions (“do something”) but, inevitably, procedural PLs also provide expressions (“calculate something”). The key insight of functional programming (FP) is that everything can be done with expressions: the commands are unnecessary.

This point of view has a solid foundation in theory. Turing (1936) introduced an abstract model of “programming”, now known as the Turing machine. Kleene (1936) and Church (1941) introduced the theory of recursive functions. The two theories were later shown (by Kleene) to be equivalent: each had the same computational power. Other theories, such as Post production systems, were shown to have the same power. This important theoretical result shows that FP is not a complete waste of time but it does not tell us whether FP is useful or practical. To decide that, we must look at the functional programming languages (FPLs) that have actually been implemented.

Most functional language support high order functions. Roughly, a high order function is a function that takes another function as a parameter or returns a function. More precisely:

  A zeroth order expression contains only variables and constants.
  A first order expression may also contain function invocations, but the results and parameters of functions are variables and constants (that is, zeroth order expressions).
  In general, in an n-th order expression, the results and parameters of functions are (n−1)th order expressions.

A high order expression is an n-th order expression with n ≥ 2. The same conventions apply in logic with “function” replaced by “function or predicate”. In first-order logic, quantifiers can bind variables only; in a high order logic, quantifiers can bind predicates.

In procedural PLs, a name denotes a storage location (value semantics). In LISP, a name is a reference to an object, not a location (reference semantics). (consequence is there will be garbages - objects with no reference). A PL in which variable names are references to objects in memory is said to have reference semantics. All FPLs and most OOPLs have reference semantics.

Note that reference semantics is not the same as “pointers” in languages such as Pascal and C. A pointer variable stands for a location in memory and therefore has value semantics; it just so happens that the location is used to store the address of another object.

* LISP: 1958. provide list processing (like IPL or FLPL); prefix notation; high order functions and notation for functions; garbage collector; first compiler written in the language it compiles; lambda expressions to denote functions; dynamic scoping; interpretation

        The close relationship between code and data in LISP mimics the von Neumann architecture at a higher level of abstraction; 

        Another way to show that LISP was neater than Turing machines was to write a universal LISP function and show that it is briefer and more comprehensible than the description of a universal Turing machine. This was the LISP function eval [e, a], which computes the value of a LISP expression e, the second argument a being a list of assignments of values to variables. . . . Writing eval required inventing a notation for representing LISP functions as LISP data, and such a notation was devised for the purpose of the paper with no thought that it would be used to express LISP programs in practice. (McCarthy 1978)

        The lambda expression itself cannot be evaluated. Consequently, LISP had to resort to programming tricks to make higher order functions work. For example, if we want to pass the squaring function as an argument to another function, we must wrap it up in a “special form”

        Although dynamic scoping is natural for an interpreter, it is inefficient for a compiler. Interpreters are slow anyway, and the overhead of searching a linear list for a variable value just makes them slightly slower still. A compiler, however, has more efficient ways of accessing variables, and forcing it to maintain a linear list would be unacceptably inefficient. Consequently, early LISP systems had an unfortunate discrepancy: the interpreters used dynamic scoping and the compilers used static scoping. Some programs gave one answer when interpreted and another answer when compiled!

* Scheme: 1975. actors model (alpha operation); prove alpha == lambda; closure, is the value of function; lexical/static scoping; continuations; higher order functions + control of state = simple form of OOP

* SASL: 1976. compile by combinator reduction; call by name optimization (call by need/lazy evaluation); short-circuit is lazy eval in fact

* SML: "Standard" ML; "metalanguage", ML; reasoning about programs; statically typed; inferred types; function declaration by cases (pattern matching); function composition; currying


Object Oriented Paradigm
------------------------

OOP provides a way of building programs by incremental modification. Programs can often be extended by adding new code rather than altering existing code. The mechanism for incremental modification without altering existing code is inheritance. However, there are several different ways in which inheritance can be defined and used, and some of them conflict.

Class-based OOPLs can be designed so as to support information hiding (encapsulation) — the separation of interface and implementation. However, there may be conflicts between inheritance and encapsulation. An interesting, but rarely noted, feature of OOP is that the call graph is created and modified during execution. This is in contrast to procedural languages, in which the call graph can be statically inferred from the program text.

* Simula: 1965 (Simula I). coroutine; multiple stacks; classes; inheritance (prefix block); garbage collector

    Deleting the procedure definitions and final statement from an Algol block gives a pure data record.
    Deleting the final statement from an Algol block, leaving procedure definitions and data declarations, gives an abstract data object.
    Adding coroutine constructs to Algol blocks provides quasi-parallel programming capabilities.
    Adding a prefix mechanism to Algol blocks provides an abstraction mechanism (the class hierarchy).

    Note that the class is syntactically more like a procedure than a data object: it has an (optional) formal parameter and it must be called with an actual parameter. It differs from an ordinary Algol procedure in that its AR remains on the heap after the invocation.

* Smalltalk: 1972 (first compiler, based on BASIC). inspired by LISP and Simula; block; private data/public function; message exchange; single root inheritance; coroutines; garbage collector

    Six principles:
    1. Everything is an object.
    2. Objects communicate by sending and receiving messages (in terms of objects).
    3. Objects have their own memory (in terms of objects).
    4. Every object is an instance of a class (which must be an object).
    5. The class holds the shared behaviour for its instances (in the form of objects in a program list).
    6. To [evaluate] a program list, control is passed to the first object and the remainder is treated as its message.

    Compare to Simula:
    1. Simula distinguishes primitive types, such as integer and real, from class types. In Smalltalk, “everything is an object”.
    2. In particular, classes are objects in Smalltalk. To create a new instance of a class, you send a message to it. Since a class object must belong to a class, Smalltalk requires metaclasses.
    3. Smalltalk effectively eliminates passive data. Since objects are “active” in the sense that they have methods, and everything is an object, there are no data primitives.
    4. Smalltalk is a complete environment, not just a compiler. You can edit, compile, execute, and debug Smalltalk programs without ever leaving the Smalltalk environment.

* CLU: 1974. information hiding; ADT; up/down (cast); mutable/immutable; modules/classes (clusters); safe encapsulation; generic cluster; exception handling

    An abstract data type (ADT) specifies a type, a set of operations that may be performed on instances of the type, and the effects of these operations. The implementation of an ADT provides the actual operations that have the specified effects and also prevents programmers from doing anything else. The word “abstract” in CLU means “user defined”, or not built in to the language:

    "I referred to the types as “abstract” because they are not provided directly by a programming languages but instead must be implemented by the user. An abstract type is abstract in the same way that a procedure is an abstract operation." (Liskov 1996, page 473) (J: what a bad name!)

    This is not a universally accepted definition. Many people use the term ADT in the following sense: An abstract data type is a system consisting of three constituents:
    1. some sets of objects;
    2. a set of syntactic descriptions of the primitive functions;
    3. a semantic description — that is, a sufficiently complete set of relationships that specify how the functions interact with each other. (Martin 1986)

    The difference is that an ADT in CLU provides a particular implementation of the type (and, in fact, the implementation must be unique) whereas Martin’s definition requires only a specification.

    Compare Simula to CLU, Simula:
    1. does not provide encapsulation: clients could directly access the variables, as well as the functions, of a class;
    2. does not provide “type generators” or, as we would say now, “generic types” (“templates” in C++);
    3. associates operations with objects rather than types;
    4. handles built-in and user-defined types differently — for example, instances of user-defined classes are always heap-allocated.

    Heap is used for objects because:
    1. Declarations are easy to implement because, for all types, space is allocated for a pointer. Stack allocation breaks encapsulation because the size of the object (which should be an implementation “secret”) must be known at the point of declaration.
    2. Variable declaration is separated from object creation. This simplifies safe initialization.
    3. Variable and object lifetimes are not tied. When the variable goes out of scope, the object continues to exist (there may be other references to it).
    4. The meaning of assignment is independent of type. The statement x := E means simply that x becomes a reference (implemented as a pointer, of course) to E. There is no need to worry about the semantics of copying an object. Copying, if needed, should be provided by a member function of the class.
    5. Reference semantics requires garbage collection. Although garbage collection has a run-time overhead, efficiency was not a primary goal of the CLU project. Furthermore, garbage collection eliminates memory leaks and dangling pointers, notorious sources of obscure errors.

    Whereas Simula provides tools for the programmer and supports a methodology, CLU provides the same tools and enforces the methodology. The cost is an increase in complexity. Several keywords (rep, cvt, down, and up) are needed simply to maintain the distinction between abstract and representation types. In Simula, three concepts — procedures, classes, and records — are effectively made equivalent, resulting in significant simplification. In CLU, procedures, records, and clusters are three different things. The argument in favour of CLU is that the compiler will detect encapsulation and other errors. But is it the task of a compiler to prevent the programmer doing things that might be completely safe? Or should this role be delegated to a style checker, just as C programmers use both cc (the C compiler) and lint (the C style checker)?

    The designers of CLU advocate defensive programming: "Of course, checking whether inputs [i.e., parameters of functions] are in the permitted subset of the domain takes time, and it is tempting not to bother with the checks, or to use them only while debugging, and suppress them during production. This is generally an unwise practice. It is better to develop the habit of defensive programming, that it writing each procedure to defend itself against errors. . . .  Defensive programming makes it easier to debug programs. . . ." (Liskov and Guttag 1986, page 100) (J: so static type and type checker is good)

C++: superset of C; hybrid (imperative and OO); emphasize stack than heap; multiple inheritance; genericity; exception handling; no GC

Effiel: strictly OO; multiple inheritance; repeated inheritance; programming by contract; unusual excpetion handling; generic classes; may provide GC

    Programming by Contract is an extension of defensive programming. Effiel use require/ensure clauses to constitute a contract between function and caller. Require is like a assertion. If argument supplied by caller doesn't meet contract, the function can return anything. Effiel also has class invariation, which is a predicate over the variables of an instance of the class. It must be true when the object is created and after each function of the class has executed. Functions in a child class must provide a contract at least as strong as that of the corresponding function in the parent class.

    Repeated inheritance is like virutal inheritance in C++, but on attribute level instead of class level.

    An Eiffel function must either fulfill its contract or report failure. If a function contains a rescue clause, this clause is invoked if an operation within the function reports failure. A return from the rescue clause indicate that the function has failed. However, a rescue clause may perform some cleaning-up actions and then invoke retry to attempt the calculation again. The mechanism seems harder to use than other, more conventional, exception handling mechanisms. It is not obvious that there are many circumstances in which it makes sense to “retry” a function.

Java: byte codes; portability; security; single inheritance; single root; interfaces for multiple inheritance; exception handling; primitive values and wrapper classes; reference semantics; GC; applets; just-in-time compilation;

Kevo: All of the OOPLs previously described in this section are class-based languages. In a class-based language, the programmer defines one or more classes and, during execution, instances of these classes are created and become the objects of the system. (In Smalltalk, classes are run-time objects, but Smalltalk is nevertheless a class-based language.) There is another, smaller, family of OOPLs that use prototypes rather than classes. Although there are several prototype-based OOPLs, we discuss only one of them here. Antero Taivalsaari (1993) has given a thorough account of the motivation and design of his language, Kevo. Taivalsaari (1993, page 172) points out that class-based OOPLs, starting with Simula, are based on an Aristotelian view in which the world is seen as a collection of objects with well-defined properties arranged [according] to a hierarchical taxonomy of concepts. The problem with this approach is that there are many classes which people understand intuitively but which are not easily defined in taxonomic terms. Common examples include “book”, “chair”, “game”, etc. Prototypes provide an alternative to classes. In a prototype-based OOPL, each new type of object is introduced by defining a prototype or exemplar , which is itself an object. Identical copies of a prototype — as many as needed — are created by cloning . Alternatively, we can clone a prototype, modify it in some way, and use the resulting object as a prototype for another collection of identical objects. The modified object need not be self-contained. Some of its methods will probably be the same as those of the original object, and these methods can be delegated to the original object rather than replicated in the new object. Delegation thus plays the role of inheritance in a prototype-based language and, for this reason, prototype-based languages are sometimes called delegation languages. This is misleading, however, because delegation is not the only possible mechanism for inheritance with prototypes.

CLOS: The Common LISP Object System is an extension to LISP that provides OOP. It has a number of interesting features, including “multi-methods” — the choice of a function is based on the class of all of its arguments rather than its first (possibly implicit) argument. CLOS also provides “before” and “after” methods — code defined in a superclass that is executed before or after the invocation of the function in a class.

Self: Self is the perhaps the best-known prototype language. Considerable effort has been put into efficient implementation, to the extent that Self programs run at up to half of the speed of equivalent C++ programs. This is an impressive achievement, given that Self is considerably more flexible than C++.


Backtracking Languages
----------------------

Backtracking languages are designed to solve problems with multiple answers, and to find as many answers as required.

Constraint PLs are attracting increasing attention. A program is a set of constraints, expressed as equalities and inequalities, that must be satisfied. Prolog is strong in logical deduction but weak in numerical calculation; constraint PLs attempt to correct this imbalance.

Prolog: first logic PL; "pure" Prolog programme is inefficient; cut; occurs check; GC

    In the declarative reading, Prolog finds multiple results simply because they are there. In the procedure reading, multiple results are obtained by backtracking . Every point in the program at which there is more than one choice of a variable binding is called a choice point. The choice points define a tree: the root of the tree is the starting point of the program (the main goal) and each leaf of the tree represents either success (the goal is satisfied with the chosen bindings) or failure (the goal cannot be satisfied with these bindings). Prolog performs a depth-first search of this tree.

    In Prolog, proving not(P) means “P cannot be proved” which means “P cannot be inferred from the facts and rules in the database”.

    The Prolog system proceeds top-down. It attempts to match the entire goal to one of the rules and, having found a match, applies the same idea to the parts of the matched formulas. The “matching” process is called unification.

    Prolog syntax corresponds to a restricted form of first-order predicate calculus called clausal form logic. It is not practical to use the full predicate calculus as a basis for a PL because it is undecidable. Clausal form logic is semi-decidable: there is an algorithm that will find a proof for any formula that is true in the logic. If a formula is false, the algorithm may fail after a finite time or may loop forever. The proof technique, called SLD resolution, was introduced by Robinson (1965). SLD stands for “S electing a literal, using a Linear strategy, restricted to Definite clauses”.

    The proof of validity of SLD resolution assumes that unification is implemented with the occurs check. Unfortunately, the occurs check is expensive to implement and most Prolog systems omit it. Consequently, most Prolog systems are technically unsound, although problems are rare in practice. A Prolog program apparently has a straightforward interpretation as a statement in logic, but the interpretation is slightly misleading. For example, since Prolog works through the rules in the order in which they are written, the order is significant. Since the logical interpretation of a rule sequence is disjunction, it follows that disjunction in Prolog does not commute.

Alma-0: start from Modula-2, add many logic PL features;

    (J: really interesting and powerful)
    BES (boolean expression as statement)
    SBE (statement sequence as boolean expression)
    EITHER-ORELSE statement
    SOME statement
    COMMIT statement (like cut in prolog)
    FORALL statement
    EQ statement
    MIX: call by mixed form, a new param passing mechanism (J: I guess this is a bug trigger)
    KNOWN statement

    The language Alma-0 has a declarative semantics in which the constructs described above correspond to logical formulas. It also has a procedural semantics that describes how the constructs can be efficiently executed, using backtracking. Alma-0 demonstrates that, if appropriate care is taken, it is possible to design PLs that smoothly combine paradigms without losing the advantages of either paradigm.

    The interesting feature of Alma-0 is that it demonstrates that it is possible to start with a simple, well-defined language (Modula-2), remove features that are not required for the current purpose, and add a small number of simple, new features that provide expressive power when used together.


Implementation
--------------

A compiler translates source language to machine language. Since practical compilation systems allow programs to be split into several components for compiling, there are usually several phases.

  1. The components of the source program is compiled into object code.
  2. The object code units and library functions required by the program are linked to form an executable file.
  3. The executable file is loaded into memory and executed.

All compilers perform some checks to ensure that the source code that they are translating is correct. 

  * A FORTRAN compiler provides no checking between components. When the compiled components are linked, the linker will report subroutines that have not been defined but it will not detect discrepancies in the number of type of arguments. The result of such a discrepancy is, at best, run-time failure.

  * C and C++ rely on the programmer to provide header files containing declarations and implementation files containing definitions. The compiler checks that declarations and definitions are compatible. Overloaded functions in C++ present a problem because one name may denote several functions. A key design decision in C++, however, was to use the standard linker (Stroustrup 1994, page 34). The solution is “name mangling” — the C++ compiler alters the names of functions by encoding their argument types in such a way that each function has a unique name.

  * Modula-2 programs consist of interface modules and implementation modules. When the compiler compiles an implementation module, it reads all of the relevant interfaces and performs full checking. In practice, the interface modules are themselves compiled for efficiency, but this is not logically necessary.

  * Eiffel programs consist of classes that are compiled separately. The compiler automatically derives an interface from the class declaration and uses the generated interfaces of other classes to perform full checking. Other OOPLs that use this method include Blue, Dee, and Java.

An interpreter accepts source language and run-time input data and executes the program directly. It is possible to interpret without performing any translation at all, but this approach tends to be inefficient. Consequently, most interpreters do a certain amount of translation before executing the code. For example, the source program might be converted into an efficient internal representation, such as an abstract syntax tree, which is then “executed”. Interpreting is slower than executing compiled code, typically by a factor of 10 or more.

Some programming environments, particularly those for LISP, provide both interpretation and compilation. In fact, some LISP environments allow source code and compiled code to be freely mixed.

Byte Codes. Another mixed strategy consists of using a compiler to generate byte codes and an interpreter to execute the byte codes. The “byte codes”, as the name implies, are simply streams of bytes that represent the program: a machine language for an “abstract machine”. One of the first programming systems to be based on byte codes was the Pascal “P” compiler.

    The advantage of byte code programs is that they can, in principle, be run on any machine. Thus byte codes provide a convenient way of implementing “portable” languages. The Pascal “P” compiler provided a simple method for porting Pascal to a new kind of machine. The “Pascal P package” consisted of the P-code compiler written in P-code. A Pascal implementor had to perform the following steps:

      1. Write a P-code interpreter. Since P-code is quite simple, this is not a very arduous job.
      2. Use the P-code interpreter to interpret the P-code compiler. This step yields a P-code program that can be used to translate Pascal programs to P-code. At this stage, the implementor has a working Pascal system, but it is inefficient because it depends on the P-code interpreter.
      3. Modify the compiler source so that the compiler generates machine code for the new machine. This is a somewhat tedious task, but it is simplified by the fact that the compiler is written in Pascal.
      4. Use the compiler created in step 2 to compile the modified compiler. The result of this step is a P-code program that translates Pascal to machine code.
      5. Use the result of step 4 to compile the modified compiler. The result is a machine code program that translates Pascal to machine code — in other words, a Pascal compiler for the new machine.

    With the exercise of due caution, the process described above (called bootstrapping ) can be simplified. For example, the first version of the compiler, created in step 2 does not have to compile the entire Pascal language, but only those parts that are needed by the compiler itself. The complete sequence of steps can be used to generate a Pascal compiler that can compile itself, but may be incomplete in other respects. When that compiler is available, it can be extended to compile the full language.

Just In Time. “Just in time” (JIT) compilation is a recent strategy that is used for Java and a few research languages. The idea is that the program is interpreted, or perhaps compiled rapidly without optimization. As each statement is executed, efficient code is generated for it. This approach is based on the following assumptions:

    1. in a typical run, much of the code of a program will not be executed;
    2. if a statement has been executed once, it is likely to be executed again.

    JIT provides fast response, because the program can be run immediately, and efficient execution, because the second and subsequent cycles of a loop are executed from efficiently compiled code. Thus JIT combines some of the advantages of interpretation and compilation.

Interactive Systems: A PL can be designed for writing large programs that run autonomously or for interacting with the user. The categories overlap: PLs that can be used interactively, such as BASIC, LISP, SML, and Smalltalk, can also be used to develop large programs. The converse is not true: an interactive environment for C++ would not be very useful. An interactive PL is usually interpreted, but interpretation is not the only possible means of implementation. PLs that start out with interpreters often acquire compilers at a later stage — BASIC, LISP, and a number of other interactive PLs followed this pattern of development. A PL that is suitable for interactive use if programs can be described in small chunks. FPLs are often supported interactively because a programmer can build up a program as a collection of function definitions, where most functions are fairly small (e.g., they fit on a screen). Block structured languages, and modular languages, are less suitable for this kind of program development.

There is a sharp distinction: either a PL is implemented with garbage collection (GC) or it is not. The implementor does not really have a choice. An implementation of LISP, Smalltalk, or Java without GC would be useless — programs would exhaust memory in a few minutes at most.  Conversely, it is almost impossible to implement C or C++ with GC, because programmers can perform arithmetic on pointers.

The imperative PL community has been consistently reluctant to incorporate GC: there is no widely-used imperative PL that provides GC. The functional PL community has been equally consistent, but in the opposite direction. OOPL community is divided on this, but most of them provide GC.

Conservative GC depends on the fact that it is possible to guess with a reasonable degree of accuracy whether a particular machine word is a pointer (Boehm and Weiser 1988). In typical modern processors, a pointer is a 4-byte quantity, aligned on a word boundary, and its value is a legal address. By scanning memory and noting words with these properties, the GC can identify regions of memory that might be in use. The problem is that there will be occasionally be words that look like pointers but actually are not: this results in about 10% of garbage being retained. The converse, and more serious problem, of identifying live data as garbage cannot occur: hence the term “conservative”.


Abstraction
-----------

Abstraction is one of the most important mental tools in Computer Science. The biggest problems that we face in Computer Science are concerned with complexity; abstraction helps us to manage complexity. Abstraction is particularly important in the study of PLs because a PL is a two-fold abstraction.

A PL must provide an abstract view of the underlying machine. The values that we use in programming — integers, floats, booleans, arrays, and so on — are abstracted from the raw bits of computer memory. The control structures that we use — assignments, conditionals, loops, and so on — are abstracted from the basic machine instructions. The first task of a PL is to hide the low-level details of the machine and present a manageable interface, preferably an interface that does not depend on the particular machine that is being used.

A PL must also provide ways for the programmer to abstract information about the world. Most programs are connected to the world in one way or another, and some programs are little more than simulations of some aspect of the world. For example, an accounting program is, at some level, a simulation of tasks that used to be performed by clerks and accountants. The second task of a PL is to provide as much help as possible to the programmer who is creating a simplified model of the complex world in a computer program.

In the evolution of PLs, the first need came before the second. Early PLs emphasize abstraction from the machine: they provide assignments, loops, and arrays. Programs are seen as lists of instructions for the computer to execute. Later PLs emphasize abstraction from the world: they provide objects and processes. Programs are seen as descriptions of the world that happen to be executable.

An abstraction mechanism provides a way of naming and parameterizing a program entity.

Procedures: The earliest abstraction mechanism was the procedure, which exists in rudimentary form even in assembly language. Procedural abstraction enables us to give a name to a statement, or group of statements, and to use the name elsewhere in the program to trigger the execution of the statements. Parameters enable us to customize the execution according to the particular requirements of the caller.

Functions: The next abstraction mechanism, which also appeared at an early stage was the function. Functional abstraction enables us to give a name to an expression, and to use the name to trigger evaluation of the expression.

    In many PLs, procedures and functions are closely related. Typically, they have similar syntax and similar rules of construction. The reason for this is that an abstraction mechanism that works only for simple expressions is not very useful. In practice, we need to abstract form arbitrarily complex programs whose main purpose is to yield a single value.

    The difference between procedures and functions is most evident at the call site, where a procedure call appears in a statement context and a function call appears in an expression context.

    FORTRAN provides procedures (called “subroutines”) and functions. Interestingly, COBOL does not provide procedure or function abstraction, although the PERFORM verb provides a rather restricted and unsafe way of executing statements remotely. LISP provides function abstraction. The effect of procedures is obtained by writing functions with side-effects. Algol 60, Algol 68, and C take the view that everything is an expression. (The idea seems to be that, since any computation leaves something in the accumulator, the programmer might as well be allowed to use it.) Thus statements have a value in these languages. It is not surprising that this viewpoint minimizes the difference between procedures and functions. In Algol 68 and C, a procedure is simple a function that returns a value of type void. Since this value is unique, it requires log 1 = 0 bits of storage and can be optimized away by the compiler.

Data Types

Classes


Computational Model
-------------------

There is an abstraction that is useful for describing a PL in general terms, without the details of every construct, and that is the computational model (also sometimes called the semantic model ). The computational model (CM) is an abstraction of the operational semantics of the PL and it describes the effects of various operations without describing the actual implementation. Programmers who use a PL acquire intuitive knowledge of its CM. When a program behaves in an unexpected way, the implementation has failed to respect the CM.

A simple CM does not imply easy implementation. Often, the opposite is true.

There are two ways in which an OOPL can treat primitive entities such as booleans, characters, and integers. Primitive values can have a special status that distinguishes them from objects. A language that uses this policy is likely to be efficient, because the compiler can process primitives without the overhead of treating them as full-fledged objects. On the other hand, it may be confusing for the user to deal with two different kinds of variable — primitive values and objects.  Primitive values can be given the same status as objects. This simplifies the programmer’s task, because all variables behave like objects, but a naive implementation will probably be inefficient.

A solution for this dilemma is to define a CM for the language in which all variables are objects. An implementor is then free to implement primitive values efficiently provided that the behaviour predicted by the CM is obtained at all times. In Smalltalk, every variable is an object. Early implementations of Smalltalk were inefficient, because all variables were actually implemented as objects. Java provides primitives with efficient implementations but, to support various OO features, also has classes for the primitive types.

FPLs are based on the theory of partial recursive functions. They attempt to use this theory as a CM. The advantage of this point of view is that program text resembles mathematics and, to a certain extent, programs can be manipulated as mathematical objects. The disadvantage is that mathematics and computation are not the same. Some operations that are intuitively simple, such as maintaining the balance of a bank account subject to deposits and withdrawals, are not easily described in classical mathematics. The underlying difficulty is that many computations depend on a concept of mutable state that does not exist in mathematics.

FPLs have an important property called referential transparency . An expression is referentially transparent if its only attribute is its value. The practical consequence of referential transparency is that we can freely substitute equal expressions.

Early CMs: modelled programs as code acting on data; structured programs by recursive decomposition of code.
Later CMs: modelled programs as packages of code and data; structured programs by recursive decomposition of packages of code and data.
The CM should: help programmers to reason about programs; help programmers to read and write programs; constrain but not determine the implementation.


Names and Binding
-----------------

free name: n+1
bound name: for n in (1..5); puts n; end

Expression with bounded name has definite value, change the name doesn't change the value.

In predicate calculus, predicates may contain free names, as in: n mod 2 = 0 => n^2 mod 2 = 0. Names are bound by the quantifiers ∀ (for all) and ∃ (there exists). We say that the formula ∀n . n mod 2 = 0 => n^2 mod 2 = 0 is closed because it contains no free variables. Strictly, we should specify the range of values that n is allowed to assume. We could do this implicitly: for example, it is very likely that n refers to integers. Alternatively, we could define the range of values explicitly, as in ∀n ∈ Z . n mod 2 = 0 => n^2 mod 2 = 0.

Precisely analogous situations occur in programming. In the function

  int f (int n)
  {
    return k + n;
  }

k occurs free and n occurs bound. Note that we cannot tell the value of n from the definition of the function but we know that n will be given a value when the function is called. In most PLs, a program with free variables will not compile. A C compiler, for example, will accept the function f defined above only if it is compiled in a scope that contains a declaration of k. Some early PLs, such as FORTRAN and PL/I, provided implicit declarations for variables that the programmer did not declare; this is now understood to be error-prone and is not a feature of recent PLs.

In mathematics, a variable normally has only one attribute: its value. In (1), n is bound to the values 0, 1, . . . , 5. Sometimes, we specify the domain from which these values are chosen, as in n ∈ Z. In programming, a name may have several attributes, and they may be bound at different times.  For example, in the sequence

  int n;
  n = 6;

the first line binds the type int to n and the second line binds the value 6 to n. The first binding occurs when the program is compiled. (The compiler records the fact that the type of n is int; neither this fact nor the name n appears explicitly in the compiled code.) The second binding occurs when the program is executed. This example shows that there are two aspects of binding that we must consider in PLs: the attribute that is bound, and the time at which the binding occurs. The attributes that may be bound to a name include: type, address, and value. The times at which the bindings occur include: compile time, link time, load time, block entry time, and statement execution time.

Definition. A binding is static if it occurs during before the program is executed: during compilation or linking. A binding is dynamic if it occurs while the program is running: during loading, block entry, or statement execution.

An anonymous contributor to the Encyclopedia of Computer Science wrote: Broadly speaking, the history of software development is the history of ever-later binding time.

  Early binding => efficiency
  Late binding => flexibility

Variable Addressing:

  In FORTRAN, addresses are bound to variable names at compile time. The result is that, in the compiled code, variables are addressed directly, without any indexing or other address calculations. (In reality, the process is somewhat more complicated. The compiler assigns an address relative to a compilation unit. When the program is linked, the address of the unit within the program is added to this address. When the program is loaded, the address of the program is added to the address. The important point is that, by the time execution begins, the “absolute” address of the variable is known.) FORTRAN is efficient, because absolute addressing is used. It is inflexible, because all addresses are assigned at load time. This leads to wasted space, because all local variables occupy space whether or not they are being used, and also prevents the use of direct or indirect recursion.

  In languages of the Algol family (Algol 60, Algol 68, C, Pascal, etc.), local variables are allocated on the run-time stack. When a function is called, a stack frame (called, in this context, an activation record or AR) is created for it and space for its parameters and local variables is allocated in the AR. The variable must be addressed by adding an offset (computed by the compiler) to the address of the AR, which is not known until the function is called. Algol-style is slightly less efficient than FORTRAN because addresses must be allocated at block-entry time and indexed addressing is required. It is more flexible than FORTRAN because inactive functions do not use up space and recursion works.

  Languages that provide dynamic allocation, such as Pascal and C and most of their successors, have yet another way of binding an address to a variable. In these languages, a statement such as new(p) allocates space on the “heap” and stores the address of that space in the pointer p. Accessing the variable requires indirect addressing, which is slightly slower then indexed addressing, but greater flexibility is obtained because dynamic variables do not have to obey stack discipline (last in, first out).

Function Addressing:

  When the compiler encounters a function definition, it binds an address to the function name. (As above, several steps must be completed before the absolute address of the function is determined, but this is not relevant to the present discussion.) When the compiler encounters a function invocation, it generates a call to the address that it has assigned to the function. This statement is true of all PLs developed before OOP was introduced. OOPLs provide “virtual” functions. A virtual function name may refer to several functions. The actual function referred to in a particular invocation is not in general known until the call is executed.

  Thus in non-OO PLs, functions are statically bound, with the result that calls are efficiently executed but no decisions can be made at run-time. In OOPLs, functions are dynamically bound. Calls take longer to execute, but there is greater flexibility because the decision as to which function to execute is made at run-time. The overhead of a dynamically-bound function call depends on the language. In Smalltalk, the overhead can be significant, because the CM requires a search. In practice, the search can be avoided in up to 95% of calls by using a cache. In C++, the compiler generates “virtual function tables” and dynamic binding requires only indirect addressing. Note, however, that this provides yet another example of the principle: Smalltalk binds later than C++, runs more slowly, but provides greater flexibility.

Definition. An entity can be named if the PL provides a definitional mechanism that associates a name with an instance of the entity. An entity can be denoted if the PL provides ways of expressing instances of the entity as expressions.

In C, we can name functions but we cannot denote them. The definition

  int f (int x)
  {
    B
  }

introduces a function with name f , parameter x, and body B. Thus functions in C can be named. There is no way that we can write a function without a name in C. Thus functions in C cannot be denoted.

We have seen that most PLs choose between two interpretations of a variable name. In a PL with value semantics, variable names denote memory locations. Assigning to a variable changes the contents of the location named by the variable. In a PL with reference semantics, variable names denote objects in memory. Assigning to a variable, if permitted by the language, changes the denotation of the name to a different object. Reference semantics is sometimes called “pointer semantics”. This is reasonable in the sense that the implementation of reference semantics requires the storage of addresses — that is, pointers. It is misleading in that providing pointers is not the same as providing reference semantics. The distinction is particularly clear in Hoare’s work on PLs. Hoare (1974) has this to say about the introduction of pointers into PLs.

  Many language designers have preferred to extend [minor, localized faults in Algol 60 and other PLs] throughout the whole language by introducing the concept of reference, pointer, or indirect address into the language as an assignable item of data. This immediately gives rise in a high-level language to one of the most notorious confusions of machine code, namely that between an address and its contents. Some languages attempt to solve this by even more confusing automatic coercion rules. Worse still, an indirect assignment through a pointer, just as in machine code, can update any store location whatsoever, and the damage is no longer confined to the variable explicitly names as the target of assignment. For example, in Algol 68, the assignment x := y always changes x, but the assignment p := y + 1 may, if p is a reference variable, change any other variable (of appropriate type) in the whole machine. One variable it can never change is p! . . . . References are like jumps, leading wildly from one part of a data structure to another. Their introduction into high-level languages has been a step backward from which we may never recover.

One year later, Hoare (1975) provided his solution to the problem of references in high-level PLs:

  In this paper, we will consider a class of data structures for which the amount of storage can actually vary during the lifetime of the data, and we will show that it can be satisfactorily accommodated in a high-level language using solely high-level problem-oriented concepts, and without the introduction of references.

The implementation that Hoare proposes in this paper is a reference semantics with types. Explicit types make it possible to achieve

  . . . . a significant improvement on the efficiency of compiled LISP, perhaps even a factor of two in space-time cost for suitable applications. (Hoare 1975)

All FPLs and most OOPLs (the notable exception, of course, being C++) use reference semantics. There are good reasons for this choice, but the reasons are not the same for each paradigm.

In a FPL, all (or at least most) values are immutable. If X and Y are have the same value, the program cannot tell whether X and Y are distinct objects that happen to be equal, or pointers to the same object. It follows that value semantics, which requires copying, would be wasteful because there is no point in making copies of immutable objects. (LISP provides two tests for equality. (eq x y) is true if x and y are pointers to the same object. (equal x y) is true if the objects x and y have the same extensional value. These two functions are provided partly for efficiency and partly to cover up semantic deficiencies of the implementation. Some other languages provide similar choices for comparison.)

One of the important aspects of OOP is object identity . If a program object X corresponds to a unique entity in the world, such as a person, it should be unique in the program too. This is most naturally achieved with a reference semantics.


Polymorphism
------------

The word “polymorphism” is derived from Greek and means, literally, “many shapes”. In PLs, “polymorphism” is used to describe a situation in which one name can refer to several different entities. The most common application is to functions. There are several kinds of polymorphism; the terms “ad hoc polymorphism” and “parametric polymorphism” are due to Christopher Strachey.

Ad Hoc Polymorphism:

  In general, ad hoc polymorphism refers to the use of a single function name to refer to two or more distinct functions. Typically the compiler uses the types of the arguments of the function to decide which function to call. Ad hoc polymorphism is also called “overloading”. Almost all PLs provide ad hoc polymorphism for built-in operators such as “+”, “−”, “*”, etc. Ada, C++, and other recent languages also allow programmers to overload functions. (Strictly, we should say “overload function names” but the usage “overloaded functions” is common.) In general, all that the programmer has to do is write several definitions, using the same function name, but ensuring that the either the number or the type of the arguments are different.

Parameterized Polymorphism:

  Suppose that a language provides the type list as a parameterized type. That is, we can make declarations such as these:

    list(int)
    list(float)

  Suppose also that we have two functions: sum computes the sum of the components of a given list, and len computes the number of components in a given list. There is an important difference between these two functions. In order to compute the sum of a list, we must be able to choose an appropriate “add” function, and this implies that we must know the type of the components. On the other hand, there seems to be no need to know the type of the components if all we need to do is count them. A function such as len, which counts the components of a list but does not care about their type, has parametric polymorphism.

Object Polymorphism:

  In OOPLs, there may be many different objects that provide a function called, say, f . However, the effect of invoking f may depend on the object. The details of this kind of polymorphism, which are discussed in Section 6, depend on the particular OOPL. This kind of polymorphism is a fundamental, and very important, aspect of OOP.


Assignment
----------

Consider the assignment x := E. Whatever the PL, the semantics of this statement will be something like:

  evaluate the expression E;
  store the value obtained in x.

The assignment is unproblematic if x and E have the same type. But what happens if they have different types? There are several possibilities:

  * The statement is considered to be an error. This will occur in a PL that provides static type checking but does not provide coercion. For example, Pascal provides only a few coercions (subrange to integer, integer to real, etc.) and rejects other type differences.
  * The compiler will generate code to convert the value of expression E to the type of x. This approach was taken to extremes in COBOL and PL/I. It exists in C and C++, but C++ is stricter than C.
  * The value of E will be assigned to x anyway. This is the approach taken by PLs that use dynamic type checking. Types are associated with objects rather than with names.


Scope and Extent
----------------

The scope of a name is the region of the program text in which a name may be used. In C++, for example, the scope of a local variable starts at the declaration of the variable and ends at the end of the block in which the declaration appears. Scope is a static property of a name that is determined by the semantics of the PL and the text of the program.

  * Global Scope. The name is visible throughout the program.
  * Local Scope. In block-structured languages, names declared in a block are local to the block. There is a question as to whether the scope starts at the point of definition or is the entire block. (In other words, does the PL require “declaration before use”?)
  * Qualified Scope. The components of a structure, such as a Pascal record or a C struct, have names. These names are usually hidden, but can be made visible by the name of the object.
  * Import and Export. In modular languages, a name or group of names can be brought into a scope with an import clause. The module that provides the definitions must have a corresponding export clause.
  * Namespaces. The mechanisms above are inadequate for very large programs developed by large teams. Problems arise when a project uses several libraries that may have conflicting names. Namespaces, provided by PLs such as C++ and Common LISP, provide a higher level of name management than the regular language features.

  * Nested scopes are convenient in small regions, such as functions. The advantage of nested scopes for large structures, such as classes or modules, is doubtful.
  * Nesting is a form of scope management. For large structures, explicit control by name qualification may be better than nesting.
  * Qualified names work well at the level of classes and modules, when the source of names is obvious.
  * The import mechanism has the problem that the source of a name is not obvious where it appears: users must scan possibly remote import declarations.
  * Qualified names are inadequate for very large programs.
  * Library designers can reduce the potential for name conflicts by using distinctive prefixes. For example, all names supplied by the commercial graphics library FastGraph are have fg_ as a prefix.
  * Namespaces provide a better solution than prefixes.

Scope management is important because the programmer has no “work arounds” if the scoping mechanisms provided by the PL are inadequate. This is because PLs typically hide the scoping mechanisms. If a program is compiled, names will normally not be accessible at run-time unless they are included for a specific purpose such as debugging. In fact, one way to view compiling is as a process that converts names to numbers. Interpreters generally have a repository that contains the value of each name currently accessible to the program, but programmers may have limited access to this repository.

The extent, also called lifetime, of a name is the period of time during program execution during which the object corresponding to the name exists. Understanding the relation between scope and extent is an important part of understanding a PL.

Global names usually exist for the entire lifetime of the execution: they have global extent. In FORTRAN, local variables also exist for the lifetime of the execution. Programmers assume that, on entry to a subroutine, its local variables will not have changed since the last invocation of the subroutine.

In Algol 60 and subsequent stack-based languages, local variables are instantiated whenever control enters a block and they are destroyed (at least in principle) when control leaves the block. It is an error to create an object on the stack and to pass a reference to that object to an enclosing scope. Some PLs attempt to detect this error (e.g., Algol 68), some try to prevent it from occurring (e.g., Pascal), and others leave it as a problem for the programmer (e.g., C and C++).

In PLs that use a reference model, objects usually have unlimited extent, whether or not their original names are accessible. Examples include Simula, Smalltalk, Eiffel, CLU, and all FPLs. The advantage of the reference model is that the problems of disappearing objects (dangling pointers) and inaccessible objects (memory leaks) do not occur. The disadvantage is that GC and the accompanying overhead are more or less indispensable.

The separation of extent from scope was a key step in the evolution of post-Algol PLs.
